<html>
<head>
<title>Intuitive Guide to Understanding GloVe Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解手套嵌入的直观指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010?source=collection_archive---------1-----------------------#2019-05-05">https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010?source=collection_archive---------1-----------------------#2019-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="26cc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/light-on-math" rel="noopener" target="_blank">点亮数学机器学习</a></h2><div class=""/><div class=""><h2 id="1fb4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理解 GloVe 和 Keras 实现背后的理论！</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eab294360e3edb2da46cddaa4c4487e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAfb-kU1fgZ27OAek9UYiA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/photos/2OCh8tuNsBo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jelleke Vanooteghem</a> on <a class="ae lh" href="https://unsplash.com/search/photos/words?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="ab3b" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak"><em class="md"/></strong>；(太久没发工资了？不用担心，您仍然可以通过以下链接访问代码)</h2><p id="5f30" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">带 Keras 的手套实现:<a class="ae lh" href="https://github.com/thushv89/exercises_thushv_dot_com/blob/master/glove_light_on_math_ml/glove_light_on_math_ml.ipynb" rel="noopener ugc nofollow" target="_blank">【此处为</a>】</p><p id="18b1" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">在本文中，您将了解 GloVe，这是一种非常强大的单词向量学习技术。本文将重点解释为什么 GloVe 更好，以及 GloVe 的成本函数背后的动机，这是算法中最关键的部分。。该代码将在后面的文章中详细讨论。</p><p id="8f33" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">要访问本系列中我以前的文章，请使用以下信件。</p><p id="f8b5" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/light-on-math-ml-attention-with-keras-dc8dbc1fad39"><strong class="mg jd">A</strong></a><strong class="mg jd">B</strong><a class="ae lh" href="http://www.thushv.com/computer_vision/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks/" rel="noopener ugc nofollow" target="_blank"><strong class="mg jd">C</strong></a><strong class="mg jd"/><a class="ae lh" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7"><strong class="mg jd">D</strong></a><strong class="mg jd">* E F G H I J</strong><a class="ae lh" href="http://www.thushv.com/machine-learning/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence/" rel="noopener ugc nofollow" target="_blank"><strong class="mg jd">K</strong></a><strong class="mg jd"/><a class="ae lh" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"><strong class="mg jd">L</strong><strong class="mg jd">*</strong></a><a class="ae lh" href="https://medium.com/p/bee5af0c01aa" rel="noopener"><strong class="mg jd">M</strong></a></p><p id="5989" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"> [ </strong>🔈🔥<strong class="mg jd">最新文章</strong>🔥🔈<strong class="mg jd"/>:<a class="ae lh" href="https://medium.com/p/bee5af0c01aa" rel="noopener">M—矩阵分解</a></p><p id="2ba9" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">GloVe 是一种词向量技术，它在短暂的沉寂之后驾驭了词向量的浪潮。只是为了刷新，单词向量将单词放入一个很好的向量空间，相似的单词聚集在一起，不同的单词相互排斥。GloVe 的优势在于，与 Word2vec 不同，GloVe 不仅仅依赖于局部统计(单词的局部上下文信息)，而是融入了全局统计(单词共现)来获取单词向量。但是请记住，手套和 Word2vec 之间有相当多的协同作用。</p><p id="fe06" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">听到使用全局统计数据来推导单词之间的语义关系的想法可以追溯到很久以前，不要感到惊讶。第一回，<a class="ae lh" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">潜在语义分析</a> (LSA)。这只是一个有趣的事实。我们继续吧。</p><h1 id="b6bd" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">Word2vec 复习</h1><p id="9388" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">Word2vec 背后的根本思想是什么？</p><blockquote class="nn"><p id="8806" class="no np it bd nq nr ns nt nu nv nw mw dk translated">从一个人交的朋友你就可以知道他说了什么——j·r·弗斯</p></blockquote><p id="63e9" class="pw-post-body-paragraph me mf it mg b mh nx kd mj mk ny kg mm lr nz mo mp lv oa mr ms lz ob mu mv mw im bi translated">单词向量就是建立在这个想法上的。基本上，你得到一个大型语料库，并制作一个元组数据集，其中每个元组包含(某个单词 x，x 的上下文中的一个单词)。然后你会使用你的老朋友，一个神经网络，学习预测 x 的上下文单词，给定单词 x。如果你想了解更多关于 Word2vec 的信息，请参考我的文章<a class="ae lh" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-word2vec-e0128a460f0f">这里</a>。</p><h1 id="7281" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">那么是什么在拉回呢？</h1><p id="b3df" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">鉴于 Word2vec 的显眼性能，为什么不坚持使用呢？原因不在于性能，而在于解决方案制定的根本。记住，Word2vec 只依赖于语言的<strong class="mg jd"> <em class="oc">本地信息</em> </strong>。也就是说，对于一个给定的单词所学习的语义，只受周围单词的影响。</p><p id="a96c" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">例如，以这个句子为例，</p><blockquote class="nn"><p id="73b8" class="no np it bd nq nr ns nt nu nv nw mw dk translated">那只猫坐在垫子上</p></blockquote><p id="1092" class="pw-post-body-paragraph me mf it mg b mh nx kd mj mk ny kg mm lr nz mo mp lv oa mr ms lz ob mu mv mw im bi translated">如果你使用 Word2vec，它不会捕捉这样的信息，</p><blockquote class="nn"><p id="eb65" class="no np it bd nq nr ns nt nu nv nw mw dk translated">“the”是“cat”和“mat”这两个词的特殊语境吗？</p></blockquote><p id="5e19" class="pw-post-body-paragraph me mf it mg b mh nx kd mj mk ny kg mm lr nz mo mp lv oa mr ms lz ob mu mv mw im bi translated">或者</p><blockquote class="nn"><p id="3f36" class="no np it bd nq nr ns nt nu nv nw mw dk translated">“the”只是一个停用词吗？</p></blockquote><p id="2c66" class="pw-post-body-paragraph me mf it mg b mh nx kd mj mk ny kg mm lr nz mo mp lv oa mr ms lz ob mu mv mw im bi translated">这可能是次优的，尤其是在理论家的眼中。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="10eb" class="nc lj it bd lk nd ok nf ln ng ol ni lq ki om kj lu kl on km ly ko oo kp mc nm bi translated"><strong class="ak">回车，手套</strong>。</h1><p id="1823" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">GloVe 代表“全局向量”。如前所述，GloVe 捕获语料库的全局统计数据和局部统计数据，以便得出单词向量。但是，我们需要全球和地方统计数据吗？</p><h1 id="0604" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">两个比一个好吗？</h1><p id="a8bd" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">事实证明，每种类型的统计都有自己的优势。例如，捕获局部统计数据的 Word2vec 在类比任务中表现非常好。然而，像 LSA 这样只使用全局统计的方法在类比任务中做得不好。然而，由于 Word2vec 方法由于仅使用局部统计而受到某些限制(如我们上面讨论的)。</p><h1 id="0f2f" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">手套介绍</h1><p id="b39a" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">手套方法建立在一个重要的理念上，</p><blockquote class="nn"><p id="95bb" class="no np it bd nq nr ns nt nu nv nw mw dk translated">你可以从共现矩阵中推导出单词之间的语义关系。</p></blockquote><p id="2cb2" class="pw-post-body-paragraph me mf it mg b mh nx kd mj mk ny kg mm lr nz mo mp lv oa mr ms lz ob mu mv mw im bi translated">给定一个含有<strong class="mg jd"> <em class="oc"> V </em> </strong>词语的语料库，共现矩阵<strong class="mg jd"> <em class="oc"> X </em> </strong>将是一个<strong class="mg jd"> <em class="oc"> V x V </em> </strong>矩阵，其中第<strong class="mg jd"><em class="oc"/></strong>行和第<strong class="mg jd"> <em class="oc"> j </em> </strong>列为<strong class="mg jd"> <em class="oc"> X </em> </strong>，<strong class="mg jd"> <em class="oc"> X_ij </em> </strong>一个示例共现矩阵可能如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/877499ea88f1e50adfb74a42fe85f458.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*QWcK8CIDs8kMkOwsOxvywA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The co-occurrence matrix for the sentence “the cat sat on the mat” with a window size of 1. As you probably noticed it is a symmetric matrix.</figcaption></figure><p id="aacc" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">我们如何从中获得一个度量词之间语义相似性的指标呢？为此，你需要一次说三个词。让我具体记下这句话。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/80a1deb0dd67088f5d70c053d06aa6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*8EI7ODLsxIX9hr7gEJnLSw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The behavior of P_ik/P_jk for various words (Source [1])</figcaption></figure><p id="0d42" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">考虑实体</p><p id="54ab" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd">T37】P _ ik/P _ JKT39】其中<strong class="mg jd">T41】P _ ik = X _ ik/X _ IT43】</strong></strong></p><p id="c099" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">这里的<strong class="mg jd"> <em class="oc"> P_ik </em> </strong>表示同时看到单词<strong class="mg jd"> <em class="oc"> i </em> </strong>和<strong class="mg jd"> <em class="oc"> k </em> </strong>的概率，通过除以同时出现<strong class="mg jd"> <em class="oc"> i </em> </strong>和<strong class="mg jd"><em class="oc">k</em></strong>(<strong class="mg jd"><em class="oc">X _ ik</em></strong>)的次数来计算</p><p id="7236" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">你可以看到，给出两个词，即<strong class="mg jd"><em class="oc"/></strong>和<strong class="mg jd"><em class="oc"/></strong>，如果第三个词<strong class="mg jd"><em class="oc"/></strong>(也叫“探针词”)，</p><ul class=""><li id="dd04" class="or os it mg b mh mx mk my lr ot lv ou lz ov mw ow ox oy oz bi translated">与冰很相似但与蒸汽无关(如<strong class="mg jd"><em class="oc">k</em></strong>=固体)<strong class="mg jd"> <em class="oc"> P_ik/P_jk </em> </strong>会很高(&gt; 1)，</li><li id="803d" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated">与蒸汽很相似但与冰无关(如<strong class="mg jd"><em class="oc">k</em></strong>=气体)<strong class="mg jd"><em class="oc">P _ ik/P _ JK</em></strong>会很小(&lt; 1)，</li><li id="d218" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated">与任一单词相关或不相关，那么<strong class="mg jd"><em class="oc">P _ ik/P _ JK</em></strong>将接近 1</li></ul><p id="ecf0" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">因此，如果我们能够找到一种方法将<strong class="mg jd"><em class="oc">P _ ik/P _ JK</em></strong>结合到计算单词向量中，我们将实现在学习单词向量时使用全局统计的目标。</p><h1 id="0eb4" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">从度量到词向量</h1><p id="d42c" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">如果你喜欢到目前为止，系好安全带。就要变得粗暴了！我们如何能得到一个字向量算法并不是很明显，</p><ul class=""><li id="f9c2" class="or os it mg b mh mx mk my lr ot lv ou lz ov mw ow ox oy oz bi translated">我们没有方程，例如<strong class="mg jd"> <em class="oc"> F(i，j，k) = P_ik/P_jk </em> </strong>，只是一个表达式。</li><li id="42ac" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated">词向量是高维向量，然而<strong class="mg jd"> <em class="oc"> P_ik/P_jk </em> </strong>是标量。所以存在维度不匹配。</li><li id="275b" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated">涉及三个实体(<strong class="mg jd"> <em class="oc"> i，j </em> </strong>，和<strong class="mg jd"> <em class="oc"> k </em> </strong>)。但是用三个元素计算损失函数可能会很麻烦，需要减少到两个。</li></ul><p id="5dec" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">回答这三个问题是 GloVe 的主要贡献。现在让我们一步一步地浏览 GloVe，看看回答这三个问题如何给我们一个词向量算法。</p><p id="0776" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">我使用下面的符号，由于在介质上渲染乳胶的困难，它与论文略有不同。</p><ul class=""><li id="b254" class="or os it mg b mh mx mk my lr ot lv ou lz ov mw ow ox oy oz bi translated"><strong class="mg jd"> <em class="oc"> w </em> </strong>，<strong class="mg jd"> <em class="oc"> u </em> </strong> —两个独立的嵌入层</li><li id="de17" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated"><strong class="mg jd"><em class="oc">w *</em></strong>—w 的转置</li><li id="bf0d" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated"><strong class="mg jd"> <em class="oc"> X </em> </strong> —共生矩阵</li><li id="d6da" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated"><strong class="mg jd"> <em class="oc"> bw </em> </strong>和<strong class="mg jd"> <em class="oc"> bu </em> </strong> —分别为 w 和 u 的偏差</li></ul><h1 id="b6da" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">让我们假设一个等式</h1><p id="88ae" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">回答第一个问题很容易。假设一下。假设有一个函数 F，它接受字向量<strong class="mg jd"> <em class="oc"> i </em> </strong>，<strong class="mg jd"> <em class="oc"> j </em> </strong>和<strong class="mg jd"> <em class="oc"> k </em> </strong>，输出我们感兴趣的比率。</p><p id="ab17" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd">T51】F(w _ I，w_j，u _ k)= P _ ik/P _ JKT53】</strong></p><p id="0429" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">现在你应该有点好奇了，因为我们看到两个嵌入层在播放(<strong class="mg jd"> w </strong>和<strong class="mg jd">T57】u</strong>)。为什么是两个？该论文称，通常这两层的性能相当，只是随机初始化不同。然而，有两层有助于模型减少<em class="oc">过度拟合</em>。</p><p id="2b32" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">现在回到函数。词向量是线性系统。例如，您可以在嵌入空间中执行算术运算，例如</p><p id="6e45" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"><em class="oc">w _ {国王}—w _ {男性}+w _ {女性} = w _ {女王} </em> </strong></p><p id="6e2b" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">因此，让我们将上面的等式改为下面的等式，</p><p id="f775" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"> <em class="oc"> F(w_i — w_j，u_k) = P_ik/P_jk </em> </strong></p><p id="869b" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">为什么<strong class="mg jd"> <em class="oc"> w_i — w_j </em> </strong>适合这里？其实你可以在嵌入空间中推导出你观察到的关于<strong class="mg jd"> <em class="oc"> P_ik/P_jk </em> </strong>的美好性质。我来详细说明一下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/45d07ce7de562850cf142af9c245ae5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJIk9Uv9-Tjwq8184slCHQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Behaviour of vector distances to a probe word w.r.t. w_i — w_j</figcaption></figure><p id="1826" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">所以你可以看到当考虑不同的单词时，距离(虚线)是如何变化的。以及两个给定词<strong class="mg jd"> <em class="oc"> i </em> </strong>和<strong class="mg jd"> <em class="oc"> k </em> </strong>之间的距离，与<strong class="mg jd"> <em class="oc"> P_{ik} </em> </strong>的倒数相关。为什么会这样呢？因为我们总是计算距离 w.r.t 字向量<strong class="mg jd"> <em class="oc"> w_i — w_j </em> </strong>(即红线)。所以从<strong class="mg jd"> <em class="oc"> w_i — w_j. </em> </strong>开始也未尝不可</p><h1 id="cd76" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">向量到标量…</h1><p id="b1eb" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">一个问题解决了，我们继续下一个问题。我们如何使 LHS 成为标量？对此有一个非常直接的答案。也就是用下面的方法在两个实体之间引入一个转置和一个点积。</p><p id="dfbc" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"> <em class="oc"> F((w_i — w_j)*。u_k) = P_ik/P_jk </em> </strong>或者，</p><p id="5178" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">如果假设一个字向量为一个<strong class="mg jd"> <em class="oc"> Dx1 </em> </strong>矩阵，<strong class="mg jd"> <em class="oc"> (w_i — w_j) </em> </strong> *将被<strong class="mg jd"> <em class="oc"> 1xD </em> </strong>整形，从而在与<strong class="mg jd"> <em class="oc"> u_k </em> </strong>相乘时给出一个标量。</p><h1 id="65f0" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">F 能是什么？</h1><p id="cf55" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">接下来，如果我们假设 F 有某个性质(即<a class="ae lh" href="https://en.wikipedia.org/wiki/Homomorphism" rel="noopener ugc nofollow" target="_blank"> <em class="oc">加法群和乘法群之间的同态</em> </a>)给出</p><p id="035b" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"><em class="oc">F(w _ I * u _ k—w _ j * u _ k)= F(w _ I * u _ k)/F(w _ j * u _ k)= P _ ik/P _ JK</em></strong></p><p id="e2ab" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">换句话说，这种特殊的同态确保了减法<strong class="mg jd"><em class="oc">【A-B】</em></strong>也可以表示为除法<strong class="mg jd"> <em class="oc"> F(A)/F(B) </em> </strong>并得到相同的结果。因此，</p><p id="9984" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd">T57】F(w _ I * u _ k)/F(w _ j * u _ k)= P _ ik/P _ JKT59】</strong></p><p id="a704" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">和</p><p id="dc48" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd">T61】F(w _ I * u _ k)= P _ ikT63】</strong></p><h1 id="8d97" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">我对你耍了点小花招…</h1><p id="0f9d" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">好吧，我是偷偷摸摸的。仅仅因为 F(A)/F(B) = G(A)/G(B)你不能说 F(A) = G(A)。因为 F(A)/F(B)=2F(A)/2F(B)，不代表 F(A)=2F(A)。从最初的论文来看，并不清楚(至少对我来说)为什么这样假设。但是让我给你一些直觉，为什么这是一个安全的假设。如果我们要正确定义上述关系，它应该是，</p><p id="d79d" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"><em class="oc">F(w _ I * u _ k)= c P _ ik</em></strong>对于某些常数<strong class="mg jd"> <em class="oc"> c </em> </strong></p><p id="5561" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">但有了这个，你也得到了<strong class="mg jd"> <em class="oc"> F(w_j* u_k) = c P_jk </em> </strong>对于任意的<strong class="mg jd"> <em class="oc"> j </em> </strong>。所以如果<strong class="mg jd"> <em class="oc"> i </em> </strong>与<strong class="mg jd"> <em class="oc"> k </em> </strong>之间的相似度按<strong class="mg jd"> <em class="oc"> c </em> </strong>增长，那么<strong class="mg jd"> <em class="oc"> j </em> </strong>与<strong class="mg jd"> <em class="oc"> k </em> </strong>(对于任意一个<strong class="mg jd"><em class="oc">j</em><em class="oc">c</em></strong>之间的相似度也将按<strong class="mg jd"> <em class="oc">增长。这意味着(在某种程度上)所有的单词向量将按系数<strong class="mg jd"> <em class="oc"> c </em> </strong>放大/缩小，这不会有任何损害，因为相对的拓扑被保留了。</em></strong></p><p id="d8e8" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">继续，如果我们假设<strong class="mg jd"> <em class="oc"> F=exp，</em> </strong>满足上述同态性质。那么让我们开始吧，</p><p id="4820" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd">T45】Exp(w _ I * u _ k)= P _ ik = X _ ik/X _ IT47】</strong></p><p id="b694" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">和</p><p id="265c" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"><em class="oc">w _ I * u _ k = log(X _ ik)—log(X _ I)</em></strong></p><p id="7240" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">接下来，<strong class="mg jd"> <em class="oc"> X_i </em> </strong>独立于<strong class="mg jd"> <em class="oc"> k </em> </strong>，我们把<strong class="mg jd"> <em class="oc"> log(X_i) </em> </strong>移到 LHS，</p><p id="6dcc" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"><em class="oc">w _ I * u _ k+log(X _ I)= log(X _ ik)</em></strong></p><p id="4913" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">注意，如果没有项<strong class="mg jd"> <em class="oc"> log(X_i) </em> </strong>，即<strong class="mg jd"> i </strong>和<strong class="mg jd"> k </strong>可以互换，则上述等式将具有对称性。我们可以添加一个 bias <strong class="mg jd"> <em class="oc"> b_i </em> </strong>来吸收<strong class="mg jd"> <em class="oc"> log(X_i) </em> </strong>并添加另一个<strong class="mg jd"> <em class="oc"> b_k </em> </strong>来恢复对称性。所以，我们会有点创意，用神经网络的说法来表达<strong class="mg jd"> <em class="oc"> log(X_i) </em> </strong>，</p><p id="9c2c" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">w_i* u_k + b_i +b_k= log(X_ik)</p><p id="2c04" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">或者，</p><p id="f033" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">w_i* u_k + b_i +b_k — log(X_ik) = 0</p><p id="9576" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">其中<strong class="mg jd"><em class="oc">b _ I</em></strong><strong class="mg jd"><em class="oc">b _ k</em></strong>是网络的偏差。</p><h1 id="efee" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">定义成本</h1><p id="8206" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">在一个理想的设置中，你有完美的词向量，上面的表达式将是零。换句话说，这就是我们的目标。所以我们将把 LHS 表达式作为我们的成本函数。</p><p id="d5ed" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd"> J(w_i，w _ J)=(w _ I * u _ J+b _ I+b _ J—log(X _ ij))</strong></p><p id="fb3e" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">注意，平方使其成为均方成本函数。对最初的发现没有损害。k 也被 j 代替了。</p><h1 id="2c1d" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">最终成本函数</h1><p id="0bb3" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">但是你的工作并没有到此为止，你还需要解决一个重要的理论问题。思考一下如果<strong class="mg jd"> <em class="oc"> X_ik </em> </strong> = 0 会发生什么。如果你对上面的成本函数做一个小实验，你会看到一个 ML 从业者最讨厌的 3 个字母，即<strong class="mg jd"><em class="oc">【NaN】</em></strong>。因为 log(0)未定义。简单的解决方法是使用被称为拉普拉斯平滑的<strong class="mg jd"> <em class="oc"> log(1+X_ik) </em> </strong>。但是手套纸背后的杰出人物提出了一种更时髦的方法。即引入一个加权函数。</p><p id="fef0" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated"><strong class="mg jd">j = f(x_ij)(w_i^t u _ j+b _ I+b _ j—log(x _ ij))</strong></p><p id="6a84" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">其中<strong class="mg jd"><em class="oc">f(x _ ij)=(x/x_{max})^a</em></strong>if<strong class="mg jd"><em class="oc">x&lt;x _ { max }</em></strong>else<strong class="mg jd"><em class="oc">0</em></strong></p><h1 id="5b9c" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">结论</h1><p id="cbc4" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">一切都结束了。GloVe 是一种利用语料库的全局和局部统计的词向量技术，以便提出使用这两者的原则性损失函数。GloVe 通过解决三个重要问题来做到这一点。</p><ul class=""><li id="8de2" class="or os it mg b mh mx mk my lr ot lv ou lz ov mw ow ox oy oz bi translated">我们没有方程，例如<strong class="mg jd"> <em class="oc"> F(i，j，k) = P_ik/P_jk </em> </strong>，只是一个表达式(即<strong class="mg jd"> <em class="oc"> P_ik/P_jk </em> </strong>)。</li><li id="6411" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated">单词向量是高维向量，然而<strong class="mg jd"> <em class="oc"> P_ik/P_jk </em> </strong>是标量。所以存在维度不匹配。</li><li id="7da5" class="or os it mg b mh pa mk pb lr pc lv pd lz pe mw ow ox oy oz bi translated">涉及三个实体(<strong class="mg jd"> <em class="oc"> i，j </em> </strong>，和<strong class="mg jd"> <em class="oc"> k </em> </strong>)。但是用三个元素计算损失函数可能会很麻烦，需要减少到两个。</li></ul><p id="a285" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">提供了用 Keras 实现手套的代码<a class="ae lh" href="https://github.com/thushv89/exercises_thushv_dot_com/blob/master/glove_light_on_math_ml/glove_light_on_math_ml.ipynb" rel="noopener ugc nofollow" target="_blank">【此处为</a>】</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="351b" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">如果你喜欢我分享的关于数据科学和机器学习的故事，考虑成为会员吧！</p><div class="pg ph gp gr pi pj"><a href="https://thushv89.medium.com/membership" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">通过我的推荐链接加入媒体</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">thushv89.medium.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px lb pj"/></div></div></a></div></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="a965" class="nc lj it bd lk nd ok nf ln ng ol ni lq ki om kj lu kl on km ly ko oo kp mc nm bi translated">想在深度网络和 TensorFlow 上做得更好？</h1><p id="241a" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">检查我在这个课题上的工作。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/c902b07566ddcbe9ec0bc8a9c98954cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVW0Dql9IQhFYMY7JLG7YA.png"/></div></div></figure><p id="c3f0" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">[1] <a class="ae lh" href="https://www.manning.com/books/tensorflow-in-action" rel="noopener ugc nofollow" target="_blank">(书)TensorFlow 2 在行动——曼宁</a></p><p id="f769" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">[2] <a class="ae lh" href="https://www.datacamp.com/courses/machine-translation-in-python" rel="noopener ugc nofollow" target="_blank">(视频教程)Python 中的机器翻译</a> — DataCamp</p><p id="ddfc" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">[3] <a class="ae lh" href="https://www.amazon.com.au/Natural-Language-Processing-TensorFlow-Ganegedara/dp/1788478312/ref=sr_1_25?dchild=1&amp;keywords=nlp+with+tensorflow&amp;qid=1603009947&amp;sr=8-25" rel="noopener ugc nofollow" target="_blank">(书)TensorFlow 中的自然语言处理 1 </a> — Packt</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="f1d0" class="nc lj it bd lk nd ok nf ln ng ol ni lq ki om kj lu kl on km ly ko oo kp mc nm bi translated">新的！加入我的新 YouTube 频道</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://www.youtube.com/channel/UC1HkxV8PtmWRyQ39MfzmtGA/"><div class="gh gi pz"><img src="../Images/b2af509f5ed9f7c6d9d3c9acc1d6a4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*KOZLLDzT0K-5Ev4l.png"/></div></a></figure><p id="bc00" class="pw-post-body-paragraph me mf it mg b mh mx kd mj mk my kg mm lr mz mo mp lv na mr ms lz nb mu mv mw im bi translated">如果你渴望看到我关于各种机器学习/深度学习主题的视频，请确保加入<a class="ae lh" href="https://www.youtube.com/channel/UC1HkxV8PtmWRyQ39MfzmtGA/" rel="noopener ugc nofollow" target="_blank"> DeepLearningHero </a>。</p><h1 id="d04f" class="nc lj it bd lk nd ne nf ln ng nh ni lq ki nj kj lu kl nk km ly ko nl kp mc nm bi translated">参考:</h1><p id="6840" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm lr mn mo mp lv mq mr ms lz mt mu mv mw im bi translated">[1] GloVe:单词表示的全局向量(<a class="ae lh" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></div></div>    
</body>
</html>