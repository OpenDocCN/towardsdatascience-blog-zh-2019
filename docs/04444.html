<html>
<head>
<title>OpenAI GPT language modeling on Gutenberg with TensorFlow Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 TensorFlow Keras 的古腾堡开放 GPT 语言建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/openai-gpt-language-modeling-on-gutenberg-with-tensorflow-keras-876f9f324b6c?source=collection_archive---------18-----------------------#2019-07-09">https://towardsdatascience.com/openai-gpt-language-modeling-on-gutenberg-with-tensorflow-keras-876f9f324b6c?source=collection_archive---------18-----------------------#2019-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/80493478716f45382852298fc81f9ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wNLLlo8PTFgHb4E6XPLZGA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by Camille Orgel on Unsplash</figcaption></figure><h1 id="1152" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">1.介绍</h1><p id="0a05" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">2018 年对于深度自然语言处理社区来说是不平凡的一年。大型预训练模型，open ai<a class="ae mb" href="https://openai.com/blog/language-unsupervised/" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">【GPT】</strong></a>和<a class="ae mb" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> GPT-2 </strong> </a>和 Google <a class="ae mb" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <strong class="lf iu"> BERT </strong> </a>，在几个监督和非监督任务上取得了 SOTA(“最先进”)的结果。发布这些模型有助于在许多 NLP 任务中获得更好的结果(就像 ImageNet 在视觉领域所做的那样)。</p><p id="90b9" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">我们一直在做的项目(一种讲故事的方式)需要一个强大的语言模型来促进自然语言的生成。本文介绍了在这项任务上所做的努力，包括技术细节和结果。完整代码以回购的形式发布在我们的团队 GitHub 中:</p><div class="mh mi gp gr mj mk"><a href="https://github.com/ShenakhtPajouh/GPT-language-model-tf.keras" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd iu gy z fp mp fr fs mq fu fw is bi translated">ShenakhtPajouh/GPT-语言-模型-tf.keras</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">在 GitHub 上创建一个帐户，为 ShenakhtPajouh/GPT-语言-模型-tf.keras 开发做出贡献。</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">github.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my jz mk"/></div></div></a></div><h1 id="06fd" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">2.动机</h1><p id="7470" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">根据<a class="ae mb" href="https://www.tensorflow.org/guide/keras" rel="noopener ugc nofollow" target="_blank"> Tensorflow 的网站</a>:</p><blockquote class="mz na nb"><p id="0f32" class="ld le nc lf b lg mc li lj lk md lm ln nd me lq lr ne mf lu lv nf mg ly lz ma im bi translated">Keras 是一个高级 API，用于构建和训练深度学习模型。它用于快速原型制作、高级研究和生产，具有三个关键优势:<strong class="lf iu">u<em class="it">ser friendly</em></strong><em class="it">，</em> <strong class="lf iu"> <em class="it">模块化和可组合</em> </strong> <em class="it">，以及</em> <strong class="lf iu"> <em class="it">易于扩展</em> </strong> <em class="it">。</em></p></blockquote><p id="65ac" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">此外，您可以在会话模式和快速执行模式下运行 TensorFlow Keras 模型。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="1c23" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">OpenAI GPT，简称<strong class="lf iu">g</strong>generate<strong class="lf iu">p</strong>re-trained<strong class="lf iu">t</strong>transformer<strong class="lf iu">，</strong>是一个在庞大语料库上训练出来的多层单向<a class="ae mb" href="https://arxiv.org/abs/1801.10198" rel="noopener ugc nofollow" target="_blank"> transformer 解码器</a> ⁴。它的开发目标是通过微调来很好地执行各种任务。OpenAI 于 2018 年 6 月发布了 GPT 的论文、代码和预训练的权重。</p><p id="1a00" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">2019 年 2 月，OpenAI 发布了 GPT-2，这是 GPT 的继任者，其参数超过 10 倍，数据量超过 10 倍。它被训练来预测下一个单词，并在 8 个测试语言建模数据集的 7 个上达到 SOTA 结果，而没有任何特定任务的微调！不可思议，不是吗？为了把樱桃放在最上面，OpenAI 决定不完全发布训练模型，而是发布一个小得多的模型，因为他们担心恶意应用程序 T2。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="d044" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">由于 OpenAI 成就和无法访问 GPT-2 完整模型，并且由于从空白状态学习更难取得成功，我们决定受益于<strong class="lf iu">转移学习</strong>技术<strong class="lf iu"> </strong>并使用 TensorFlow Keras 实现 GPT。</p><h1 id="b4bc" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">3.履行</h1><p id="ce9e" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">原始 GPT 被实现以在语言模型和分类任务上表现良好。因为我们只需要一个语言模型，所以我决定简化它的架构。有一个由<a class="ae mb" href="https://github.com/ceshine" rel="noopener ugc nofollow" target="_blank"> Ceshine Lee </a>设计的重构满足了这个需求。查看他解释原始和修改模型的帖子:</p><div class="mh mi gp gr mj mk"><a href="https://medium.com/the-artificial-impostor/notes-improving-language-understanding-by-generative-pre-training-4c9d4214369c" rel="noopener follow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd iu gy z fp mp fr fs mq fu fw is bi translated">[注意]通过生成性预训练提高语言理解</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">练习:从微调后的模型重建语言模型</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">medium.com</p></div></div><div class="mt l"><div class="nn l mv mw mx mt my jz mk"/></div></div></a></div><p id="9a40" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">上面的帖子包含了对变形金刚、GPT 和修改的详细解释，所以这篇帖子的其余部分将涵盖我所做的其他更改。</p><h2 id="b954" class="no kg it bd kh np nq dn kl nr ns dp kp lo nt nu kt ls nv nw kx lw nx ny lb nz bi translated">3.1.数据迭代器</h2><p id="641e" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我们使用 Guttenberg 数据集来重新训练模型。它包含超过 140，000 个段落。每一个样本都是一段被<a class="ae mb" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank"> BPE </a> ⁵符号化的段落(带着尊重的面具)。所以在<code class="fe oa ob oc od b"><a class="ae mb" href="https://github.com/ShenakhtPajouh/last-sentence-generation-transformer/blob/master/utils.py#L51" rel="noopener ugc nofollow" target="_blank">iter_data</a></code>函数中，段落被打乱并通过小批量返回:</p><figure class="oe of og oh gt ju"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="3444" class="no kg it bd kh np nq dn kl nr ns dp kp lo nt nu kt ls nv nw kx lw nx ny lb nz bi translated">3.2.将张量流模型转换为 tf.keras 模型</h2><p id="76a7" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><a class="ae mb" href="https://github.com/ShenakhtPajouh/last-sentence-generation-transformer/blob/master/model.py" rel="noopener ugc nofollow" target="_blank">整个变压器网络已经改造成 tf.keras </a>。作为网络一部分的每个张量流函数都被重新实现。<code class="fe oa ob oc od b">model</code>、<code class="fe oa ob oc od b">embed</code>、<code class="fe oa ob oc od b">block</code>、<code class="fe oa ob oc od b">attn</code>、<code class="fe oa ob oc od b">mlp</code>、<code class="fe oa ob oc od b">norm</code>、<code class="fe oa ob oc od b">cov1d</code>函数被转换为<code class="fe oa ob oc od b">Transformer</code>、<code class="fe oa ob oc od b">EmbeddingLayer</code>、<code class="fe oa ob oc od b">Block</code>、<code class="fe oa ob oc od b">Attention</code>、<code class="fe oa ob oc od b">MLP</code>、<code class="fe oa ob oc od b">Norm</code>、<code class="fe oa ob oc od b">Conv1D </code>类，这些类是 tf.keras 模型和层。</p><h2 id="c040" class="no kg it bd kh np nq dn kl nr ns dp kp lo nt nu kt ls nv nw kx lw nx ny lb nz bi translated">3.3.添加自回归模块</h2><p id="142e" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">开发 GPT 的初衷并不是为了成为一个语言生成器。因此，它不具备生成文本的自回归模块。所以我们决定设计一个快速语言生成器，采用贪婪解码(它在每一步选择最有可能的下一个标记)。</p><p id="f6d3" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">在生成的每一步，该模块应该处理并获取下一个令牌的 logits，选择最可能的令牌，并在下一步为新序列执行该过程。</p><p id="f972" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">获取下一个令牌的 logits 的一个简单方法是传递整个令牌序列(带有生成令牌的早期令牌)。但是太慢了！<em class="nc">为什么？</em>因为它在每一步都有一个重复的操作:对于每一个<em class="nc"> i，j &lt; k </em>，计算<em class="nc"> i </em>和<em class="nc"> j </em>之间的注意力来预测第<em class="nc"> k </em>个令牌。</p><p id="0be7" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">我们的方法简单地省略了这个重复的操作。我用<code class="fe oa ob oc od b">mem_k</code>和<code class="fe oa ob oc od b">mem_v</code>来记忆键和值矩阵。首先，<a class="ae mb" href="https://github.com/ShenakhtPajouh/last-sentence-generation-transformer/blob/master/init_model.py" rel="noopener ugc nofollow" target="_blank"> init_model </a>对输入序列进行预处理，将关键字和值存储在<code class="fe oa ob oc od b">mem_k</code>和<code class="fe oa ob oc od b">mem_v</code>中，并选择下一个令牌。</p><figure class="oe of og oh gt ju"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="7b32" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">然后在每次迭代中，<a class="ae mb" href="https://github.com/ShenakhtPajouh/last-sentence-generation-transformer/blob/master/gen_model.py" rel="noopener ugc nofollow" target="_blank"> gen_model </a>获得查询、密钥和前一个令牌的值。然后将关键字和查询添加到<code class="fe oa ob oc od b">mem_k</code>和<code class="fe oa ob oc od b">mem_v</code>，并计算关注度。</p><figure class="oe of og oh gt ju"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="4acd" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">有一些方法可以根据其分布生成下一个令牌。最简单的方法是<strong class="lf iu">贪婪解码</strong>。我们在每一步选择最可能的记号。<strong class="lf iu">波束搜索</strong>是另一种试图解决贪婪解码问题的著名技术。但是他们都有相同的内在问题，导致他们无法产生人类文本。(在这篇<a class="ae mb" href="https://arxiv.org/abs/1904.09751" rel="noopener ugc nofollow" target="_blank">论文</a>中，⁶解释了这些方法的缺点)</p><p id="516e" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">因此，在一些论文中，这两种方法被基于抽样的替代方法所取代。在这些方法中，在每一步，我们从下一个令牌分布中进行采样。我们实现了两个强大的解码器，称为<strong class="lf iu"> top-k 采样</strong>和<strong class="lf iu"> nucleus(top-p) </strong>采样(【2】、【6】、【7】)。这确实提高了我们模型的世代。</p><p id="9f5b" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">您可以在<a class="ae mb" href="https://github.com/ShenakhtPajouh/GPT-language-model-tf.keras/blob/master/utils.py#L126" rel="noopener ugc nofollow" target="_blank">实用程序</a>中找到解码器的代码:</p><figure class="oe of og oh gt ju"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="f31c" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">4.重新训练模型</h1><p id="5ada" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我用<a class="ae mb" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Adam optimizer </a>尝试了几种常用的训练方法，都失败了。然后，根据 ULMFiT 的论文，我做了一些改变:</p><ul class=""><li id="4387" class="ok ol it lf b lg mc lk md lo om ls on lw oo ma op oq or os bi translated">亚当换成梯度下降。亚当技术极有可能让你的模型忘记预先训练好的权重。</li><li id="35fd" class="ok ol it lf b lg ot lk ou lo ov ls ow lw ox ma op oq or os bi translated">我还使用了倾斜的三角形学习率(STLR)。它在一小部分训练步骤中线性地增加了学习率。然后，它在剩余步骤中线性降低学习速率。如论文中所述，“<em class="nc">我们希望模型在训练开始时快速收敛到参数空间的合适区域，然后优化其参数。</em>”于是 STLR 提出了。经过一些失败之后，我为 STLR 找到了一些好的超参数:</li></ul><figure class="oe of og oh gt ju"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="6744" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">在和 STLR 一起训练后，我用一个简单的非线性衰减学习率继续训练:<code class="fe oa ob oc od b">lr -= lr * (1 / t ** 0.5)</code></p><h1 id="b457" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">5.结果</h1><p id="3584" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">下面的学习曲线清楚地显示了 STLR 技术的效率。(因为仅在 2400 步之后，列车困惑度就显著下降)</p><figure class="oe of og oh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/e3e84f38ea651765fddfd2c3489c77b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBPORbPMv0qk-1oI-H7VSQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">train ppl in 30,000 steps.</figcaption></figure><p id="c14a" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">验证集的复杂度从 130.21 降低到 22.57。</p><figure class="oe of og oh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/14a2e626a9d541ec0d5a9b29845317e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROcrvYK0TN8ww8KdGjBmdQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">validation ppl</figcaption></figure><h1 id="90b2" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">6.承认</h1><p id="d66a" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我衷心感谢实现了一些 Transformer 模块的茯苓 Yavari 和实现采样解码器的 Mehrnaz Mofakhami。</p><h1 id="f11f" class="kf kg it bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">7.参考</h1><p id="e169" class="pw-post-body-paragraph ld le it lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">[1] <a class="ae mb" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">利用无监督学习提高语言理解</em>作者:亚历克·拉德福德、卡蒂克·纳拉西姆汉、蒂姆·萨利曼斯和伊利亚·苏茨基弗</a></p><p id="a67c" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">[2] <a class="ae mb" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nc">更好的语言模型及其含义</em>作者:亚历克·拉德福德、杰夫·吴、雷文·柴尔德、大卫·栾、达里奥·阿莫代伊和伊利亚·苏茨基弗</a></p><p id="079e" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">[3] <a class="ae mb" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <em class="nc"> BERT:用于语言理解的深度双向转换器的预训练</em>作者:Jacob Devlin，张明蔚，Kenton Lee，Kristina Toutanova </a></p><p id="48c2" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">[4] <a class="ae mb" href="https://arxiv.org/abs/1801.10198" rel="noopener ugc nofollow" target="_blank"> <em class="nc">通过总结长序列生成维基百科</em>作者:Peter J. Liu，Mohammad Saleh，Etienne Pot，Ben Goodrich，Ryan Sepassi，Lukasz Kaiser，Noam Shazeer </a></p><p id="0e61" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">[5] <a class="ae mb" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank"> <em class="nc">由 Rico Sennrich，Barry Haddow，Alexandra Birch </em></a>编写的具有子词单元的罕见词的神经机器翻译</p><p id="8fe6" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated"><a class="ae mb" href="https://arxiv.org/abs/1904.09751" rel="noopener ugc nofollow" target="_blank"><em class="nc"/>阿里·霍尔茨曼著《神经文本退化的奇特案例》</a></p><p id="4c67" class="pw-post-body-paragraph ld le it lf b lg mc li lj lk md lm ln lo me lq lr ls mf lu lv lw mg ly lz ma im bi translated">[7] <a class="ae mb" href="https://arxiv.org/abs/1805.04833" rel="noopener ugc nofollow" target="_blank"> <em class="nc">分层次的神经故事生成</em>由安吉拉·范、、扬·王太子</a></p></div></div>    
</body>
</html>