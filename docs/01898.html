<html>
<head>
<title>Attention and its Different Forms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意及其不同形式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc?source=collection_archive---------0-----------------------#2019-03-29">https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc?source=collection_archive---------0-----------------------#2019-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e266" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">概括注意的不同类型和用途。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e807d9f3612db858cf08afd9e484cf1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Z5HCxumPblmIGkbAv00Ag.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://unsplash.com/photos/tEVGmMaPFXk" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="f4c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我假设你已经熟悉递归神经网络(包括 seq2seq 编码器-解码器架构)。</p><h1 id="fc7c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">瓶颈问题</strong></h1><p id="e735" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在编码器-解码器架构中，完整的信息序列必须由单个向量捕获。这给在序列开始时保留信息和编码长程相关性带来了问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/2ce39a6b64652e1f4ee4205fb80ccd44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ymcqS8-rNq42VhmtdR4ooQ.png"/></div></div></figure><p id="a04a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意力的核心思想是关注每个输出的输入序列中最相关的部分。<br/>通过为输入提供一条直接的路径，注意力也有助于缓解渐变消失的问题。</p><h1 id="a746" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">计算注意力</h1><p id="7a6b" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">假设您有一个顺序解码器，但是除了前一个单元的输出和隐藏状态之外，您还要输入一个上下文向量<strong class="ky ir"> c </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/de9cb9c63590034c5d68f677b2fcd9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*We87td0yKdnI_pDGkyvS1g.png"/></div></figure><p id="5744" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> c </strong>是编码器隐藏状态的加权和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ad13979b022e982fb109cf19b3534c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*1AJ98ylreRRzou_J9qOKHQ.png"/></div></figure><p id="358b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里<strong class="ky ir"> αᵢⱼ </strong>是第<em class="ls"> i </em>个输出应该关注第<em class="ls"> j </em>个输入的量，而<strong class="ky ir"> hⱼ </strong>是第<em class="ls"> j </em>个输入的编码器状态。</p><p id="411b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> αᵢⱼ </strong>通过对输入相对于<em class="ls"> i </em> th 输出的注意力分数取软最大值来计算，用<strong class="ky ir"> e、</strong>表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/989855e322dd28c1a68110b1f3d62ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*Bg3uFg-i_9TIr4PBBKVEOA.png"/></div></figure><p id="362c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0f3a5062d8ee65a13b6426f79181ba48.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*TGQdOX3w9IsJNR0XyaVWUQ.png"/></div></figure><p id="bc30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里<strong class="ky ir"> f </strong>是对齐模型，其对位置<em class="ls"> j </em>周围的输入和位置<em class="ls"> i </em>处的输出的匹配程度进行评分，并且<strong class="ky ir"> sᵢ₋₁ </strong>是来自前一时间步的隐藏状态。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d4dc8ead76f9b98742e7571475a58abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*CldamnPNJgi-xkt-SKjDwg.png"/></div></figure><p id="cd48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对齐模型可以通过小型神经网络来近似，然后可以使用任何梯度优化方法(例如梯度下降)来优化整个模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/2190fd363d6b896c0e56c04609811fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5gMnFBG_0omdHPl7oj3LQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Graphic illustration of the attention mechanism (<a class="ae kv" href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="539c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上下文向量<strong class="ky ir"> cᵢ </strong>也可以用于计算解码器输出<strong class="ky ir"> yᵢ </strong>。</p><h2 id="b694" class="mx lu iq bd lv my mz dn lz na nb dp md lf nc nd mf lj ne nf mh ln ng nh mj ni bi translated">应用:机器翻译</h2><p id="8274" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">注意力首先由 Bahdanau 等人[1]提出，用于神经机器翻译。该机制对于机器翻译特别有用，因为与输出最相关的单词经常出现在输入序列中的相似位置。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/174ddd00f7ba2106f17e3ad2c047886e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*_cWv-in-l9VBx0BQf-PsCA.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="eb35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的矩阵显示了与每个翻译输出单词最相关的输入单词。<br/>这样的注意力分布也有助于为模型提供一定程度的可解释性。</p><h1 id="fdfd" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">普遍注意力</h1><p id="6061" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">给定一个查询<strong class="ky ir"> q </strong>和一组键-值对<strong class="ky ir"> (K，V) </strong>，注意力可以概括为计算依赖于查询和相应键的值的加权和。<br/>查询确定关注哪些值；我们可以说查询“关注”了这些值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/33f1596683f791b974c6db0043aafcce.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*48ew5TFT26OOjJnAsrkIEg.png"/></div></figure><p id="53a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在前面的计算中，查询是前一个隐藏状态<strong class="ky ir"> sᵢ₋₁ </strong>，而编码器隐藏状态集<strong class="ky ir"> h₀ </strong>到<strong class="ky ir"> hₙ </strong>表示键和值。</p><p id="db0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">反过来，对准模型可以以各种方式计算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/1cf35c01caa133e909256cf7e3183b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HscK_oEZqt20chNz58K5Og.png"/></div></div></figure><h1 id="f67c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">自我关注</h1><p id="59b4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">有了自我关注，每一个隐藏状态都会关注同一个 RNN 的先前隐藏状态。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f5dfd8a6325440758b2bcc19f604da19.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*c5lLFJI3LPNPJIm7t4rPNg.png"/></div></figure><p id="b410" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里<strong class="ky ir"> sₜ </strong>是查询，而解码器隐藏状态<strong class="ky ir"> s₀ </strong>到 s <strong class="ky ir"> ₜ₋₁ </strong>表示键和值。</p><h2 id="e138" class="mx lu iq bd lv my mz dn lz na nb dp md lf nc nd mf lj ne nf mh ln ng nh mj ni bi translated">应用:语言建模</h2><p id="af0e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">论文“指针哨兵混合模型”[2]使用自我注意进行语言建模。</p><p id="0eb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本思想是，细胞的输出“指向”先前遇到的具有最高注意力分数的单词。然而，该模型还在词汇 V 上使用标准的 softmax 分类器，使得它除了从最近的上下文中再现单词之外，还可以预测不存在于输入中的输出单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/baf2f4192f3b0e012999359933f8c85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mM7SDJlK6xGcSBuFO5VsWA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://arxiv.org/pdf/1609.07843.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="56de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在指针词汇表分布中分配给给定单词的概率是分配给给定单词出现的所有标记位置的概率之和</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2c0a93ed9816f7d89851a9d52fbec78a.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*roTT0SnE8Cv6zIWhxthUsg.png"/></div></figure><p id="137a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> <em class="ls"> I(w，x) </em> </strong>在输入<strong class="ky ir"> x </strong>和<strong class="ky ir"> pₚₜᵣ∈ Rⱽ </strong>中得到单词<strong class="ky ir"> w </strong>的所有位置。这种技术被称为指针和注意力。</p><p id="b842" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型使用门<strong class="ky ir"> g </strong>将 softmax 词汇分布与指针词汇分布相结合，该门被计算为查询和标记向量的乘积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9e91c4b22f4e07516d79fe7ccd781afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*bjwRPbMpRag1TLtM2JpEOQ.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://arxiv.org/pdf/1609.07843.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/880fd6e5c1952ff6dba94d97adc85123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tr7OLxn6C5hzncq0YzqZ8g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">An output example (<a class="ae kv" href="https://arxiv.org/pdf/1609.07843.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><h2 id="5d98" class="mx lu iq bd lv my mz dn lz na nb dp md lf nc nd mf lj ne nf mh ln ng nh mj ni bi translated">申请:总结</h2><p id="ed58" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">论文“用于抽象概括的深度强化模型”[3]介绍了一种具有新颖的自我关注的神经网络模型，该模型分别关注输入和连续生成的输出，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/2c53ac979d4e12ed544ce170669a4d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5VKCgYAANxIUOEMa8AZy2A.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://arxiv.org/pdf/1705.04304.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="898c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所涉及的计算可以总结如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/11431be40d36c96e6635799d71a809b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OcP-U-jB3N9QUDhG0aaveQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture11.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/1918e627a062907ada0dde07bec67d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v0odTWd1i4riPCSvUDrVlA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">An output example (<a class="ae kv" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture11.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><h1 id="4109" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">多头注意力</h1><h2 id="553c" class="mx lu iq bd lv my mz dn lz na nb dp md lf nc nd mf lj ne nf mh ln ng nh mj ni bi translated"><strong class="ak">多重查询</strong></h2><p id="6b5a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">当我们有多个查询时，我们可以把它们堆在一个矩阵中。</p><p id="7626" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们使用基本的点积注意力来计算对齐，则用于计算上下文向量的方程组可以简化如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/8684ead0434a13ae6d0b2bc1d80f15c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zit9LXszcZbxKHl0reGGLA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture12.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="f3cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多头关注更进了一步。</p><p id="90ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用权重矩阵将 q、K 和 V 映射到较低维度的向量空间，然后将结果用于计算注意力(其输出我们称为“头部”)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/67f76e9adb27c25c4e4903b7f0ff5d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*YEFoFyJQeOJ4-yBy4-CKwg.jpeg"/></div></figure><p id="3b55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有 h 组这样的权重矩阵，这给了我们 h 个头。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/26c419022f19d94057f4c396f79b7e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*_W5aqxN4J_DO6FQeihvFHg.jpeg"/></div></figure><p id="a3ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，使用输出权重矩阵连接和变换 h 个头。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2b7a2d1e9fb1ec120f4b870b9cecd868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*qEIXouqnDD0cNQTFcU1Qkg.jpeg"/></div></figure><h1 id="d3dd" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">变形金刚(电影名)</h1><p id="d915" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">变压器是在论文“你所需要的只是注意力”中首次提出的[4]。它基于这样一种思想，即序列模型可以完全省去，输出可以仅使用注意机制来计算。</p><p id="d366" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">转换器使用单词向量作为键、值以及查询的集合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/7367aa1342691b891cca96ed0eef054d.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*duAo5Uf9GndoMx-2ssLxHg.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Transformer’s Multi-Head Attention block (<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="4a35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它包含多头注意力的块，而注意力计算本身是成比例的点积注意力。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1da528c02e7176e4674873add8df2666.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*Zcu2qz2sq9fjUWP2DY2mgg.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="b293" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> dₖ </strong>是查询/关键向量的维度。</p><p id="3780" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">执行缩放是为了使 softmax 函数的参数不会因更高维的键而变得过大。</p><p id="efef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是完整的变压器模型图，以及一些带有附加细节的注释。有关更深入的解释，请参考附加资源。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/d4c257e2086a12efc6fb6020f1f261aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1zCjtNRpZ33RB7iP8GFSDA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="5097" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">额外资源</h1><ul class=""><li id="a3a3" class="oi oj iq ky b kz ml lc mm lf ok lj ol ln om lr on oo op oq bi translated">deeplearning.ai 的<a class="ae kv" href="https://www.youtube.com/watch?v=SysgYptB198" rel="noopener ugc nofollow" target="_blank">注意力模型直觉(C5W3L07) </a>和<a class="ae kv" href="https://www.youtube.com/watch?v=quoGRI-1l0A" rel="noopener ugc nofollow" target="_blank">注意力模型(C5W3L08) </a></li><li id="b8cd" class="oi oj iq ky b kz or lc os lf ot lj ou ln ov lr on oo op oq bi translated"><a class="ae kv" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">“注意？注意！”由莉莲翁</a></li><li id="c931" class="oi oj iq ky b kz or lc os lf ot lj ou ln ov lr on oo op oq bi translated"><a class="ae kv" href="https://distill.pub/2016/augmented-rnns/" rel="noopener ugc nofollow" target="_blank">《注意力和增强的递归神经网络》作者 Olah &amp; Carter，Distill，2016 </a></li><li id="007e" class="oi oj iq ky b kz or lc os lf ot lj ou ln ov lr on oo op oq bi translated"><a class="ae kv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《图解变形金刚》杰伊·阿拉玛</a></li></ul><h1 id="02b6" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="376c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">[1] <a class="ae kv" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> D. Bahdanau，K. Cho 和 Y. Bengio，通过联合学习对齐和翻译进行神经机器翻译(2014) </a></p><p id="d281" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] <a class="ae kv" href="https://arxiv.org/abs/1609.07843" rel="noopener ugc nofollow" target="_blank"> S. Merity，C. Xiong，J. Bradbury 和 R. Socher，Pointer Sentinel 混合模型(2016) </a></p><p id="138d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] <a class="ae kv" href="https://arxiv.org/abs/1705.04304" rel="noopener ugc nofollow" target="_blank"> R. Paulus，C. Xiong，R. Socher，抽象概括的深度强化模型(2017) </a></p><p id="ac6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] <a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> A .瓦斯瓦尼、n .沙泽尔、n .帕马尔、j .乌兹科雷特、l .琼斯、A. N .戈麦斯、l .凯泽、I .波罗苏欣，《你只需要关注》(2017) </a></p></div></div>    
</body>
</html>