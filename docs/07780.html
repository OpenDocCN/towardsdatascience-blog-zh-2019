<html>
<head>
<title>Face Alignment: Deep multi-task learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人脸对齐:深度多任务学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/face-allignment-deep-multi-task-learning-203f46a22106?source=collection_archive---------11-----------------------#2019-10-28">https://towardsdatascience.com/face-allignment-deep-multi-task-learning-203f46a22106?source=collection_archive---------11-----------------------#2019-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="99b7" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> 1。简介</strong></h1><p id="0c6c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">面部关键点预测:给定一张面部图片，预测各种面部特征的位置。</p><p id="e0e5" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这个问题属于计算机视觉的范畴，并伴随着它自身的挑战。</p><p id="d24f" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在这方面已经做了很多工作。在本文中，我们将深入研究这篇<a class="ae lo" href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> <em class="lp">论文</em> </strong> </a>提出的独特方法，并从头实现它，全部使用 keras。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/901ccdc173a49a111e762822822d181b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lgEM901BbnX8BMFVJ6VGNA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Deep Multi-task learning</figcaption></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c86d" class="jn jo iq bd jp jq mn js jt ju mo jw jx jy mp ka kb kc mq ke kf kg mr ki kj kk bi translated"><strong class="ak"> 2。为什么我们需要解决它？它的应用是什么？</strong></h1><p id="4bec" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你有没有用过 Snapchat，专门试用过他们的图像滤镜？它是如何变得如此神奇的？它是如何如此准确地用假胡子代替你的胡子的？</p><p id="c372" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">要做到这一点，首先它需要识别你脸上与胡子相对应的部分。然后切掉一部分(当然是内部的)并用人造的替换掉。</p><p id="6d8b" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这就是面部关键点检测发挥作用的地方。识别面部的不同部分。</p><p id="c3d0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这只是一个具体的应用，还有很多这样的应用。查看<a class="ae lo" href="https://www.learnopencv.com/facial-landmark-detection/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>了解更多信息。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="1c6c" class="jn jo iq bd jp jq mn js jt ju mo jw jx jy mp ka kb kc mq ke kf kg mr ki kj kk bi translated">3.数据概述</h1><p id="040d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这个项目的数据集由论文作者自己提供，可以在<a class="ae lo" href="http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">这里</strong> </a>找到。</p><p id="8e6d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">数据总共有 12295 幅图像，其中 10000 幅是训练图像，2295 幅是测试图像。</p><p id="46c5" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">数据还附带了两个 txt 文件:<code class="fe ms mt mu mv b">training.txt</code>和<code class="fe ms mt mu mv b">testing.txt</code>。这两个文件保存了关于图像路径、面部特征的坐标位置和 4 个其他面部属性的信息:</p><p id="4df0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">第一属性:性别【男/女】</strong></p><p id="a9fa" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">第二属性:微笑/不微笑</strong></p><p id="11ad" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">第三属性:戴眼镜/不戴眼镜</strong></p><p id="fb67" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">第四属性:姿态变化</strong></p><p id="f9f8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> 3.1 加载和清理数据</strong></p><p id="674c" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们加载 training.txt 文件，并尝试理解和分析数据。当你使用<code class="fe ms mt mu mv b">pandas read_csv</code>函数读取 training.txt 文件时，使用空格作为分隔符，它不会被正确加载，这是因为每行的开头都有空格。所以，我们需要把它去掉。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mw"><img src="../Images/e567137330473431664c2d2e11a51890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mi5n9uYtbqBwlSyarcbNbg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Training.txt file</figcaption></figure><p id="dcf8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">下面的代码将完全做到这一点。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="315a" class="nb jo iq mv b gy nc nd l ne nf">f = open('training.txt','r')<br/>f2 = open('training_new.txt','w')<br/>for i,line in enumerate(f.readlines()):<br/>    if i==0:<br/>        continue<br/>    line = line.strip()<br/>    <br/>    f2.write(line)<br/>    f2.write('\n')<br/>f2.close()<br/>f.close()</span></pre><p id="95a2" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，我们将在项目中使用这个新创建的文件<code class="fe ms mt mu mv b">training_new.txt</code>。对于<code class="fe ms mt mu mv b">testing.txt</code>文件也是如此。</p><p id="6ada" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">读取已清理的<code class="fe ms mt mu mv b">training.txt</code>文件。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="0e14" class="nb jo iq mv b gy nc nd l ne nf">names = ['Path']+list('BCDEFGHIJK')+['Gender','Smile','Glasses','Pose']</span><span id="c9e8" class="nb jo iq mv b gy ng nd l ne nf">train = pd.read_csv('training_new.txt',sep=' ',header=None,names=names)</span><span id="dd46" class="nb jo iq mv b gy ng nd l ne nf"><br/>train['Path'] = train['Path'].str.replace('\\','/')</span></pre><p id="ae61" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">下面是培训文件中每个属性的含义。</p><ul class=""><li id="3f0c" class="nh ni iq kn b ko lj ks lk kw nj la nk le nl li nm nn no np bi translated">路径:图像的路径(绝对路径)</li><li id="b586" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">b:右眼中心的 x 坐标</li><li id="e739" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">c:左眼中心的 x 坐标</li><li id="69b8" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">鼻中心的 D: x 坐标</li><li id="61e2" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">e:嘴部最右侧点的 x 坐标</li><li id="b01b" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">f:嘴部最左侧点的 x 坐标</li><li id="ea6b" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">右眼中心的 G: y 坐标</li><li id="3369" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">左眼中心的 H: y 坐标</li><li id="ea56" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">鼻中心的 I: y 坐标</li><li id="babc" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">嘴部最右端点的 y 坐标</li><li id="2995" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">嘴巴最左边的 y 坐标</li><li id="847e" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">性别:此人是男是女，1:男，2:女</li><li id="c8bf" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">微笑:不管对方是否微笑，1:微笑，2:不微笑</li><li id="bf5b" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">眼镜:这个人是否戴眼镜，1:戴眼镜，2:不戴眼镜</li><li id="1087" class="nh ni iq kn b ko nq ks nr kw ns la nt le nu li nm nn no np bi translated">姿势:【姿势估计】，5 类。</li></ul><p id="af67" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> 3.2 可视化数据</strong></p><p id="8c77" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，让我们想象一些带有面部关键点的图像。</p><p id="bf28" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">代码:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="68eb" class="nb jo iq mv b gy nc nd l ne nf">#visualising the dataset<br/>images = []<br/>all_x = []<br/>all_y= []<br/>random_ints = np.random.randint(low=1,high=8000,size=(9,))<br/>for i in random_ints:<br/>    img = cv2.imread(train['Path'].iloc[i])<br/>    x_pts = train[list('BCDEF')].iloc[i].values.tolist()<br/>    y_pts = train[list('GHIJK')].iloc[i].values.tolist()<br/>    all_x.append(x_pts)<br/>    all_y.append(y_pts)<br/>    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)<br/>    images.append(img)</span><span id="1351" class="nb jo iq mv b gy ng nd l ne nf">fig,axs = plt.subplots(nrows=3,ncols=3,figsize=(14,10))</span><span id="5ff3" class="nb jo iq mv b gy ng nd l ne nf">k =0<br/>for i in range(0,3):<br/>    for j in range(0,3):<br/>        axs[i,j].imshow(images[k])<br/>        axs[i,j].scatter(all_x[k],all_y[k])<br/>        k += 1</span><span id="971a" class="nb jo iq mv b gy ng nd l ne nf">plt.show()</span></pre><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nv"><img src="../Images/182b3d562f104267482cfb3ad1205406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vyyiY9w6XuEwz_l8eZT7MQ.png"/></div></div></figure><h1 id="b0bc" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> 4。深潜</strong></h1><p id="acfc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，我们知道面部关键点预测是怎么回事了。让我们深入了解一下它的技术细节。</p><blockquote class="nw nx ny"><p id="eb04" class="kl km lp kn b ko lj kq kr ks lk ku kv nz ll ky kz oa lm lc ld ob ln lg lh li ij bi translated">接受图像作为输入，并给出面部特征的坐标。</p></blockquote><p id="c1f3" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这是一个回归问题，因为它预测连续值，即面部标志的坐标。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oc"><img src="../Images/bb8e1a1beaa8d1eda778f76887aa89fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dhrEhGRS1aOtpve9ryD7g.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Face Alignment, Magic box?</figcaption></figure><p id="3b78" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">盒子里有什么神奇的东西能做这些？</p><p id="6596" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们更深入地了解它。</p><p id="c612" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">到目前为止，有两种方法可以解决这个问题，一种是普通的计算机视觉技术(如 viola 和 jones 的人脸包围盒预测)，另一种是基于深度学习的，特别是基于卷积神经网络的。</p><p id="9fdd" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">但是这个卷积神经网络到底是个什么鬼？</strong></p><p id="3df9" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">简单地说，它是用来从图像中提取和检测有意义信息的技术。如果你有兴趣了解更多，请点击这里的<a class="ae lo" href="https://bit.ly/2ABv6Ho" rel="noopener ugc nofollow" target="_blank"><strong class="kn ir"/></a>。</p><p id="71b6" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">对于这个问题，我们将采取第二条路线，即基于深度学习。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="f2ec" class="jn jo iq bd jp jq mn js jt ju mo jw jx jy mp ka kb kc mq ke kf kg mr ki kj kk bi translated">5.<strong class="ak">文献综述</strong></h1><p id="9a0e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">不同的研究人员在这个领域做了大量的工作。围绕这个问题的大部分工作把它作为一个单一的任务问题，他们试图单独解决这个问题。但是<a class="ae lo" href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">这篇研究论文</strong> </a>提出了一个有趣的想法，那就是，他们把它作为一个深层的多任务问题。</p><p id="82e8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">什么是多任务问题？</p><p id="1e5b" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">多任务问题:与其只解决一个主要问题，不如一起解决相关的多个问题。</p><p id="3549" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">不要只解决面部标志检测问题，让我们也解决相关的辅助问题，比如:图像中的人是否相似，这个人的性别，等等..</p><p id="6d1d" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">但是为什么要一起解决多个任务呢？ <br/>上述论文的作者注意到关于面部标志检测(主要任务)的一个非常关键的细节，即面部标志的位置高度依赖于人是否在微笑、图像中人的姿势以及其他支持的任务。因此，他们引入了深度多任务学习的概念，并发现它对于这个问题是准确的。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c49e" class="jn jo iq bd jp jq mn js jt ju mo jw jx jy mp ka kb kc mq ke kf kg mr ki kj kk bi translated">6.履行</h1><p id="b7fc" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果你已经熟悉深度学习，到现在，你应该知道这是一个多输出问题，因为我们试图同时解决这个多任务。由于我们将使用<strong class="kn ir"> keras </strong>来实现，所以多输出模型可以通过<strong class="kn ir">功能 API 来实现，而不是顺序 API。</strong></p><p id="732c" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">根据数据，我们手头有 5 个任务，其中面部对齐是主要的一个。因此，我们将使用多输出模型来训练这 5 项任务的模型。</p><p id="5104" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">我们将用不同的辅助任务来训练主任务(人脸对齐)，以评估深度多任务学习的有效性。</strong></p><p id="981b" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><em class="lp">第一个模型:面部对齐+所有其他辅助任务(4) </em></p><p id="b6da" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><em class="lp">第二种模式:脸部对齐+性别+微笑+眼镜</em></p><p id="22df" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><em class="lp">第三种模型:人脸对齐+姿态估计</em></p><p id="1106" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><em class="lp">第四种模式:仅面部对齐</em></p><p id="7f69" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir">网络架构</strong></p><p id="76bf" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">我们将使用四个卷积层、三个最大池层、一个密集层，以及用于所有任务的独立输出层。除了图像的输入形状之外，网络架构与本文作者实现的网络架构相同。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi od"><img src="../Images/deb5cb48848c304ad64fc9e8ef19e0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ls7P5t2S3yi9WZwdneEltA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Network architecture. <a class="ae lo" href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="2d65" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> 6.1 第一个模型的实施</strong></p><p id="8394" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">代码如下:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="c247" class="nb jo iq mv b gy nc nd l ne nf">inp = Input(shape=(160,160,3))</span><span id="402a" class="nb jo iq mv b gy ng nd l ne nf">#1st convolution pair<br/>conv1 = Conv2D(16,kernel_size=(5,5), activation='relu')(inp)<br/>mx1 = MaxPooling2D(pool_size=(2,2))(conv1)</span><span id="d5da" class="nb jo iq mv b gy ng nd l ne nf">#2nd convolution pair<br/>conv2 = Conv2D(48,kernel_size=(3,3), activation='relu')(mx1)<br/>mx2 = MaxPooling2D(pool_size=(2,2))(conv2)</span><span id="ef60" class="nb jo iq mv b gy ng nd l ne nf">#3rd convolution pair<br/>conv3 = Conv2D(64,kernel_size=(3,3), activation='relu')(mx2)<br/>mx3 = MaxPooling2D(pool_size=(2,2))(conv3)</span><span id="04ef" class="nb jo iq mv b gy ng nd l ne nf">#4th convolution pair<br/>conv4 = Conv2D(64,kernel_size=(2,2), activation='relu')(mx3)</span><span id="a184" class="nb jo iq mv b gy ng nd l ne nf">flt = Flatten()(conv4)</span><span id="ccee" class="nb jo iq mv b gy ng nd l ne nf">dense = Dense(100,activation='relu')(flt)</span><span id="d7e5" class="nb jo iq mv b gy ng nd l ne nf">reg_op = Dense(10,activation='linear',name='key_point')(dense)</span><span id="cff2" class="nb jo iq mv b gy ng nd l ne nf">gndr_op = Dense(2,activation='sigmoid',name='gender')(dense)</span><span id="c66d" class="nb jo iq mv b gy ng nd l ne nf">smile_op = Dense(2,activation='sigmoid',name='smile')(dense)</span><span id="be23" class="nb jo iq mv b gy ng nd l ne nf">glasses_op = Dense(2,activation='sigmoid',name='glasses')(dense)</span><span id="45e4" class="nb jo iq mv b gy ng nd l ne nf">pose_op = Dense(5,activation='softmax',name='pose')(dense)</span><span id="7bcb" class="nb jo iq mv b gy ng nd l ne nf">model = Model(inp,[reg_op,gndr_op,smile_op,glasses_op,pose_op])</span><span id="e2e0" class="nb jo iq mv b gy ng nd l ne nf">model.summary()</span></pre><p id="0ee5" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这将打印出以下输出:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oe"><img src="../Images/81bbebbe683fea9aa969252559f4996f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aigPJx2hEti6qkzfHzpZow.png"/></div></div></figure><p id="6fc2" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">现在，下一步是提及我们将在 keras 中为每个输出使用的损失函数。这很容易弄清楚。我们将对面部关键点使用<strong class="kn ir">均方误差(MSE) </strong>，对性别输出、微笑输出和眼镜输出使用<strong class="kn ir">二元交叉熵</strong>，对姿势输出使用<strong class="kn ir">分类交叉熵</strong>。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="35c1" class="nb jo iq mv b gy nc nd l ne nf">loss_dic = {'key_point':'mse','gender':'binary_crossentropy','smile':'binary_crossentropy', 'glasses':'binary_crossentropy' , 'pose':'categorical_crossentropy'}</span></pre><p id="7de0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">在内部，总损失将是所有单个损失的总和。现在，我们也可以在 keras 中为每个损失函数显式设置权重，得到的损失将是所有单个损失的加权和。</p><p id="3380" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">由于主要任务是关键点检测，因此我们将给予更多的权重。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="0df5" class="nb jo iq mv b gy nc nd l ne nf">loss_weights = {'key_point':7,'gender':2,'smile':4,'glasses':1,'pose':3}</span></pre><p id="3516" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">每项任务的衡量标准是:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="5f8a" class="nb jo iq mv b gy nc nd l ne nf">metrics = {'key_point':'mse','gender':['binary_crossentropy','acc'],'smile':['binary_crossentropy','acc'], 'glasses':['binary_crossentropy','acc'] , 'pose':['categorical_crossentropy','acc']}</span></pre><p id="a639" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">一切就绪。让我们训练网络。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="4aea" class="nb jo iq mv b gy nc nd l ne nf">epochs = 35<br/>bs = 64</span><span id="a631" class="nb jo iq mv b gy ng nd l ne nf">H = model.fit(train_images,[train_keypoint_op,]+train_categorical_ops, epochs = epochs, batch_size=bs, validation_data=(val_images,[val_keypoint_op,]+val_categorical_ops),callbacks=[TrainValTensorBoard(log_dir='./log',write_graph=False)])</span></pre><p id="74d2" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们评价一下 model 的性能。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="faae" class="nb jo iq mv b gy nc nd l ne nf">train_pred = model.predict(train_images)<br/>val_pred = model.predict(val_images)</span><span id="335d" class="nb jo iq mv b gy ng nd l ne nf">print('MSE on train data: ', mean_squared_error(train_keypoint_op,train_pred[0]))<br/>print('MSE on validation data: ', mean_squared_error(val_keypoint_op,val_pred[0]))</span></pre><p id="299a" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">上面的代码片段给出了以下输出:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="cde7" class="nb jo iq mv b gy nc nd l ne nf">MSE on train data:  2.0609966325423565<br/>MSE on validation data:  29.55315040683187</span></pre><p id="7ead" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">可视化验证集上的结果。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi of"><img src="../Images/73d5a40dbaacbffa7803bd31a9d1a678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bXYYkANxX_6ZZx4X9hYZ_g.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Output Result: Model 1</figcaption></figure><p id="0654" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">它仅用 35 个纪元和相当简单的模型架构就能很好地工作。</p><p id="fce1" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> 6.2 第二种模式的实施</strong></p><p id="6d01" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这一个的代码如下:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="9ebb" class="nb jo iq mv b gy nc nd l ne nf">inp = Input(shape=(160,160,3))</span><span id="805d" class="nb jo iq mv b gy ng nd l ne nf">#1st convolution pair<br/>conv1 = Conv2D(16,kernel_size=(5,5), activation='relu')(inp)<br/>mx1 = MaxPooling2D(pool_size=(2,2))(conv1)</span><span id="e8a6" class="nb jo iq mv b gy ng nd l ne nf">#2nd convolution pair<br/>conv2 = Conv2D(48,kernel_size=(3,3), activation='relu')(mx1)<br/>mx2 = MaxPooling2D(pool_size=(2,2))(conv2)</span><span id="b629" class="nb jo iq mv b gy ng nd l ne nf">#3rd convolution pair<br/>conv3 = Conv2D(64,kernel_size=(3,3), activation='relu')(mx2)<br/>mx3 = MaxPooling2D(pool_size=(2,2))(conv3)</span><span id="cccf" class="nb jo iq mv b gy ng nd l ne nf">#4th convolution pair<br/>conv4 = Conv2D(64,kernel_size=(2,2), activation='relu')(mx3)</span><span id="dbfa" class="nb jo iq mv b gy ng nd l ne nf">flt = Flatten()(conv4)</span><span id="d74d" class="nb jo iq mv b gy ng nd l ne nf">dense = Dense(100,activation='relu')(flt)</span><span id="e4f2" class="nb jo iq mv b gy ng nd l ne nf">reg_op = Dense(10,activation='linear',name='key_point')(dense)</span><span id="f100" class="nb jo iq mv b gy ng nd l ne nf">gndr_op = Dense(2,activation='sigmoid',name='gender')(dense)</span><span id="1589" class="nb jo iq mv b gy ng nd l ne nf">smile_op = Dense(2,activation='sigmoid',name='smile')(dense)</span><span id="534c" class="nb jo iq mv b gy ng nd l ne nf">glasses_op = Dense(2,activation='sigmoid',name='glasses')(dense)</span><span id="6268" class="nb jo iq mv b gy ng nd l ne nf">model = Model(inp,[reg_op,gndr_op,smile_op,glasses_op])</span><span id="a427" class="nb jo iq mv b gy ng nd l ne nf">model.summary()</span></pre><p id="7dee" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">编译模型。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="c6d3" class="nb jo iq mv b gy nc nd l ne nf">loss_dic ={'key_point':'mse','gender':'binary_crossentropy','smile':'binary_crossentropy', 'glasses':'binary_crossentropy' }</span><span id="b799" class="nb jo iq mv b gy ng nd l ne nf">loss_weights = {'key_point':2,'gender':1,'smile':4,'glasses':1}</span><span id="80d8" class="nb jo iq mv b gy ng nd l ne nf">metrics = {'key_point':'mse','gender':['binary_crossentropy','acc'],'smile':['binary_crossentropy','acc'], 'glasses':['binary_crossentropy','acc'] }</span><span id="fc1a" class="nb jo iq mv b gy ng nd l ne nf">model.compile(optimizer='adam',loss=loss_dic,loss_weights=loss_weights,metrics=metrics)</span></pre><p id="2058" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">一切就绪。让我们训练网络。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="963d" class="nb jo iq mv b gy nc nd l ne nf">H = model.fit(train_images, [train_keypoint_op,]+train_categorical_ops[:-1], epochs = epochs, batch_size=bs, validation_data=(val_images,[val_keypoint_op,]+val_categorical_ops[:-1]),callbacks=[TrainValTensorBoard(log_dir='./log3',write_graph=False)])</span></pre><p id="b973" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们评价一下 model 的性能。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="5314" class="nb jo iq mv b gy nc nd l ne nf">train_pred = model.predict(train_images)<br/>val_pred = model.predict(val_images)</span><span id="b15d" class="nb jo iq mv b gy ng nd l ne nf">print('MSE on train data: ', mean_squared_error(train_keypoint_op,train_pred[0]))<br/>print('MSE on validation data: ', mean_squared_error(val_keypoint_op,val_pred[0]))</span></pre><p id="991c" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">上面的代码片段给出了以下输出:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="c51c" class="nb jo iq mv b gy nc nd l ne nf">MSE on train data:  2.9205250961752722<br/>MSE on validation data:  35.072992153148434</span></pre><p id="b3ad" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">可视化验证集上的结果。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi og"><img src="../Images/b5e67fd8a5f7f55c1006de293068218f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpqNWxVMpz4FQ_jZI0SmLQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Output Result: Model 2</figcaption></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="2337" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> 6.3 第三种模式的实施</strong></p><p id="6cd3" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这一个的代码如下:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="9364" class="nb jo iq mv b gy nc nd l ne nf">inp = Input(shape=(160,160,3))</span><span id="e467" class="nb jo iq mv b gy ng nd l ne nf">#1st convolution pair<br/>conv1 = Conv2D(16,kernel_size=(5,5), activation='relu')(inp)<br/>mx1 = MaxPooling2D(pool_size=(2,2))(conv1)</span><span id="9a36" class="nb jo iq mv b gy ng nd l ne nf">#2nd convolution pair<br/>conv2 = Conv2D(48,kernel_size=(3,3), activation='relu')(mx1)<br/>mx2 = MaxPooling2D(pool_size=(2,2))(conv2)</span><span id="ba6a" class="nb jo iq mv b gy ng nd l ne nf">#3rd convolution pair<br/>conv3 = Conv2D(64,kernel_size=(3,3), activation='relu')(mx2)<br/>mx3 = MaxPooling2D(pool_size=(2,2))(conv3)</span><span id="21c9" class="nb jo iq mv b gy ng nd l ne nf">#4th convolution pair<br/>conv4 = Conv2D(64,kernel_size=(2,2), activation='relu')(mx3)</span><span id="acaa" class="nb jo iq mv b gy ng nd l ne nf">flt = Flatten()(conv4)</span><span id="021b" class="nb jo iq mv b gy ng nd l ne nf">dense = Dense(100,activation='relu')(flt)</span><span id="c1df" class="nb jo iq mv b gy ng nd l ne nf">reg_op = Dense(10,activation='linear',name='key_point')(dense)</span><span id="5d07" class="nb jo iq mv b gy ng nd l ne nf">pose_op = Dense(5,activation='softmax',name='pose')(dense)</span><span id="6e67" class="nb jo iq mv b gy ng nd l ne nf">model = Model(inp,[reg_op,pose_op])</span><span id="423d" class="nb jo iq mv b gy ng nd l ne nf">model.summary()</span></pre><p id="2631" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">编译模型。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="b7da" class="nb jo iq mv b gy nc nd l ne nf">loss_dic = {'key_point':'mse','pose':'categorical_crossentropy'}</span><span id="37e3" class="nb jo iq mv b gy ng nd l ne nf">loss_weights = {'key_point':4,'pose':11}</span><span id="847a" class="nb jo iq mv b gy ng nd l ne nf">metrics = {'key_point':'mse', 'pose':['categorical_crossentropy','acc']}</span><span id="1a91" class="nb jo iq mv b gy ng nd l ne nf">model.compile(optimizer='adam',loss=loss_dic,loss_weights=loss_weights,metrics=metrics)</span></pre><p id="3daa" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">一切就绪。让我们训练网络。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="2915" class="nb jo iq mv b gy nc nd l ne nf">H = model.fit(train_images, [train_keypoint_op,train_categorical_ops[-1]], epochs = epochs, batch_size=bs, validation_data=(val_images,[val_keypoint_op,val_categorical_ops[-1]]),callbacks=[TrainValTensorBoard(log_dir='./log4',write_graph=False)])</span></pre><p id="08e5" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们评价一下 model 的性能。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="dbf7" class="nb jo iq mv b gy nc nd l ne nf">train_pred = model.predict(train_images)<br/>val_pred = model.predict(val_images)</span><span id="5b7a" class="nb jo iq mv b gy ng nd l ne nf">print('MSE on train data: ', mean_squared_error(train_keypoint_op,train_pred[0]))<br/>print('MSE on validation data: ', mean_squared_error(val_keypoint_op,val_pred[0]))</span></pre><p id="29ee" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">上面的代码片段给出了以下输出:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="c8c6" class="nb jo iq mv b gy nc nd l ne nf">MSE on train data:  2.825882283863525<br/>MSE on validation data:  31.41507419233826</span></pre><p id="c2e0" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">可视化验证集上的结果。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oh"><img src="../Images/ea24f07cbee5adfce2134b3e0b3898a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43zVYU6EhyfYgLyg3c2RZQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Output Result: Model 3</figcaption></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="6ae9" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated"><strong class="kn ir"> 6.4 第四种模式的实施</strong></p><p id="689e" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这一个的代码如下:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="d47b" class="nb jo iq mv b gy nc nd l ne nf">inp = Input(shape=(160,160,3))</span><span id="c959" class="nb jo iq mv b gy ng nd l ne nf">#1st convolution pair<br/>conv1 = Conv2D(16,kernel_size=(5,5), activation='relu')(inp)<br/>mx1 = MaxPooling2D(pool_size=(2,2))(conv1)</span><span id="5938" class="nb jo iq mv b gy ng nd l ne nf">#2nd convolution pair<br/>conv2 = Conv2D(48,kernel_size=(3,3), activation='relu')(mx1)<br/>mx2 = MaxPooling2D(pool_size=(2,2))(conv2)</span><span id="c0f7" class="nb jo iq mv b gy ng nd l ne nf">#3rd convolution pair<br/>conv3 = Conv2D(64,kernel_size=(3,3), activation='relu')(mx2)<br/>mx3 = MaxPooling2D(pool_size=(2,2))(conv3)</span><span id="a7d6" class="nb jo iq mv b gy ng nd l ne nf">#4th convolution pair<br/>conv4 = Conv2D(64,kernel_size=(2,2), activation='relu')(mx3)</span><span id="2bc3" class="nb jo iq mv b gy ng nd l ne nf">flt = Flatten()(conv4)</span><span id="d1f2" class="nb jo iq mv b gy ng nd l ne nf">dense = Dense(100,activation='relu')(flt)</span><span id="85f9" class="nb jo iq mv b gy ng nd l ne nf">reg_op = Dense(10,activation='linear',name='key_point')(dense)</span><span id="e735" class="nb jo iq mv b gy ng nd l ne nf">model = Model(inp,reg_op)</span><span id="ad08" class="nb jo iq mv b gy ng nd l ne nf">model.summary()</span></pre><p id="72c7" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">编译模型。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="4cf3" class="nb jo iq mv b gy nc nd l ne nf">loss_dic = {'key_point':'mse'}</span><span id="745a" class="nb jo iq mv b gy ng nd l ne nf">metrics = {'key_point':['mse','mae']}</span><span id="95ac" class="nb jo iq mv b gy ng nd l ne nf">model.compile(optimizer='adam',loss=loss_dic,metrics=metrics)</span></pre><p id="9451" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">一切就绪。让我们训练网络。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="f8c9" class="nb jo iq mv b gy nc nd l ne nf">H = model.fit(train_images, train_keypoint_op, epochs = epochs, batch_size=bs, validation_data=(val_images,val_keypoint_op),callbacks=[TrainValTensorBoard(log_dir='./log5',write_graph=False)])</span></pre><p id="fb80" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">让我们评价一下 model 的性能。</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="a58c" class="nb jo iq mv b gy nc nd l ne nf">train_pred = model.predict(train_images)<br/>val_pred = model.predict(val_images)</span><span id="f403" class="nb jo iq mv b gy ng nd l ne nf">print('MSE on train data: ', mean_squared_error(train_keypoint_op,train_pred ))<br/>print('MSE on validation data: ', mean_squared_error(val_keypoint_op,val_pred ))</span></pre><p id="16e9" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">上面的代码片段给出了以下输出:</p><pre class="lr ls lt lu gt mx mv my mz aw na bi"><span id="fbee" class="nb jo iq mv b gy nc nd l ne nf">MSE on train data:  2.822843715225789<br/>MSE on validation data:  30.50257287238015</span></pre><p id="6e27" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">可视化验证集上的结果。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oi"><img src="../Images/025899b388453e14f7cfd04e4d63e42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f2pyJW7MCxcdd6-FTcFM-w.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Output Result: Model 4</figcaption></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="0777" class="jn jo iq bd jp jq mn js jt ju mo jw jx jy mp ka kb kc mq ke kf kg mr ki kj kk bi translated">结论</h1><p id="f809" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">根据上面的实验，很容易得出结论，多任务学习比单独解决这个问题更有效。</p><p id="1d37" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">有时，解决多任务问题比单独解决问题更有帮助，但请注意，如果主问题依赖于辅助问题，则只解决辅助问题。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="3d2a" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">这个项目的完整源代码可以在  这里找到<a class="ae lo" href="https://github.com/rishabhgarg7/Face-Allignment" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> <em class="lp">。</em></strong></a></p><p id="6bf8" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">希望，你喜欢这篇文章，如果你从中学到了什么新的东西，那么你可以通过与他人分享和关注我来展示你的爱..花了这么多时间来写这么全面的博文，希望我的努力能帮助你们中的一些人理解这个案例研究的细节，以便你们也能在自己的地方实施它。</p><p id="cefa" class="pw-post-body-paragraph kl km iq kn b ko lj kq kr ks lk ku kv kw ll ky kz la lm lc ld le ln lg lh li ij bi translated">欢迎在<a class="ae lo" href="https://www.linkedin.com/in/rishabhgarg7/" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> LinkedIn </strong> </a>上与我联系，在<a class="ae lo" href="https://twitter.com/rishabh_grg" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> Twitter </strong> </a>和<a class="ae lo" href="https://www.quora.com/profile/Rishabh-Garg-109" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> Quora </strong> </a>上关注我。</p></div></div>    
</body>
</html>