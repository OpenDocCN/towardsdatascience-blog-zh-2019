<html>
<head>
<title>What the heck is Word Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-the-heck-is-word-embedding-b30f67f01c81?source=collection_archive---------8-----------------------#2019-02-11">https://towardsdatascience.com/what-the-heck-is-word-embedding-b30f67f01c81?source=collection_archive---------8-----------------------#2019-02-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="70e2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">透过神经网络的透镜看文本数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d845289d1233f08a53e8f873ba892467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q5g8UYOIh6U7FK6p"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dmitry Ratushny</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kz"><p id="76a8" class="la lb it bd lc ld le lf lg lh li lj dk translated">Word Embedding = &gt;学习将词汇表中的一组单词或短语映射到数值向量的模型的统称。</p></blockquote><p id="a58a" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi translated">神经网络被设计成从数字数据中学习。</p><p id="40c8" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">单词嵌入实际上就是提高网络从文本数据中学习的能力。通过将数据表示为低维向量。这些向量被称为嵌入。</p><p id="f873" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">这种技术用于降低文本数据的维度，但这些模型也可以了解词汇表中单词的一些有趣特征。</p><h1 id="7c9f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">这是怎么做到的！</h1><p id="dcc8" class="pw-post-body-paragraph lk ll it lm b ln nc ju lp lq nd jx ls lt ne lv lw lx nf lz ma mb ng md me lj im bi translated">处理文本数据中单词的一般方法是对文本进行一次性编码。你的文本词汇中会有成千上万个独特的单词。用这样的一次性编码向量来计算这些单词将是非常低效的，因为一次性编码向量中的大多数值将是 0。因此，将在一个热点矢量和第一个隐藏层之间发生的矩阵计算将导致具有大部分 0 值的输出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/1e39b8a011b8f489da64b9079efe6af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmPlMkLLIBV0uEXzQtT97Q.png"/></div></div></figure><p id="09fe" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">我们使用<strong class="lm iu">嵌入</strong>来解决这个问题，大大提高了我们网络的效率。嵌入就像一个完全连接的层。我们将这一层称为嵌入层，将权重称为嵌入权重。</p><p id="cc07" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">现在，我们直接从嵌入权重矩阵中获取值，而不是在输入和隐藏层之间进行矩阵乘法。我们可以这样做，因为独热向量与权重矩阵的乘法返回对应于“1”输入单元的索引的矩阵行</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/2b2f61881e575e43440d3663fc230ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZj1Hk1mhS5pIMv3ZrpLYw.png"/></div></div></figure><p id="1995" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">因此，我们使用这个权重矩阵作为查找表。我们将单词编码为整数，例如,“酷”编码为 512,“热”编码为 764。然后，为了得到“酷”的隐藏层输出值，我们只需要在权重矩阵中查找第 512 行。这个过程被称为<strong class="lm iu">嵌入查找</strong>。隐藏层输出的维数就是<strong class="lm iu">嵌入维数</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/d9d55be575440420d74ff190b83db2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hPDk0gPyIBg0D5SzY5t7Q.png"/></div></div></figure><p id="6a96" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">重申:-</p><p id="1303" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">a)嵌入层只是一个隐藏层</p><p id="24ee" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">b)查找表只是一个嵌入权重矩阵</p><p id="5645" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">c)查找只是矩阵乘法的捷径</p><p id="506c" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">d)查找表就像任何权重矩阵一样被训练</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="a406" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">目前使用的流行的现成单词嵌入模型:</p><ol class=""><li id="3999" class="nr ns it lm b ln mf lq mg lt nt lx nu mb nv lj nw nx ny nz bi translated">Word2Vec(谷歌)</li><li id="4516" class="nr ns it lm b ln oa lq ob lt oc lx od mb oe lj nw nx ny nz bi translated">手套(斯坦福)</li><li id="9117" class="nr ns it lm b ln oa lq ob lt oc lx od mb oe lj nw nx ny nz bi translated">快速文本(脸书)</li></ol><blockquote class="of og oh"><p id="da89" class="lk ll oi lm b ln mf ju lp lq mg jx ls oj mh lv lw ok mi lz ma ol mj md me lj im bi translated"><a class="ae ky" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank"> <strong class="lm iu"> Word2Vec </strong> </a>:</p></blockquote><p id="7b7f" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">该模型由谷歌提供，并在谷歌新闻数据上进行训练。该模型有 300 个维度，并根据谷歌新闻数据中的 300 万个词进行训练。</p><p id="6cda" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">团队使用跳格法和负抽样来建立这个模型。2013 年上映。</p><blockquote class="of og oh"><p id="36f4" class="lk ll oi lm b ln mf ju lp lq mg jx ls oj mh lv lw ok mi lz ma ol mj md me lj im bi translated"><a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="lm iu">手套</strong> </a>:</p></blockquote><p id="9673" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">单词表示的全局向量(GloVe)由斯坦福大学提供。他们基于 2642840 亿个令牌提供了从 25、50、100、200 到 300 个维度的各种模型</p><p id="f304" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">团队使用词到词共现来建立这个模型。换句话说，如果两个词多次共同出现，这意味着它们在语言或语义上有一些相似之处。</p><blockquote class="of og oh"><p id="9c89" class="lk ll oi lm b ln mf ju lp lq mg jx ls oj mh lv lw ok mi lz ma ol mj md me lj im bi translated"><a class="ae ky" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank">T5】fast text:T7】</a></p></blockquote><p id="0aa6" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">这个模型是由脸书开发的。他们提供了 3 个模型，每个模型有 300 个维度。</p><p id="5479" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">fastText 能够实现单词表示和句子分类的良好性能，因为它们利用了字符级表示。</p><p id="4f10" class="pw-post-body-paragraph lk ll it lm b ln mf ju lp lq mg jx ls lt mh lv lw lx mi lz ma mb mj md me lj im bi translated">除了单词本身之外，每个单词都被表示为一包 n 元字符。例如，对于单词<code class="fe om on oo op b">partial</code>，当 n=3 时，字符 n-grams 的快速文本表示是<code class="fe om on oo op b">&lt;pa, art, rti, tia, ial, al&gt;</code>。添加了<code class="fe om on oo op b">&lt;</code>和<code class="fe om on oo op b">&gt;</code>作为边界符号，将 n-grams 与单词本身分开。</p><h1 id="b0b0" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">感谢阅读！</h1><ul class=""><li id="3697" class="nr ns it lm b ln nc lq nd lt oq lx or mb os lj ot nx ny nz bi translated">如果你喜欢这个，请在 medium 上关注我。</li><li id="5bb3" class="nr ns it lm b ln oa lq ob lt oc lx od mb oe lj ot nx ny nz bi translated">你的掌声对你写更多、写得更好是一个巨大的鼓励和动力。</li><li id="1fe0" class="nr ns it lm b ln oa lq ob lt oc lx od mb oe lj ot nx ny nz bi translated">有兴趣合作吗？我们在<a class="ae ky" href="https://www.linkedin.com/in/samarth-agrawal-2501/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上连线吧。</li><li id="ab02" class="nr ns it lm b ln oa lq ob lt oc lx od mb oe lj ot nx ny nz bi translated">请随意写下您的想法/建议/反馈。</li></ul></div></div>    
</body>
</html>