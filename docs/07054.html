<html>
<head>
<title>Ridge regularization on linear regression and deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归和深度学习中的岭正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-regularization-on-linear-regression-and-deep-learning-a32cd9dc5a78?source=collection_archive---------21-----------------------#2019-10-05">https://towardsdatascience.com/ridge-regularization-on-linear-regression-and-deep-learning-a32cd9dc5a78?source=collection_archive---------21-----------------------#2019-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="ac36" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">介绍</h1><p id="b248" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">正如我在之前关于<a class="ae lm" href="https://medium.com/@snaveenmathew/a-short-note-on-regularization-42ee07c65d90" rel="noopener">正则化</a>和<a class="ae lm" rel="noopener" target="_blank" href="/lasso-regularization-on-linear-regression-and-other-models-70f65efda40c">套索惩罚</a>的文章中所讨论的，正则化可以用来对抗过度参数化模型中的过度拟合。正则化作为最佳子集选择的计算有效的替代方案，但是有其缺点(例如:估计器的效率低)。本文的目的是介绍岭回归的数学基础，推导其解析解，讨论其几何解释，并将其与奇异值分解联系起来进行分量分析。</p><h1 id="0f49" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">制定</h1><p id="6a9f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">线性回归的公式和正规方程形式见我的<a class="ae lm" rel="noopener" target="_blank" href="/lasso-regularization-on-linear-regression-and-other-models-70f65efda40c">上篇</a>。</p><h2 id="8574" class="ln jr it bd js lo lp dn jw lq lr dp ka kz ls lt ke ld lu lv ki lh lw lx km ly bi translated">山脊公式和解决方案</h2><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/2e3c2ce630682277be3e9300b59732c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/0*KtTL7wnCLImT7wqG.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Regression equation</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/8a7bdf71620ace445d416f3253794f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*pnP7MbBXfg1CyhGLIHVQVw.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge solution estimated on a sample</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/c9aeca90b032941eb87fb1713c7ed5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*-24OWHx3vQqov_s4jD6USg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge loss function</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/29723085b31902044371a153d7d5acd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*9r4GW8T3kOt-Gbwt5DzboQ.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge solution</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/dde8f92903a7d005d9bd0ab30bb136ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*kWpmc0Dfljg3qmuBkV2sow.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge estimate using normal equation</figcaption></figure><p id="211b" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">注:岭可以被视为修改协方差矩阵的惩罚，从而减少变量之间的共线性。脊的这一特性有助于避免损失函数中的平稳段。</p><h1 id="27c5" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">几何解释</h1><h2 id="4506" class="ln jr it bd js lo lp dn jw lq lr dp ka kz ls lt ke ld lu lv ki lh lw lx km ly bi translated">优化的对偶形式</h2><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c2910338c1f763ca527d970062684e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*SUfefssiXWT8ouPtQ7qE0A.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge optimization: primal (unconstrained)</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/9754c9d8887c9976feb98e6c95cac6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*c6SdJz7Ad4W2PSjxBMbzww.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Dual form of ridge optimization (constrained)</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/8af0a8debdf4b0034f1f7fa877e9c0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*vs5g9Lp7FtNGrqjCPf_Mmg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Contour plot for ridge. Image credits: <a class="ae lm" href="https://stats.stackexchange.com/questions/30456/geometric-interpretation-of-penalized-linear-regression" rel="noopener ugc nofollow" target="_blank">https://stats.stackexchange.com/questions/30456/geometric-interpretation-of-penalized-linear-regression</a></figcaption></figure><p id="6723" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">紫色圆圈对应于不同 s 的|β|₂ ≤ s，其中|β|₂ =沿圆周的常数。增加λ会减小圆的大小。红色椭圆对应于不同的 y-xβ)₂值，其中 y-xβ)₂ =沿椭圆的常数。对于固定的λ，s 的值是固定的:这对应于一个紫色圆圈。</p><p id="9c1b" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">在无约束的情况下，y-xβ)₂的最小值出现在椭圆的中心。然而，在|β|₂ ≤ s 的约束情况下，解将向原点位移。</p><p id="1870" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">独特的山脊解决方案位于这两条“曲线”的接触点。由于曲线|β|₂ ≤ s 是可微的，所以对于有限的λ，所有βᵢs 的脊解将是非零的。随着λ的增大(s 的减小)，βᵢs 越来越接近 0。即使βᵢs 移近 0，这也不会导致有限λ的稀疏。因此，脊线惩罚不能用于特征选择。</p><h1 id="a8ed" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">岭惩罚和奇异值分解的关联</h1><p id="9797" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">对 x 应用奇异值分解，我们得到</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/3a3acd4310883ae89ce18337fa3704aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/1*H0Is8rrHFZl24BMhIA5JVA.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">SVD on independent variables</figcaption></figure><p id="383e" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">将此代入岭的解析解，假设独立变量居中，利用旋转矩阵和投影矩阵的性质，我们得到</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi my"><img src="../Images/fcbd4b8ebf1df91369fb4789f274f383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*BZFuPIw9tkJYjpH-IwI-4g.png"/></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2132a4e2f99cd5ba72d26bf637cd1864.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*NkBkjzqv-JiJyQiElbaPrw.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge solution in terms of principal components</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi na"><img src="../Images/284f4b7437d94b32f02b46bc52035bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*JneiNO55MSC8dC8Wbg2j5A.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge prediction</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b4267b6e67316e0c6ef5f2846e225eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*-t4QlapwuVwr9d-4WRy14Q.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Diagonal matrix in ridge prediction</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/29154d33c2924f00dad27cc8b6f38587.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*Sv0r9n4lU1EQEQkJQhrYPg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Ridge prediction</figcaption></figure><p id="0442" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">让我们一步一步地检查这一点:</p><ol class=""><li id="2602" class="nd ne it kq b kr mp kv mq kz nf ld ng lh nh ll ni nj nk nl bi translated">PCⱼ空间中的 y 项目，其中 PCⱼ指的是 jᵗʰ主分量。所有的投影都在 x 的列空间中</li><li id="4f58" class="nd ne it kq b kr nm kv nn kz no ld np lh nq ll ni nj nk nl bi translated">按一个因子缩小投影(λ &gt; 0 时发生严格的缩小)</li><li id="5b15" class="nd ne it kq b kr nm kv nn kz no ld np lh nq ll ni nj nk nl bi translated">重新变换投影</li></ol><p id="e291" class="pw-post-body-paragraph ko kp it kq b kr mp kt ku kv mq kx ky kz mr lb lc ld ms lf lg lh mt lj lk ll im bi translated">从上面的等式中，我们可以使用岭估计回归模型的自由度:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/09f58c06168e24ac3275c6810b3572af.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/format:webp/1*cyryihZ7LHqEq-J0Z2WqeA.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Degrees of freedom of ridge</figcaption></figure><h1 id="e88b" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">深度学习的延伸</h1><p id="8152" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当特定层的权重相关时，深度学习会遭受过度拟合。这通常发生在网络的全连接部分。Ridge 将训练集的输出特征映射投影到主成分上，并缩小预测。这使得损失曲线更加凸出，即使在独立变量之间完全共线性的情况下。对于适当选择的λ，权重将非常小。因此，在存在脊形损失的情况下，当我们通过网络执行正向传递时，非线性缓慢地建立。这个论点也适用于小λ。</p><h1 id="58fd" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="36ee" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在<a class="ae lm" rel="noopener" target="_blank" href="/lasso-regularization-on-linear-regression-and-other-models-70f65efda40c">之前的文章</a>中，我们讨论了套索正则化作为一种通过执行变量选择来解决过度拟合的方法。当需要强制抑制特征消除时，岭回归非常有用。在实际问题中，当已知所有特征，并且所有特征对结果都具有“科学价值”时，此属性非常有用:因此，分析中不能遗漏任何特征。然而，当预测精度很重要时，使用套索和脊线惩罚的线性组合。这叫做 elasticnet。L1 正则化的比例也是一个超参数，应该与其他超参数一起调整。Elasticnet 是 L1 和 L2 正则化的一般情况。当在验证集上进行适当调整时，它有望胜过 ridge 和 lasso。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a76dfcce97999b21d1a2263c7b67cc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*c261ApwxOp-Ob-fxrYPHew.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Elasticnet. Equation credit: glmnet package documentation</figcaption></figure></div></div>    
</body>
</html>