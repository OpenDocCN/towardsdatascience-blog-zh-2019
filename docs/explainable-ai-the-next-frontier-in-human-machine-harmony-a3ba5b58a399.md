# 可解释的人工智能:人机和谐的下一个前沿

> 原文：<https://towardsdatascience.com/explainable-ai-the-next-frontier-in-human-machine-harmony-a3ba5b58a399?source=collection_archive---------35----------------------->

![](img/0e493dbcc38b93c267bce82da022905e.png)

Photo by [Andy Kelly](https://unsplash.com/@askkell?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

起初，没有机器。人类仅仅依靠体力劳动生存。然后他们发明了工具，从那些工具中进化出了更复杂的工具。最终，由于跨越数千年的创新，这些工具变成了机器。

发明机器的目的是增强我们人类的能力和虚弱的能力，因为从设计上来说，它们超过了我们的身体能力。机械机器在原始能力方面给了我们优势，而计算机器为我们提供了以我们自己根本无法做到的速度生成、组织和处理大量数据的手段。

我们现在正处于计算机中酝酿的另一项人类创新的初级阶段，以便机器能够比我们更好地学习(机器学习或 ML)、推理和解决问题(人工智能或 AI)。如果我们做对了。这些机器将处于下一波人类进步的前沿。但是危险是什么，我们如何避免它们呢？

# 危险的 AI

人工智能的明显危险是那些可能以这样或那样的方式伤害人类或生命的危险。人工智能/人工智能的概念出现不到 100 年，与智人出现以来的 20 万年相比，这是一个相当小的间隔。因此，很容易怀疑人工智能可能的指数进化，并问自己这样的问题；我们什么时候能造出超越人类认知能力(奇点)的机器，到时候会发生什么？

我邀请你，现在，把奇点问题留给科幻小说。请注意一件事，那就是今天的 ML/AI 已经呈现出有意义的危险，我们有责任尽快减轻这种责任。此外，如果一方面，我们希望在未来“奇点”发生的那一天最大限度地增加积极成果的机会，那么随着人工智能的不断发展，我们应该不断应对风险。另一方面，如果我们想最大化我们的机会去实现那个未来。

## **今日 AI 的危险**

鉴于前面描述的危险，让我们专注于我们认为当今 ML/AI 最紧迫的问题之一，它源于观察到的过度信任基于 AI/ML 的系统的趋势，因为它们现在处于做出明智决策的模糊理想中，因此，它衍生出我们应该瞄准的下一个趋势；那就是深思熟虑地设计基于人工智能的系统，旨在增强我们人类的认知能力，而不是取代它们。

先说，我说的过度信任 AI/ML 是什么意思？今天，如果你面临选择一个队友下棋的决定，你的选择是在这项任务中最好的人类选手或最好的计算机程序之间，你不会因为选择计算机程序而疯狂。在这场游戏中，如果你碰巧因为盲目信任这个程序所做的决定而赢了，难道你不认为信任这样一个程序是最明智的决定吗？当我们盲目地依赖一个可以接收人类无法接收的所有数据的系统时，我们是否在做出最佳决策？

为了回答这些问题，我们还应该问自己，盲目信任特定的人工智能系统会有什么影响？并承认这个问题的答案因情况而异。为了进一步发展这一思想，让我们来检验这样一个假设:越来越依赖机器来做决策已经被证明是对我们人类的一种威胁。

最近一个令人悲伤的例子是在航空领域(波音 737 MAX 问题)，人们可以认为飞机越来越多地由自动驾驶系统控制，飞行员只能在起飞和着陆期间执行例行程序。这对飞行员在意外情况下操纵飞机的能力产生了严重的影响。人们还可以说，对机器决策的日益依赖正导致飞行员放弃他们的认知技能和应对困难局面的能力，因为他们盲目信任自动驾驶系统。但是这个问题的答案是我们抛弃自动驾驶系统吗？

也许一个理想的场景是，如果专家(在这种情况下是飞行员)观察到违背他们直觉的东西，他们可以做我们人类自然的事情，这就是质疑。*自动驾驶:你在做什么，为什么？如果答案不令人信服，他们(飞行员)应该能够立即控制局面，并且作为拥有巨大认知能力的人类:承担行动的责任。*

如果你是那些同意前面的将是一个*理想解决方案的人之一，*你并不孤单，因为它说明了对一些人所谓的“可解释的人工智能”或 XAI 的需要。因为我们应该能够从任何基于人工智能的系统中询问并获得关于它正在做出的选择的合理解释。好奇是使我们与众不同的人类特征之一，我们应该尽一切努力保持这种人类属性。

# 可解释的人工智能(XAI)

给你一点背景知识，但不要过多地谈论细节；国防高级研究计划局的人创造了术语“可解释的人工智能”( XAI ),作为一项研究计划，旨在揭示人工智能的一个关键缺点。越复杂的 ML/AI 模型越难以解释。此外，目前形式的人工智能旨在学习特定领域的知识，并从具体的数据示例中学习，仅限于训练他们解决的特定问题，因为它仍然需要人类抽象思维的能力来理解问题的全部背景。

鉴于这些基于人工智能/人工智能的系统的理解范围狭窄，很自然地认为，如果这些算法被用于做出涉及某人生活或社会的关键决策，那么(对我来说很明显，我希望对你也是如此)我们不应该摆脱它们，但我们不应该委托这些系统承担做出这些关键决策的全部责任。

为了进一步阐述这一点，我建议你阅读[迈克尔·乔丹](https://medium.com/u/2e99d8165809?source=post_page-----3839052ab8dc----------------------)(不是篮球运动员，而是来自伯克利的知名工程师)，他在他的文章《人工智能——革命尚未发生》中阐述了他的想法。我和他在某些事情上意见一致；我们应该问自己，今天 ML/AI 需要哪些重要的发展，以安全地增强人类解决非常复杂问题的能力。我认为，XAI 肯定是这些重要发展之一。

# XAI 重要问题

想想另一个系统给出建议的例子:如果医生使用人工智能系统来帮助她诊断医疗状况，预测的诊断是不够的，逻辑流程是医生还应该能够要求系统解释预测的诊断，在理想的情况下，答案还应该尽可能以医生可解释的术语出现，而不是用机器学习模型及其可调参数中使用的复杂行话来解释。

这向我们所有机器学习学科的人发出了邀请，让我们问自己，我们如何构建可以预测并解释的系统？此外，承认努力的重要性。因为来自 AI/ML 解释的所有学科的领域专家应该能够通过利用他们可用的越来越多的数据和计算能力来推进他们领域的知识。

你们中的一些人可能仍然想知道，最终来说，是否真的需要领域专家？为此，我建议你读一读 Rich Sutton(谷歌 Deepmind 的重要思想家之一)的文章《痛苦的教训》(The bitter lesson)，在这篇文章中，他认为领域专业知识无法与机器学习模型的模式识别能力相竞争。我们不必同意或不同意。然而，我的观点是，我们把“竞争”这个词从等式中去掉，而是简单地认为增强了我们人类的认知能力。

因此，第二个邀请是设计这些新工具，使每个人都可以理解问题可以从数据的角度来看，这些工具也应该足够简单，任何人都可以在自己的领域成为*【数据科学家】*，并通过机器学习的奇妙模式识别能力获得新的见解和发现，这些见解和发现隐藏在乍一看对我们人类来说可能不明显的数据中。如果这样的系统不仅能够发现模式，而且能够向我们解释它们，我们不仅会意识到机器拥有的*搜索和学习*的探索能力，而且我们还可以塑造下一种形式的人类*智能增强* (IA)。

这就是为什么需要建立一个单独的 HCI 类(人机界面),在我看来，这可以归结为解决两个问题:

**第一；当使用一个人工智能系统时，不管问题是什么，我们必须能够得到答案的那些基本问题应该是什么？**

让我们把这个问题的范围缩小到预测，因为我们相信预测最终会为大多数决策过程提供信息。因此，我们最好能够回答任何人工智能辅助决策的三个基本问题:

*   我能相信这个预测吗？为什么？
*   为什么是这个预测而不是别的呢？
*   我怎样才能使预测更可靠/更好？

**第二；我们如何向其他人解释这些问题的答案？**

DARPA 有一个独立的团队致力于研究解释心理学。这个团队完全专注于从心理学的角度挖掘解释科学的文学知识库，以建立有助于衡量解释有效性的框架。这是 XAI 向 UX 迈出的一步。

除了 DARPA 之外，在大公司和大学里也有许多研究项目正在为 XAI 开发工具。我认为，开发能够回答 XAI 基本问题的系统遵循两个阶段(软 XAI 和内省人工智能):

**软 XAI**

想象一下，你正在试图理解为什么一种动物会有这样的行为。但是你不能和这样的动物交流？同样，你试图理解人工智能系统决策背后的基本原理，但该系统目前无法自己回答它为什么要做这些事情。有什么方法可以解决这些问题呢？

**深层方法**，试图理解系统内部发生的一切。我们可以尝试详细了解系统的不同组成部分，以及它们如何学习，以及这些任务如何影响最终结果。例如，如果我们在谈论人工神经网络，这意味着查看每个感知器的参数，了解是什么使它们“激活”以及这些激活如何传播到我们可以理解的概念中。然而，解释这些结果需要高度的专业化，而且很难一概而论。

与此类似的是，试图通过分析特定刺激下的神经通路和大脑内部来从我们的行为中获得解释，这可以导致关于大脑如何工作的重要发现，并且神经科学和机器学习领域继续从彼此的发现中学习。然而，我们的经验是，可以从更简单的方法中获得可解释结果的替代方案。

**黑盒方法**，这是通过将系统视为一个我们不知道其内部的黑盒来获得答案，因此只有通过操纵其输入并找出输出中有趣的内容才能理解其行为。这提供了一个很好的框架来获得许多见解，并且在其他领域也很有效，从物理学到心理学。

鉴于这种方法的实用性， [MindsDB](http://www.mindsdb.com/) 将其作为解释的手段，在这条路上，我们有幸站在巨人的肩膀上，因为我们研究并借鉴了其他人的想法(谷歌的 [What If](https://pair-code.github.io/what-if-tool/) 、IBM 的 [Seq2Seq-Vis](https://venturebeat.com/2018/11/01/ibm-harvard-develop-tool-to-tackle-black-box-problem-in-ai-translation/) 、香港科技大学的 [VisLab](http://vis.cse.ust.hk/groups/xai-vis/) 、SHAP 的、 [LIME](https://github.com/marcotcr/lime) 等等)

**内省 AI**

一旦我们实现了软 XAI，我们实际上可以教机器如何做到这一点，这将涉及重新设计 AI/ML，以便它可以对自己以及为什么它会这样做做出解释，就像我们学习如何解释我们的行为和思想一样。我们相信下一代人工智能系统将被教会分析自己的行为。

在我看来，内省式人工智能将源自黑盒方法和我们对语言的理解。背后的推理来自于我们人类的观察；我们对我们的大脑知之甚少，但同样可以解释为什么我们会以这样或那样的方式思考。我们已经进化到能够解释我们的行为和我们对周围世界的解释。有人可能会说，可解释性本身是传递知识的基础，它在进化过程中塑造了我们的大脑，使人类成为唯一能够在知识基础上不断构建知识的物种。

# 展望未来

围绕人工智能及其能力的所有兴奋，任何 XAI 技术都有一个重要的作用，那就是为后代保持人工智能的安全和理智。

我和我在 MindsDB 的同事们，将不断地发布我们对可解释性的贡献。您可以在此处找到公共存储库:

[https://github.com/mindsdb](https://github.com/mindsdb)

随着我们在未来几年继续为实际成果做出贡献。我们相信，在为现实世界构建任何人工智能驱动的应用程序时，XAI 很可能会成为一项法定要求。

鸣谢*:有很多人在撰写本文期间的评论和其中包含的想法给了我很大的帮助，包括李正吉·达博、乔治·霍苏、亚当·卡里根*

**参考文献**

[https://medium . com/@ mijordan 3/artificial-intelligence-the revolution-not-happen-that-5e1d 5812 e1e 7](https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7)

[http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

[https://bdtechtalks . com/2019/01/10/DARPA-xai-explable-artificial-intelligence/](https://bdtechtalks.com/2019/01/10/darpa-xai-explainable-artificial-intelligence/)

[https://bdtechtalks . com/2018/09/25/explaible-interpretable-ai/](https://bdtechtalks.com/2018/09/25/explainable-interpretable-ai/)

[https://www . DARPA . mil/program/explaible-artificial-intelligence](https://www.darpa.mil/program/explainable-artificial-intelligence)

[https://medium . com/@ BonsaiAI/explaible-ai-3-deep-explaining-approachs-to-xai-1807 e251e 537](https://medium.com/@BonsaiAI/explainable-ai-3-deep-explanations-approaches-to-xai-1807e251e537)

https://arxiv.org/pdf/1606.04155

【https://arxiv.org/pdf/1705.04146 

[http://www.deeplearningpatterns.com/doku.php?id =合理化](http://www.deeplearningpatterns.com/doku.php?id=rationalization)