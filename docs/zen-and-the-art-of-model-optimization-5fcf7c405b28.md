# 禅宗和模型优化的艺术

> 原文：<https://towardsdatascience.com/zen-and-the-art-of-model-optimization-5fcf7c405b28?source=collection_archive---------16----------------------->

## 在数据世界里争吵

## 社交媒体 NLP 挑战中的模型比较

![](img/14fe463c52b8362534dbc7d1749002d5.png)

Z *en 和摩托车维修艺术*是我大学时最喜欢的书之一。这本书以一对父子穿越美国的摩托车旅行为背景，思考如何过有意义的生活。可以说，作者罗伯特皮尔西格阐述的关键信息是，只有当我们全心全意地投入手头的任务时，我们才能实现卓越。**如果一件事值得做，那就值得做好。**

几乎与此同时，研究设计和统计推断课程向我灌输了可解释性和简约性的重要性。沟通确实广而告之(希望？)作为一个数据科学家的必备技能。毫无疑问，向外行观众解释某些模型的发现和工作原理比其他模型更容易。对于我们日常工作中的许多人来说，优化的可解释模型几乎可以做得一样好，有时甚至超过更复杂的模型。**如果一个模型值得运行，那么它就值得优化运行**。

有条不紊的数据清理、仔细的特征选择/工程、明智的评分标准选择以及模型超参数微调可以有力地提升模型性能。在这篇文章中，我在一次自然语言处理挑战中用**逻辑回归** (logit)和**多项朴素贝叶斯** (MNB)与**随机森林** (RF)分类器和**支持向量分类器** (SVC)进行了较量。

逻辑回归长期以来被用于统计推断，并且是机器学习分类任务的主力，而 MNB 被认为是 NLP 的基本模型之一。尽管随机森林算法不是“黑盒”，但它们比罗吉特或 MNB 更难解释。然而，支持向量机绝对是黑盒。我在预测和推理之间的大辩论中不持立场，而选择 NLP，因为这是一个预测是唯一目标的领域。

# 自然语言处理分类

选择的数据集来自 *Crowdflower 的 Data For Everyone Library* ，其中包含来自美国政客社交媒体账户的 5000 条消息。这些信息从几个方面进行了分类，如党派偏见、目标受众和主题。我将试图预测这些信息的**政治偏见**——“党派”或“中立”。

数据集中有相同数量的*脸书*帖子和*推特*推文，但只有 **26%** 的消息被归类为“党派”。因此，我们有了一个不平衡的数据集，其中“中立”信息几乎是“党派”信息的三倍。换句话说，如果这是一个装有四个黑白球的袋子，那么只有(大约)一个黑球对三个白球。因此，如果我的目标是选一个黑球，也就是预测一个“党派”信息，我将有很大的胜算。

![](img/b168bc65a5fdceba3b3946b7ae05f21d.png)

在目标变量中**“党派”类编码为 1** **，“中立”类编码为 0。鉴于我的目标是预测“党派”信息，我将避开整体模型准确性分数，而是关注“党派”(少数民族)类别的 **F1 分数、**以及 **Cohen 的 kappa 统计。****

假设我们有一个分为“正”或“负”的二元变量，这是纯粹用来区分这两个类别的术语。根据下面描述的风格化的**混淆矩阵**，我们在水平轴上有二元变量的**预测**类，在垂直轴上有**实际**类。

自然，预测很可能不是 100%准确的，所以我们可以将那些“肯定的”预测分为正确的(真肯定)或错误的(假肯定)。我们可以对“负面”预测做同样的事情。“阳性”类别的精确率将是真阳性的总和除以总阳性类别预测(或真阳性+假阳性)。**所以** **精度是模型从其*预测的*阳性观测值**中挑选出一个真阳性的概率。

![](img/511f2ee90c010b4d2d4a01efb8ac2727.png)

另一方面，召回率是真阳性的数量除以真阳性和假阴性之和的比率。请注意，假阴性是那些被模型错误地标记为“阴性”但实际上是“阳性”的观察结果。**所以回忆描述了模型从*实际*阳性观察值**中挑选出一个真阳性的概率。

当正类在不平衡数据集中占少数时，精确度和召回率是模型预测能力的更有用的度量，并且主要目标是正确识别那些正观察值。**F1 得分是准确率和召回率的调和平均值。**

Cohen 的 kappa 是一个更广泛的统计数据，它告诉我们该模型在预测“积极”和“消极”类别方面的表现比单独的随机机会好多少。kappa 统计值大于 0 意味着模型比 chance 更好，它越接近最大上限 1 意味着模型对数据的分类越好。

# 一袋单词

“*单词包*”(BoW)是 NLP 中基本的机器学习技术。网上有无数的帖子详细描述了 BoW，所以我没有必要赘述。只要说 BoW 方法从预先分类的文本中提取单个单词就足够了，不考虑语法和顺序。它假设文本之间的意义和相似性被编码在抽象词汇中。虽然这听起来很简单，但它确实非常有效。

BoW 的一个主要问题是提取单个单词作为特征会很快导致财富的尴尬，或者被称为**“维数灾难】**。除了具有比观察值更多的特征的潜在问题之外，词频通常遵循长尾 **Zipfian 分布**，这会对广义线性模型的性能产生不利影响。如果这些问题还不够，多重共线性是另一个问题。

例如，在我们的 5000 个社交媒体帖子的样本中，将有数万个单独的单词和符号被提取，其中许多是不相关的(即 *https、www、@、today* 等。).维数灾难往往是逻辑回归的一个问题(长尾分布加剧了这一问题)，加上其他如 kNN 和决策树。然而，MNB 在这方面往往表现良好。RF 也可以做得很好，因为它是基于随机组合回归变量的重复抽样。

因此，我们需要无情地筛选潜在的变量数量。我首先清理文本，删除不相关的符号，增加停用词列表，以减少特征维数。下面显示了来自数据集中的两条消息的比较，并列显示了原始版本和清除版本。显然，对清理后的消息还需要做更多的工作，下一步是添加一个停用词列表。

![](img/76f25f91f0164ff96f5f1e6995a06984.png)

下一步应该跟在**词干化或词汇化、**之后，通常只有一个或另一个就足够了。词干通过丢弃复数、性别、时态、大小写、基数等将单词合并在一起。，并且基本上是 NLP 的一种形式**特征工程**来减少特征空间。我将 *Porter* 词干提取算法插入到 *CountVectorizer* 中，以提取文本数据*。*我可以使用的其他特征工程选项是*计数矢量器*中的 ***n_grams*** 和***min _ df****参数。*

# *跨过程和模型的网格搜索*

*我在以下四个模型上运行了*GridsearchCV*:logit、MNB、RF 分类器和 SVC。Python 中的 sklearn 库允许您更改 logit、RF 和 SVC 的 *class-weight* 参数，在处理不平衡数据时，建议指明**class _ weight = " balanced "**。*

*Gridsearch 被设置为**优化“partisan”类上的 F1 分数**，其中包括*计数矢量器*(带有*波特*词干分析器、 *n_gram* 和 *min_df* )、*tfidfttransformer*以及模型。数据经过**80–20 训练测试分割**，训练设置到进一步的 **cv=5 交叉验证**程序。*

*![](img/ab1e9843286458af142ce53d26bbd73b.png)**![](img/4b3357b11ab14f424d51534f70d2ad62.png)**![](img/029b2cd78d12ae888df5b30f4dab03c6.png)**![](img/45dedc18487ca050be23daf9e132c0ff.png)*

*SVC 在训练集上取得了最高的平均 CV 分数(F1 ),但在测试集上取得了最低的 F1 分数。在测试集上，RF 和 SVC 都取得了明显高于 logit 或 MNB 的总体准确率分数(下图中的绿条)。然而，当测试集上的 **F1 分数下降时**罗吉特和 MNB** 都胜过了更复杂的模型(下图中的红色条)。***

*![](img/0f4db07fc779a3067e3827a908bffc82.png)*

*所有结果都基于**单独优化的算法**，详情如下。有趣的是，尽管避免使用*波特斯特梅尔*和 *TfidfTransformer，MNB 在测试集的 F1 分数方面成为了赢家。所有其他型号都求助于使用词干分析器，尽管 SVC 也没有使用 TfidfTransformer。**

*![](img/fdf49418580db2fbf21b09e585fa8a94.png)*

*检查各个**混淆矩阵**的细节将有助于深入了解每个模型的表现。下面显示了来自 logit 和 SVC 模型的测试集结果的矩阵，以供说明。*

*根据 logit 的混淆矩阵，它从 435 个(或 203 + 232)的总正面预测中正确地选择了 203 个真正正面(“党派”)的消息，这使它的准确率达到 46.7%。还可以将 203 个真阳性消息与 262 个(或 203 + 59)的实际阳性消息总数进行比较，这意味着召回率为 77.5%。因此，logit 模型取得了 58.2%的 F1 分数。*

*![](img/e7a9c480b64d165123073f9dd3d2b3d9.png)*

*相比之下，SVC 模型在这方面表现不佳。它设法从 175 个(或 99 + 76)的总正面预测中获得 99 个真正正面的信息。虽然精确率可能看起来不太差，但 99 个真正的正面预测需要与 262 个(或 99 + 163)的总实际正面消息进行比较。微弱的召回率拖累 SVC 的 F1 成绩仅为 45.3%。似乎 SVC 的预测能力更集中于负面类别，这使它获得了更高的总体准确性分数，但这不是这里的目的。*

# *对少数民族类进行重采样*

*一种可能是使用**单词嵌入**，这是一种比 BOW 更高级的方法。然而，单词嵌入往往在一个非常大的训练文档集上表现最好。在处理相对较小的数据集中的简短和特定领域的社交媒体帖子时，这可能不是最佳方法，就像本例一样。*

*我认为走另一条路会更有成效，特别是考虑到不平衡的数据问题，那就是**对训练集**进行重新采样，以便模型在更平衡的数据上进行训练。选择的算法是流行的 **SMOTE** (合成少数过采样技术)和组合 **SMOTE-Tomek** 。*

*SMOTE 通过对少数类进行**过采样**来帮助平衡这些类，并通过对它们之间的空间进行线性插值(使用*k-最近邻*方法)来生成**合成**少数类观察值。插值过程意味着它将仅从可用的示例中生成合成观测值。*

**另一方面，Tomek links* 是一个**欠采样**过程，它移除多数类观察值，以便在多数类和少数类之间创建更清晰的空间分界。SMOTE-Tomek 算法基本上结合了 SMOTE 和 Tomek 链接的过程。*

*我重复了**80–20 训练测试分割**，在训练集上又进行了**五次交叉验证**程序。重采样过程应该只在训练集上进行，然后在未改变的测试集上对训练的模型进行评分。*

*为简洁起见，我将只报告使用上述优化模型参数的 SMOTE 和 SMOTE-Tomek 重新采样的结果。SMOTE 帮助提高了 logit 模型在测试集上的整体准确性分数和 Cohen 的 kappa，但对 F1 分数仅产生了微小的改善。SMOTE-Tomek 更适合 MNB 模型，尽管改进并不显著。*

*另一方面，重采样技术削弱了 RF 和 SVC 模型的分数。这似乎证实了这些更复杂的模型正将其预测能力集中在该数据集中的大多数“负面”类别上。在任何情况下，logit 和 MNB 仍然表现出比 RF 和 SVC 更低的准确性分数，但他们的**优于 F1 的分数**仍然更高。*

*![](img/e13c37f143c2ff02a4048faa7dcc1117.png)*

***MNB 最终摘得桂冠**在“党派”级别的 F1 分数和科恩的 kappa 统计数据方面，在 SMOTE-Tomek 重采样的帮助下获得了最高分。洛吉特紧随其后。让我们按照 SMOTE-Tomek 程序比较来自 MNB 和 RF 模型的测试集的混淆矩阵。*

*![](img/2a3b03234966ffbda14ba51e05e8c3d6.png)*

*我们可以观察到，MNB 达到了 50%的准确率，甚至更高，72.1%的召回率。另一方面，RF 模型获得了 51.9%的精确率和 52.7%的召回率。与 SVC 模型类似，随机森林分类器似乎更加关注多数否定类。*

*因此， **logit 和 MNB 在这场 NLP 挑战中胜过了 RF 和 SVC** ，其目标是预测“党派”政治信息。记住，偶然预测到“党派”信息的基线概率只有 26%。MNB 和 logit 模型的 F1 值为 0.58-0.59，科恩的 kappa 值为 0.39-0.40。RF 和 SVC 算法返回更高的准确度分数，但是在目标“党派”类上的 F1 分数较弱。*

# ***结论***

*这篇文章不是关于哪个机器学习模型是绝对最好的。它是关于为什么我们应该努力优化我们选择运行的模型的性能。优化模型性能涉及从系统的**数据清理**，仔细的**特征选择/工程**，明智的选择**评分指标**，**模型超参数**微调，到其他相关措施以提高模型性能，例如在不平衡数据集的情况下进行重采样。*

*通过这样做，简单且可解释的模型通常可以产生令人惊讶的性能结果。如果这有助于我们数据科学工作的交流方面，那就更好了。我将引用罗伯特·皮尔西格的一段话来结束我的发言，他进而解释了柏拉图的话:*

> *“菲德鲁斯，什么是好的，什么是不好的——我们需要别人告诉我们这些吗？”*