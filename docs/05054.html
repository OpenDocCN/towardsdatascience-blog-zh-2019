<html>
<head>
<title>Machine Learning Pipelines: Nonlinear Model Stacking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习管道:非线性模型堆叠</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-pipelines-nonlinear-model-stacking-668f2b720344?source=collection_archive---------19-----------------------#2019-07-29">https://towardsdatascience.com/machine-learning-pipelines-nonlinear-model-stacking-668f2b720344?source=collection_archive---------19-----------------------#2019-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/9d93a6fda04f830ae2bb19f027fca8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/0*FpoXfJ_2XC256c3A"/></div></figure><p id="8c79" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通常，我们面对的数据集是相当线性的，或者可以被处理成一个数据集。但是，如果我们正在检查的数据集真的应该以非线性的方式来看待呢？步入非线性特征工程的世界。首先，我们来看看非线性数据的例子。接下来，我们将简要讨论作为非线性特征工程手段的 K-means 算法。最后，我们将在逻辑回归的基础上应用 K-means 来构建一个优秀的分类模型。</p><h1 id="3c7b" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">非线性数据的例子</h1><p id="1215" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">非线性数据在商业世界中经常出现。示例包括:分组群体行为(营销)、群体活动的库存模式(销售)、以前交易的异常检测(财务)等。[1].举一个更具体的例子(供应链/物流)，我们甚至可以在卡车司机超速行驶数据的可视化中看到它[1]:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ly"><img src="../Images/99904a6165e564f762b00c520fe16154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gw1_cTPHWMM8PhdV"/></div></div></figure><p id="d1fb" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">快速浏览一下，我们可以看到这个数据集中至少有 2 个组。一组分为 100 米以上和 100 米以下。直观地，我们可以看到，在这里拟合一个线性模型将是可怕的。因此，我们需要一个不同类型的模型。应用 K-means，我们实际上可以找到如下所示的四个组[1]:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mh"><img src="../Images/9c3b2eb8ae5ca48e020d2357fd3f5f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*miCvHkVqCI4lQUQj"/></div></div></figure><p id="02a2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">借助 K-means，我们现在可以对上述驾驶员数据集进行额外分析，以产生预测性见解，帮助企业对驾驶员的行驶距离和超速模式进行分类。在我们的例子中，我们将 K-means 应用于我们自己的虚拟数据集，以节省更多的特征工程现实生活数据的步骤。</p><h1 id="00a5" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">k 均值</h1><p id="a091" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">在我们开始构建我们的数据之前，让我们花一些时间来回顾一下 K-means 实际上是什么。K-means 是一种在未标记的数据集内寻找一定数量的聚类的算法[2]。注意“未标记”这个词。这意味着 K-means 是一个无监督的学习模型。当你得到数据却不知道如何标注时，这非常有用。K-means 可以帮你标记群组——非常酷！</p><h1 id="f147" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">应用非线性特征工程</h1><p id="a5cc" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">对于我们的数据，我们将使用来自 sklearn [3]的 make_circles 数据。好吧，让我们来看看例子:</p><pre class="lz ma mb mc gt mi mj mk ml aw mm bi"><span id="fcaf" class="mn kw it mj b gy mo mp l mq mr">#Load up our packages<br/>import pandas as pd<br/>import numpy as np<br/>import sklearn<br/>import scipy<br/>import seaborn as sns<br/>from sklearn.cluster import KMeans<br/>from sklearn.preprocessing import OneHotEncoder<br/>from scipy.spatial import Voronoi, voronoi_plot_2d<br/>from sklearn.data sets.samples_generator import make_circles<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>import matplotlib.pyplot as plt<br/>%matplotlib notebook</span></pre><p id="417b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们的下一步是使用创建一个 K-means 类。对于那些不熟悉类(不是你在学校学的科目)的人来说，把编码中的类想象成一个超级函数，里面有很多函数。现在，我知道 sklearn 中已经有一个 k-means 聚类算法，但是我非常喜欢 Alice Zheng 的这个类，因为它有详细的注释和我们很快就会看到的可视化效果[4]:</p><pre class="lz ma mb mc gt mi mj mk ml aw mm bi"><span id="c0fa" class="mn kw it mj b gy mo mp l mq mr">class KMeansFeaturizer:<br/>    """Transforms numeric data into k-means cluster memberships.<br/>    <br/>    This transformer runs k-means on the input data and converts each data point<br/>    into the id of the closest cluster. If a target variable is present, it is <br/>    scaled and included as input to k-means in order to derive clusters that<br/>    obey the classification boundary as well as group similar points together.</span><span id="4a08" class="mn kw it mj b gy ms mp l mq mr">    Parameters<br/>    ----------<br/>    k: integer, optional, default 100<br/>        The number of clusters to group data into.</span><span id="ea4a" class="mn kw it mj b gy ms mp l mq mr">    target_scale: float, [0, infty], optional, default 5.0<br/>        The scaling factor for the target variable. Set this to zero to ignore<br/>        the target. For classification problems, larger `target_scale` values <br/>        will produce clusters that better respect the class boundary.</span><span id="69a0" class="mn kw it mj b gy ms mp l mq mr">    random_state : integer or numpy.RandomState, optional<br/>        This is passed to k-means as the generator used to initialize the <br/>        kmeans centers. If an integer is given, it fixes the seed. Defaults to <br/>        the global numpy random number generator.</span><span id="0bb9" class="mn kw it mj b gy ms mp l mq mr">    Attributes<br/>    ----------<br/>    cluster_centers_ : array, [k, n_features]<br/>        Coordinates of cluster centers. n_features does count the target column.<br/>    """</span><span id="8bb5" class="mn kw it mj b gy ms mp l mq mr">    def __init__(self, k=100, target_scale=5.0, random_state=None):<br/>        self.k = k<br/>        self.target_scale = target_scale<br/>        self.random_state = random_state<br/>        self.cluster_encoder = OneHotEncoder().fit(np.array(range(k)).reshape(-1,1))<br/>        <br/>    def fit(self, X, y=None):<br/>        """Runs k-means on the input data and find centroids.</span><span id="ccd4" class="mn kw it mj b gy ms mp l mq mr">        If no target is given (`y` is None) then run vanilla k-means on input<br/>        `X`. </span><span id="c0d6" class="mn kw it mj b gy ms mp l mq mr">        If target `y` is given, then include the target (weighted by <br/>        `target_scale`) as an extra dimension for k-means clustering. In this <br/>        case, run k-means twice, first with the target, then an extra iteration<br/>        without.</span><span id="eabf" class="mn kw it mj b gy ms mp l mq mr">        After fitting, the attribute `cluster_centers_` are set to the k-means<br/>        centroids in the input space represented by `X`.</span><span id="6e82" class="mn kw it mj b gy ms mp l mq mr">        Parameters<br/>        ----------<br/>        X : array-like or sparse matrix, shape=(n_data_points, n_features)</span><span id="88ea" class="mn kw it mj b gy ms mp l mq mr">        y : vector of length n_data_points, optional, default None<br/>            If provided, will be weighted with `target_scale` and included in <br/>            k-means clustering as hint.<br/>        """<br/>        if y is None:<br/>            # No target variable, just do plain k-means<br/>            km_model = KMeans(n_clusters=self.k, <br/>                              n_init=20, <br/>                              random_state=self.random_state)<br/>            km_model.fit(X)</span><span id="acbf" class="mn kw it mj b gy ms mp l mq mr">            self.km_model_ = km_model<br/>            self.cluster_centers_ = km_model.cluster_centers_<br/>            return self</span><span id="4096" class="mn kw it mj b gy ms mp l mq mr">        # There is target information. Apply appropriate scaling and include<br/>        # into input data to k-means            <br/>        data_with_target = np.hstack((X, y[:,np.newaxis]*self.target_scale))</span><span id="3591" class="mn kw it mj b gy ms mp l mq mr">        # Build a pre-training k-means model on data and target<br/>        km_model_pretrain = KMeans(n_clusters=self.k, <br/>                                   n_init=20, <br/>                                   random_state=self.random_state)<br/>        km_model_pretrain.fit(data_with_target)</span><span id="8bf3" class="mn kw it mj b gy ms mp l mq mr">        # Run k-means a second time to get the clusters in the original space<br/>        # without target info. Initialize using centroids found in pre-training.<br/>        # Go through a single iteration of cluster assignment and centroid <br/>        # recomputation.<br/>        km_model = KMeans(n_clusters=self.k, <br/>                          init=km_model_pretrain.cluster_centers_[:,:2], <br/>                          n_init=1, <br/>                          max_iter=1)<br/>        km_model.fit(X)<br/>        <br/>        self.km_model = km_model<br/>        self.cluster_centers_ = km_model.cluster_centers_<br/>        return self<br/>        <br/>    def transform(self, X, y=None):<br/>        """Outputs the closest cluster id for each input data point.</span><span id="f42b" class="mn kw it mj b gy ms mp l mq mr">        Parameters<br/>        ----------<br/>        X : array-like or sparse matrix, shape=(n_data_points, n_features)</span><span id="a7eb" class="mn kw it mj b gy ms mp l mq mr">        y : vector of length n_data_points, optional, default None<br/>            Target vector is ignored even if provided.</span><span id="898a" class="mn kw it mj b gy ms mp l mq mr">        Returns<br/>        -------<br/>        cluster_ids : array, shape[n_data_points,1]<br/>        """<br/>        clusters = self.km_model.predict(X)<br/>        return self.cluster_encoder.transform(clusters.reshape(-1,1))<br/>    <br/>    def fit_transform(self, X, y=None):<br/>        """Runs fit followed by transform.<br/>        """<br/>        self.fit(X, y)<br/>        return self.transform(X, y)</span></pre><p id="f1f7" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">不要让大量的文本困扰你。我只是把它放在那里，以防你想在你自己的项目中试验它。之后，我们将创建我们的训练/测试集，并将种子设置为 420 以获得相同的结果:</p><pre class="lz ma mb mc gt mi mj mk ml aw mm bi"><span id="8397" class="mn kw it mj b gy mo mp l mq mr"># Creating our training and test set<br/>seed = 420</span><span id="76e3" class="mn kw it mj b gy ms mp l mq mr">training_data, training_labels = make_circles(n_samples=2000, factor=0.2)</span><span id="7569" class="mn kw it mj b gy ms mp l mq mr">kmf_hint = KMeansFeaturizer(k=100, target_scale=10, random_state=seed).fit(training_data, training_labels)</span><span id="9177" class="mn kw it mj b gy ms mp l mq mr">kmf_no_hint = KMeansFeaturizer(k=100, target_scale=0, random_state=seed).fit(training_data, training_labels)</span><span id="69b3" class="mn kw it mj b gy ms mp l mq mr">def kmeans_voronoi_plot(X, y, cluster_centers, ax):<br/>    #Plots Voronoi diagram of k-means clusters overlaid with data<br/>    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', alpha=0.2)<br/>    vor = Voronoi(cluster_centers)<br/>    voronoi_plot_2d(vor, ax=ax, show_vertices=False, alpha=0.5)</span></pre><p id="4b4d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，让我们来看看未标记的非线性数据:</p><pre class="lz ma mb mc gt mi mj mk ml aw mm bi"><span id="37d3" class="mn kw it mj b gy mo mp l mq mr">#looking at circles data<br/>df = pd.DataFrame(training_data)<br/>ax = sns.scatterplot(x=0, y=1, data=df)</span></pre><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/adda3b8b7118dd97e81bdf0fbd47e404.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*gRp9ZLhNIGFho8ur"/></div></figure><p id="a68c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">就像我们司机的 into 数据集一样，我们的圈内圈外绝对不是线性数据集。接下来，我们将应用 K-means 比较视觉结果，给它一个关于我们所想的提示和没有提示:</p><pre class="lz ma mb mc gt mi mj mk ml aw mm bi"><span id="79a4" class="mn kw it mj b gy mo mp l mq mr">#With hint<br/>fig = plt.figure()<br/>ax = plt.subplot(211, aspect='equal')<br/>kmeans_voronoi_plot(training_data, training_labels, kmf_hint.cluster_centers_, ax)<br/>ax.set_title('K-Means with Target Hint')</span><span id="5d77" class="mn kw it mj b gy ms mp l mq mr">#Without hint<br/>ax2 = plt.subplot(212, aspect='equal')<br/>kmeans_voronoi_plot(training_data, training_labels, kmf_no_hint.cluster_centers_, ax2)<br/>ax2.set_title('K-Means without Target Hint')</span></pre><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/707db6608370a47391631d470b91834a.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/0*MzQh-DaD7LhtiI1U"/></div></figure><p id="2481" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我发现在有提示和没有提示的情况下，结果相当接近。如果您想要更多的自动化，那么您可能不想应用任何提示。但是如果你能花些时间看看你的数据集给它一点提示，我会的。原因是它可以节省你运行模型的时间，所以 k-means 花更少的时间自己计算。给 k-means 一个提示的另一个原因是，您在您的数据集中有领域专业知识，并且知道有特定数量的聚类。</p><h1 id="97fe" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">用于分类的模型堆叠</h1><p id="028b" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">有趣的部分到了——制作堆叠模型。有些人可能会问，堆叠模型和系综模型有什么区别。集成模型将多个机器学习模型结合起来，形成另一个模型[5]。所以，不多。我认为模型堆叠在这里更精确，因为 k-means 正在进入逻辑回归。如果我们能画一个文氏图，我们会在系综模型的概念中找到堆叠的模型。我在谷歌图片上找不到一个好的例子，所以我运用 MS paint 的魔力来呈现一个粗略的插图，供你观赏。</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mv"><img src="../Images/afda3d088633f2d8d370728de8522ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AGPKCC3BiNLnyK7i"/></div></div></figure><p id="cef3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好了，美术课结束，回到编码。我们要做一个 kNN 的 ROC 曲线，逻辑回归(LR)，k 均值馈入逻辑回归。</p><pre class="lz ma mb mc gt mi mj mk ml aw mm bi"><span id="dc28" class="mn kw it mj b gy mo mp l mq mr">#Generate test data from same distribution of training data<br/>test_data, test_labels = make_moons(n_samples=2000, noise=0.3, random_state=seed+5)</span><span id="7385" class="mn kw it mj b gy ms mp l mq mr">training_cluster_features = kmf_hint.transform(training_data)<br/>test_cluster_features = kmf_hint.transform(test_data)</span><span id="4feb" class="mn kw it mj b gy ms mp l mq mr">training_with_cluster = scipy.sparse.hstack((training_data, training_cluster_features))<br/>test_with_cluster = scipy.sparse.hstack((test_data, test_cluster_features))</span><span id="01f8" class="mn kw it mj b gy ms mp l mq mr">#Run the models<br/>lr_cluster = LogisticRegression(random_state=seed).fit(training_with_cluster, training_labels)</span><span id="3738" class="mn kw it mj b gy ms mp l mq mr">classifier_names = ['LR',<br/>                    'kNN']<br/>classifiers = [LogisticRegression(random_state=seed),<br/>               KNeighborsClassifier(5)]<br/>for model in classifiers:<br/>    model.fit(training_data, training_labels)   <br/>    <br/>#Plot the ROC<br/>def test_roc(model, data, labels):<br/>    if hasattr(model, "decision_function"):<br/>        predictions = model.decision_function(data)<br/>    else:<br/>        predictions = model.predict_proba(data)[:,1]<br/>    fpr, tpr, _ = sklearn.metrics.roc_curve(labels, predictions)<br/>    return fpr, tpr</span><span id="9754" class="mn kw it mj b gy ms mp l mq mr">plt.figure()<br/>fpr_cluster, tpr_cluster = test_roc(lr_cluster, test_with_cluster, test_labels)<br/>plt.plot(fpr_cluster, tpr_cluster, 'r-', label='LR with k-means')</span><span id="8c7c" class="mn kw it mj b gy ms mp l mq mr">for i, model in enumerate(classifiers):<br/>    fpr, tpr = test_roc(model, test_data, test_labels)<br/>    plt.plot(fpr, tpr, label=classifier_names[i])<br/>    <br/>plt.plot([0, 1], [0, 1], 'k--')<br/>plt.legend()<br/>plt.xlabel('False Positive Rate', fontsize=14)<br/>plt.ylabel('True Positive Rate', fontsize=14)</span></pre><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/830bdb06073bab8f8615657f64f60eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/0*fT_II6GqfzfrFkrU"/></div></figure><p id="e8aa" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">好吧，我第一次看到 ROC 曲线的时候，我就想我该怎么读这个东西？你想要的是最快到达左上角的模型。在这种情况下，我们最精确的模型是堆叠模型——带 k 均值的线性回归。我们的模型工作的分类是挑选哪里，哪个数据点属于大圆或小圆。</p><h1 id="21aa" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">结论</h1><p id="e671" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">唷，我们在这里讨论了相当多的事情。首先，我们来看看现实世界中可能会遇到的非线性数据和例子。其次，我们将 k-means 作为一种工具来发现更多以前没有的数据特征。接下来，我们将 k-means 应用于我们自己的数据集。最后，我们将 k-means 堆叠到逻辑回归中以建立一个更好的模型。总的来说很酷。需要注意的是，我们没有对模型进行调优，这会改变性能，我们也没有比较那么多模型。但是，将无监督学习结合到监督模型中可能会证明非常有用，并帮助您提供通过其他方式无法获得的洞察力！</p><p id="9e96" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">免责声明:本文陈述的所有内容均为我个人观点，不代表任何雇主。还撒了附属链接。</p><p id="1fec" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">[1] A，特雷维尼奥，K-means 聚类介绍(2016)，【https://www.datascience.com/blog/k-means-clustering T2】</p><p id="d23a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">[2] J，VanderPlas，Python 数据科学手册:处理数据的基本工具(2016)，<a class="ae my" href="https://amzn.to/2SMdZue" rel="noopener ugc nofollow" target="_blank">https://amzn.to/2SMdZue</a></p><p id="d362" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">[3] Scikit-learn 开发者，sk learn . data sets . make _ circles(2019)，<a class="ae my" href="https://datalab.cc/foundations/toc?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><br/></a><a class="ae my" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles" rel="noopener ugc nofollow" target="_blank">https://Scikit-learn . org/stable/modules/generated/sk learn . data sets . make _ circles . html # sk learn . data sets . make _ circles</a></p><p id="5421" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">[4] A，郑等，面向机器学习的特征工程:面向数据科学家的原理与技术(2018)，<a class="ae my" href="https://amzn.to/2SOFh3q" rel="noopener ugc nofollow" target="_blank"/></p><p id="fd2e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">[5] F，Gunes，为什么堆叠系综模型会赢得数据科学竞赛？(2017)，<a class="ae my" href="https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/" rel="noopener ugc nofollow" target="_blank">https://blogs . SAS . com/content/subcivily musings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/</a></p></div></div>    
</body>
</html>