<html>
<head>
<title>In-place Operations in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 的就地操作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/in-place-operations-in-pytorch-f91d493e970e?source=collection_archive---------7-----------------------#2019-07-10">https://towardsdatascience.com/in-place-operations-in-pytorch-f91d493e970e?source=collection_archive---------7-----------------------#2019-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/30cda8e4bddbd2afd205446157e76c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lv48q7cD2RSG3c9C5J0Xwg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://www.pexels.com/@fancycrave?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Fancycrave.com </a>from <a class="ae jg" href="https://www.pexels.com/photo/green-ram-card-collection-825262/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><div class=""/><div class=""><h2 id="8220" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">它们是什么，为什么要避开它们</h2></div><p id="2827" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">今天先进的深度神经网络有数百万个可训练参数(例如，参见本文中的比较)，试图在 Kaggle 或 Google Colab 等免费 GPU 上训练它们往往会导致 GPU 上的内存耗尽。有几种简单的方法可以减少模型占用的 GPU 内存，例如:</p><ul class=""><li id="bf4a" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">考虑改变模型的架构或使用具有较少可训练参数的模型类型(例如，选择<a class="ae jg" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank"> DenseNet-121 而不是 DenseNet-169 </a>)。这种方法会影响模型的性能指标。</li><li id="a81f" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">减少批处理大小或手动设置数据加载器工作进程的数量。在这种情况下，模型需要更长的训练时间。</li></ul><p id="f612" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在神经网络中使用就地操作可能有助于避免上述方法的缺点，同时节省一些 GPU 内存。但是，由于几个原因，不建议使用就地操作。</p><p id="9a59" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我想:</p><ul class=""><li id="4512" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">描述什么是就地操作，并演示它们如何帮助节省 GPU 内存。</li><li id="7fe6" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">告诉我们为什么应该避免就地操作或非常谨慎地使用它们。</li></ul><h1 id="bfc9" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">就地操作</h1><blockquote class="na nb nc"><p id="598b" class="ky kz nd la b lb lc kk ld le lf kn lg ne li lj lk nf lm ln lo ng lq lr ls lt im bi translated">“就地操作是直接改变给定的线性代数、向量、矩阵(张量)的内容，而不制作副本的操作。”—定义摘自<a class="ae jg" href="https://www.tutorialspoint.com/inplace-operator-in-python" rel="noopener ugc nofollow" target="_blank">本 Python 教程</a>。</p></blockquote><p id="a5db" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据定义，就地操作不会复制输入。这就是为什么在处理高维数据时，它们可以帮助减少内存使用。</p><p id="e92f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我想演示就地操作如何帮助消耗更少的 GPU 内存。为此，我将使用这个简单的函数来测量 PyTorch 中的<a class="ae jg" href="https://pytorch.org/docs/stable/nn.html#relu" rel="noopener ugc nofollow" target="_blank">非适当位置 ReLU 和适当位置 ReLU 的分配内存:</a></p><figure class="nh ni nj nk gt iv"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Function to measure the allocated memory</figcaption></figure><p id="8c47" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">调用函数来测量为不合适的 ReLU 分配的内存:</p><figure class="nh ni nj nk gt iv"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Measure the allocated memory for the out-of-place ReLU</figcaption></figure><p id="dda8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我收到如下输出:</p><pre class="nh ni nj nk gt nn no np nq aw nr bi"><span id="bc0c" class="ns mj jj no b gy nt nu l nv nw">Allocated memory: 382.0<br/>Allocated max memory: 382.0</span></pre><p id="3bf1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后调用就地 ReLU，如下所示:</p><figure class="nh ni nj nk gt iv"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Measure the allocated memory for the in-place ReLU</figcaption></figure><p id="2cf1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我收到的输出如下:</p><pre class="nh ni nj nk gt nn no np nq aw nr bi"><span id="bf9f" class="ns mj jj no b gy nt nu l nv nw">Allocated memory: 0.0<br/>Allocated max memory: 0.0</span></pre><p id="3c9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来使用就地操作可以帮助我们节省一些 GPU 内存。<strong class="la jk">但是，在使用就地操作时要极其谨慎，要检查两遍。在下一部分，我将告诉你为什么。</strong></p><h1 id="498c" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">就地操作的缺点</h1><p id="9967" class="pw-post-body-paragraph ky kz jj la b lb nx kk ld le ny kn lg lh nz lj lk ll oa ln lo lp ob lr ls lt im bi translated">就地操作的主要缺点是，<strong class="la jk">它们可能会覆盖计算梯度所需的值，</strong>这意味着会破坏模型的训练过程。这就是 PyTorch 官方亲笔签名文件所说的:</p><blockquote class="na nb nc"><p id="c7f9" class="ky kz nd la b lb lc kk ld le lf kn lg ne li lj lk nf lm ln lo ng lq lr ls lt im bi translated">在亲笔签名中支持就地操作是一件困难的事情，我们不鼓励在大多数情况下使用它们。Autograd 的积极的缓冲区释放和重用使其非常有效，并且很少有就地操作实际上降低内存使用量的情况。除非你的内存压力很大，否则你可能永远都不需要使用它们。</p><p id="a722" class="ky kz nd la b lb lc kk ld le lf kn lg ne li lj lk nf lm ln lo ng lq lr ls lt im bi translated">有两个主要原因限制了就地作业的适用性:</p><p id="177f" class="ky kz nd la b lb lc kk ld le lf kn lg ne li lj lk nf lm ln lo ng lq lr ls lt im bi translated">1.就地操作可能会覆盖计算梯度所需的值。</p><p id="4a35" class="ky kz nd la b lb lc kk ld le lf kn lg ne li lj lk nf lm ln lo ng lq lr ls lt im bi translated">2.每个就地操作实际上都需要实现重写计算图。不在位置的版本只是分配新的对象并保持对旧图的引用，而在位置操作中，需要改变表示该操作的函数的所有输入的创建者。</p></blockquote><p id="84c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">小心就地操作的另一个原因是它们的实现非常棘手。这就是为什么我会推荐使用 PyTorch 标准的就地操作(就像上面的就地 ReLU ),而不是手动实现。</p><p id="7aaf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看一个<a class="ae jg" href="https://arxiv.org/pdf/1606.08415.pdf" rel="noopener ugc nofollow" target="_blank">路斯</a>(或 Swish-1)激活函数的例子。这是路斯的不合时宜的实现:</p><figure class="nh ni nj nk gt iv"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Out-of-place SiLU implementation</figcaption></figure><p id="700b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们尝试使用<code class="fe oc od oe no b"> torch.<a class="ae jg" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">sigmoid</a>_ </code>就地函数实现就地路斯:</p><figure class="nh ni nj nk gt iv"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Incorrect implementation of in-place SiLU</figcaption></figure><p id="a545" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的代码<strong class="la jk">错误地实现了</strong>就地路斯。我们可以通过比较两个函数返回的值来确定这一点。实际上，函数<code class="fe oc od oe no b">silu_inplace_1</code>返回<code class="fe oc od oe no b">sigmoid(input) * sigmoid(input)</code>！使用<code class="fe oc od oe no b">torch.sigmoid_</code>就地实施路斯的工作示例如下:</p><figure class="nh ni nj nk gt iv"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="829a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个小例子演示了为什么我们在使用就地操作时应该小心谨慎并检查两次。</p><h1 id="5b1d" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">结论</h1><p id="2fab" class="pw-post-body-paragraph ky kz jj la b lb nx kk ld le ny kn lg lh nz lj lk ll oa ln lo lp ob lr ls lt im bi translated">在本文中:</p><ul class=""><li id="d6ec" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">我描述了就地操作及其目的。演示了就地操作如何帮助减少 GPU 内存消耗。</li><li id="c817" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">我描述了就地作业的<strong class="la jk">重大缺点</strong>。人们应该非常小心地使用它们，并检查两次结果。</li></ul></div></div>    
</body>
</html>