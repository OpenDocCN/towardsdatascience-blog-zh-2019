<html>
<head>
<title>K-Means Clustering for Unsupervised Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于无监督机器学习的 k-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-for-unsupervised-machine-learning-afd96fbd37ac?source=collection_archive---------10-----------------------#2019-08-09">https://towardsdatascience.com/k-means-clustering-for-unsupervised-machine-learning-afd96fbd37ac?source=collection_archive---------10-----------------------#2019-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7ee2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">无监督学习的 Pythonic 指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/780dc56a239964c0619713ee78afc060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A9Vk37nX1n9sVkDZ"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@donramxn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ramón Salinero</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="de78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与人类历史上的任何其他技术不同，人工智能(AI)和机器学习(ML)彻底改变了我们生活的方方面面，并扰乱了我们做生意的方式。这种破坏给专业人士和企业带来了许多挑战。在本文中，我将介绍一种最常用的机器学习方法，<strong class="lb iu"> K-Means。</strong></p><h2 id="8e08" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">首先:</strong>到底什么是机器学习(ML)？！这是一种新的模式吗？</h2><p id="83c3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">机器学习是一种科学方法，它利用统计方法和机器的计算能力将数据转换为人类或机器本身可以用来采取特定行动的智慧。<em class="mt">“它是</em> <a class="ae ky" href="https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html" rel="noopener ugc nofollow" target="_blank"> <em class="mt">人工智能</em> </a> <em class="mt">的一个分支，基于系统可以从数据中学习、识别模式并在最少人工干预的情况下做出决策的想法。”</em> <a class="ae ky" href="https://www.sas.com/en_us/insights/analytics/machine-learning.html" rel="noopener ugc nofollow" target="_blank"> <em class="mt"> (SaS) </em> </a></p><p id="2abc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你认为人工智能是一种新的范式，你应该知道机器学习这个名字是亚瑟·塞缪尔在 1959 年创造的。然而，这发生在 20 世纪 50 年代艾伦·图灵的一个提议之后，他用“机器能思考吗？”机器能做我们(作为思维实体)能做的事情吗？，或者换句话说，“机器能学习吗？”</p><p id="9328" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，ML 已经存在半个世纪了。然而，随着最近机器计算能力的进步，以及我们正在生成、收集和存储的大量数据，ML 已经成为许多行业的下一个大事件。</p><h2 id="2898" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">机器学习的主要领域有哪些？</h2><p id="a63e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">ML 中有许多字段，但我们可以将三个主要字段命名为:</p><p id="8ce3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">监督学习(SL): </strong> SL 是使用一组输入(预测器)和期望输出(目标)来建立和训练 ML 模型。许多回归(简单或多元)或分类模型都属于这一类。</p><p id="c75a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">无监督学习(UL): </strong>当目标未知时，使用 UL，目标是推断数据中的模式或趋势，从而做出决策，或者有时将问题转化为 SL 问题(也称为<em class="mt">迁移学习，TL </em>)。这篇文章的重点是 UL 聚类，特别是 K-Means 方法。</p><p id="abfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">强化学习(RL) </strong> </a> <strong class="lb iu"> : </strong>这种范式比 SL 和 UL 更复杂，然而<a class="ae ky" href="https://www.oreilly.com/ideas/reinforcement-learning-explained" rel="noopener ugc nofollow" target="_blank">这篇文章提供了 RL </a>的一个简单而技术性的定义。一般来说，RL 关注的是“主体”(例如模型)如何在环境中采取行动，并且在每个步骤中试图最大化回报(例如优化函数)。RL 的一个很好的例子是使用遗传算法和蛮力的路由优化(在后面的文章中会有更多的介绍)。</p><p id="1758" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阿卜杜勒·瓦希德的下图很好地展示了曼梯·里的这些主要领域。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/89569d8d50dde8811d6487a62f907b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Jvw-YXrCum63uUar"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">From <a class="ae ky" href="https://www.slideshare.net/awahid/big-data-and-machine-learning-for-businesses" rel="noopener ugc nofollow" target="_blank">https://www.slideshare.net/awahid/big-data-and-machine-learning-for-businesses</a>, Credit: <a class="ae ky" href="https://www.slideshare.net/awahid?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideview" rel="noopener ugc nofollow" target="_blank">Abdul Wahid</a></figcaption></figure><h2 id="1dba" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">k-均值聚类，已定义</h2><p id="8d90" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu"><em class="mt">k</em>-均值聚类</strong>是一种来自信号处理的方法，目的是将观测值放入<em class="mt"> k </em>个聚类中，其中每个观测值属于一个具有最近均值的聚类。这些簇在数学上也被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Voronoi_cell" rel="noopener ugc nofollow" target="_blank"> Voronoi 细胞</a>。</p><p id="b484" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在进入 Python 代码的细节之前，让我们看一下 K-Means 聚类的基础。</p><h2 id="88b3" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">K-Means 如何对观察值进行聚类？</h2><p id="9aa9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">聚类算法的主要输入是聚类的数量(这里称为<em class="mt"> k </em>)。<em class="mt"> k </em>决定集群机制，以及集群如何形成。在你知道哪一个应该属于一个聚类之前，想出聚类的数量可能是具有挑战性的，尤其是因为你正在处理一个无监督的学习问题。</p><p id="d96d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有其他无监督学习方法来确定 K-Means 聚类方法的正确聚类数，包括<a class="ae ky" href="https://en.wikipedia.org/wiki/Hierarchical_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="mt">分层聚类</em> </strong>，</a>，但我们在本文中不涉及该主题。我们的假设是您知道集群的数量，或者对集群的正确数量有一个大致的概念。最好的方法是进行几次试错，以找到最佳的聚类数。</p><p id="83ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦知道了聚类的数量，就有三种不同的方法来指定聚类中心:</p><ul class=""><li id="63ad" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated">手动，</li><li id="c818" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">随机地，而且</li><li id="81e9" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">“<em class="mt">k-在 SKLearn 中的意思是++ </em></li></ul><p id="ee51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">后者以一种智能的方式为 k-mean 聚类选择初始聚类中心以加速收敛。<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">你可以在这里找到更多。</a></p><p id="4103" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应当注意，初始聚类中心对最终聚类结果没有任何影响，原因将在下面解释。给定初始聚类中心，该算法重复以下步骤，直到它收敛:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/6227ec70f2abcef52c6901b54818e451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/0*o6ClujPx3f2WP0I2.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Illustration of K-Means Algorithm, Wikipedia Creative Commons, credit: <a class="ae ky" href="https://commons.wikimedia.org/wiki/User:Chire" rel="noopener ugc nofollow" target="_blank">Chire</a></figcaption></figure><ul class=""><li id="6727" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated"><strong class="lb iu">分配步骤</strong>:将每个观察值分配给其平均值具有最小平方<a class="ae ky" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>的组，这是直观的“最近”平均值。</li><li id="f4d9" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><strong class="lb iu">更新步骤</strong>:计算新聚类中观测值的新均值(<a class="ae ky" href="https://en.wikipedia.org/wiki/Centroids" rel="noopener ugc nofollow" target="_blank">质心</a>)。</li><li id="dca3" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><strong class="lb iu">检查收敛:</strong>当分配不再改变时，算法假定收敛。</li></ul><p id="5b81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要记住的一点是，K-Means 几乎总是收敛的，但不能保证找到最优解，因为它在局部最小值处终止循环，可能不会达到全局最小状态。</p><p id="91af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧！算法说够了。让我们进入令人兴奋的部分，也就是 Python 代码。</p><h2 id="6e29" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">关于 K-均值数据缩放的注记</h2><p id="1dcb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">由于 K-Means 基于数据点到聚类中心的距离工作，因此将数据缩放到相同的比例对于结果的准确性至关重要。</p><h2 id="6630" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">Python 中的 K-Means</h2><p id="3ef5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为此，我们将使用<strong class="lb iu"> SciKit Learn </strong>库。你可以在这里阅读 K-Means 聚类包的文档。</p><p id="3e92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们先导入包。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="540e" class="lv lw it nl b gy np nq l nr ns">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="6914" class="lv lw it nl b gy nt nq l nr ns">from sklearn.cluster import KMeans<br/>from sklearn.datasets import make_blobs</span></pre><p id="e624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了说明这个算法是如何工作的，我们将使用<code class="fe nu nv nw nl b">sklearn.datasets.</code>中的<code class="fe nu nv nw nl b">make_blob</code>包。下面的代码片段将生成 5 个集群。在我们的聚类中，我们将不使用聚类名称(y)。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="b8bf" class="lv lw it nl b gy np nq l nr ns"># Create 5 blobs of 2,000 random data<br/>n_samples = 2000<br/>random_state = 42<br/>X, y = make_blobs(n_samples=n_samples, <br/>                  random_state=random_state, <br/>                  centers=5)</span></pre><p id="751e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们可视化集群，看看他们在哪里。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="8fc4" class="lv lw it nl b gy np nq l nr ns"># Plot the random blub data</span><span id="be84" class="lv lw it nl b gy nt nq l nr ns">plt.figure(figsize=(6, 6))</span><span id="7d2f" class="lv lw it nl b gy nt nq l nr ns">plt.scatter(X[:, 0], X[:, 1], s=5)<br/>plt.title(f"No Clusters Assigned")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a2b8de9462d1aeb8f0d85a15c6f0ac25.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*OX0o8mh3iURmWUEwYHaGMQ.png"/></div></figure><p id="d94b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">观察这些斑点，我们可以看到我们有三个不同的“区域”，由 5 个斑点组成:</p><ul class=""><li id="7da1" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated">左下角有一个斑点，</li><li id="8170" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">在左上区域中有两个彼此大致邻近的斑点，并且</li><li id="b293" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">在中间右区有两个几乎重叠的斑点。</li></ul><p id="59e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看 K-Means 聚类如何处理这个问题。我们将查看不同的集群编号，从 1 到 10。下面提供了代码，生成的图形放在下面的动画中。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="3689" class="lv lw it nl b gy np nq l nr ns"># Plot the data and color code based on clusters</span><span id="0309" class="lv lw it nl b gy nt nq l nr ns"># changing the number of clusters <br/>for i in range(1,11):<br/>    plt.figure(figsize=(6, 6))<br/>    <br/>    # Predicting the clusters<br/>    y_pred = KMeans(n_clusters=i, random_state=random_state).fit_predict(X)</span><span id="621b" class="lv lw it nl b gy nt nq l nr ns"># plotting the clusters<br/>    plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=5)<br/>    plt.title(f"Number of Clusters: {i}")</span><span id="27d4" class="lv lw it nl b gy nt nq l nr ns">plt.show();</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e595671f1b780b0cf29ecb319df629f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*QMdZNmm5xg0q2OgvJBOJ_Q.gif"/></div></figure><p id="e968" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动画情节是用<code class="fe nu nv nw nl b">Image.Io</code>包制作的。有关这方面的更多信息，请参考<a class="nz oa ep" href="https://medium.com/u/87e07c82677a?source=post_page-----afd96fbd37ac--------------------------------" rel="noopener" target="_blank"> Johannes Huessy </a>博客(<a class="ae ky" href="https://medium.com/swlh/animating-visualizations-in-python-with-matplotlib-and-imageio-10a14c571e81" rel="noopener">点击此处</a>)。</p><h2 id="246e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">评估 K-Means 聚类算法</h2><p id="2169" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">所以你已经完成了聚类，但是这个聚类有多好，你如何衡量算法的性能？</p><p id="0d3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">惯性:</strong>我们在上一节中讨论了一个度量，即到聚类中心的距离的类内平方和。这就是所谓的<strong class="lb iu"> <em class="mt">惯性</em> </strong>。该算法旨在选择使<strong class="lb iu">、</strong>惯性最小化的质心，其中<strong class="lb iu">、</strong>可以被识别为内部一致性聚类如何的度量。</p><p id="26aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用下面的代码来获得集群的<code class="fe nu nv nw nl b">inertia</code>分数:</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="ffa9" class="lv lw it nl b gy np nq l nr ns">km = KMeans(n_clusters=i, random_state=random_state)<br/>km.fit(X)<br/>km.inertia_</span></pre><p id="48e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码计算了我们之前所做的 10 个不同聚类数的<code class="fe nu nv nw nl b">inertia</code>分数，并将它们保存在一个列表中，我们将使用该列表进行绘制(稍后将详细介绍)。惯性分数对聚类数的作图称为“<strong class="lb iu"> <em class="mt">肘形曲线</em> </strong>”。</p><p id="5b7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">剪影评分:</strong>剪影评分是基于聚类<strong class="lb iu"><em class="mt"/></strong>(聚类中的点相对于彼此有多近)和<strong class="lb iu"> <em class="mt">分离</em> </strong>(聚类相对于彼此有多远)<strong class="lb iu">的组合。</strong></p><p id="47f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">剪影得分在-1(聚类差)和+1(聚类优)之间。</p><p id="434c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">惯性</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="898d" class="lv lw it nl b gy np nq l nr ns"># Calculating the inertia and silhouette_score¶</span><span id="e118" class="lv lw it nl b gy nt nq l nr ns">inertia = []<br/>sil = []</span><span id="addd" class="lv lw it nl b gy nt nq l nr ns"># changing the number of clusters <br/>for k in range(2,11):<br/>    <br/>    km = KMeans(n_clusters=k, random_state=random_state)<br/>    km.fit(X)<br/>    y_pred = km.predict(X)<br/>    <br/>    inertia.append((k, km.inertia_))<br/>    sil.append((k, silhouette_score(X, y_pred)))</span></pre><p id="2a4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了惯性和轮廓分数，让我们绘制它们并评估聚类算法的性能。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="8a4f" class="lv lw it nl b gy np nq l nr ns">fig, ax = plt.subplots(1,2, figsize=(12,4))</span><span id="31b0" class="lv lw it nl b gy nt nq l nr ns"># Plotting Elbow Curve<br/>x_iner = [x[0] for x in inertia]<br/>y_iner  = [x[1] for x in inertia]<br/>ax[0].plot(x_iner, y_iner)<br/>ax[0].set_xlabel('Number of Clusters')<br/>ax[0].set_ylabel('Intertia')<br/>ax[0].set_title('Elbow Curve')</span><span id="bc0e" class="lv lw it nl b gy nt nq l nr ns"># Plotting Silhouetter Score<br/>x_sil = [x[0] for x in sil]<br/>y_sil  = [x[1] for x in sil]<br/>ax[1].plot(x_sil, y_sil)<br/>ax[1].set_xlabel('Number of Clusters')<br/>ax[1].set_ylabel('Silhouetter Score')<br/>ax[1].set_title('Silhouetter Score Curve')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/266247fb2f5367e2bfb46d3f9e1a794a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9SW9Jed6HpN8IZzRPbnD9Q.png"/></div></div></figure><p id="0aa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以看到，当你增加集群的数量时，惯性得分总是下降。然而，肘曲线可以告诉你以上 4 个集群，惯性的变化并不显著。现在，让我们看看轮廓曲线。您可以看到，最大得分发生在 4 个聚类处(轮廓得分越高，聚类越好)。</p><p id="67b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将肘部曲线与轮廓得分曲线结合起来，可以提供对 K-Means 性能的宝贵见解。</p><h2 id="de0f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">K 均值的其他用例</h2><p id="102a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">K-Means 方法有很多用例，从图像矢量化到文本文档聚类。你可以在这里找到一些例子。</p><p id="375a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您发现本指南有助于理解使用 Python 的 SkLearn 包的 K-Means 聚类方法。请继续关注更多类似的话题！</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="3ebb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="mt">尼克·米奈博士</em> </strong> <em class="mt"> ( </em> <a class="ae ky" href="https://www.linkedin.com/in/nickminaie/" rel="noopener ugc nofollow" target="_blank"> <em class="mt">领英简介</em> </a> <em class="mt">)是一位资深顾问和富有远见的数据科学家，代表了领导技能、世界级数据科学专业知识、商业敏锐度和领导组织变革能力的独特组合。他的使命是推进人工智能(AI)和机器学习在行业中的实践。</em></p></div></div>    
</body>
</html>