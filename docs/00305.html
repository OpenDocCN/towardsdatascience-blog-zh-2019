<html>
<head>
<title>Capsule Networks: The New Deep Learning Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">胶囊网络:新的深度学习网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/capsule-networks-the-new-deep-learning-network-bd917e6818e8?source=collection_archive---------1-----------------------#2019-01-14">https://towardsdatascience.com/capsule-networks-the-new-deep-learning-network-bd917e6818e8?source=collection_archive---------1-----------------------#2019-01-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="64bf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解和使用胶囊网络的指南和介绍。</h2></div><p id="db7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">卷积网络在深度学习领域取得了巨大的成功，它们是深度学习现在如此受欢迎的主要原因！它们非常成功，但是它们的基本架构存在缺陷，导致它们不能很好地完成某些任务。</p><p id="8284" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CNN 检测图像中的特征，并学习如何利用这些信息识别物体。靠近起点的层检测非常简单的特征，如边缘，而更深的层可以检测更复杂的特征，如眼睛、鼻子或整张脸。然后，它使用所有这些它已经学会的特征，做出最终的预测。这就是这个系统的缺陷所在——没有在 CNN 的任何地方使用的<strong class="kk iu">空间信息</strong>,并且用于连接层的池功能<strong class="kk iu">真的真的效率很低吗</strong>。</p><blockquote class="le lf lg"><p id="264d" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">卷积神经网络中使用的池操作是一个很大的错误，它如此有效的事实是一场灾难。<a class="ae ll" href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyj4jv" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></blockquote><h2 id="f0b8" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">最大池化</h2><p id="cd52" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">在最大池化的过程中，许多重要信息丢失，因为只有最活跃的神经元被选择移动到下一层。这种操作是有价值的空间信息在层间丢失的原因。为了解决这个问题，Hinton 建议我们使用一个叫做“协议路由”的过程。这意味着较低级别的特征(手指、眼睛、嘴)将只被发送到与其内容相匹配的较高级别的层。如果它包含的特征类似于眼睛或嘴巴，它将被发送到“脸”，或者如果它包含手指和手掌，它将被发送到“手”。这一完整的解决方案将空间信息编码为要素，同时还使用动态路由(协议路由)，由深度学习领域最有影响力的人之一 Geoffrey Hinton 在 NIPS 2017 上展示；<strong class="kk iu">胶囊网络。</strong></p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h2 id="dbc8" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">胶囊</h2><p id="f0d1" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">当我们在计算机图形学中通过<strong class="kk iu">渲染</strong>来构造对象时，我们必须指定并提供某种几何信息，告诉计算机<strong class="kk iu">在哪里</strong>绘制对象、该对象的<strong class="kk iu">比例</strong>、其<strong class="kk iu">角度、</strong>以及其他空间信息。这些信息都在屏幕上显示为一个对象。但是，如果我们仅仅通过观察图像中的一个物体就能提取这些信息，那会怎么样呢？这是胶囊网络所基于的过程，<strong class="kk iu">逆渲染。</strong></p><p id="bd59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看<strong class="kk iu">胶囊</strong>以及它们如何着手解决提供空间信息的问题。</p><p id="3066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们看到 CNN 背后的一些逻辑时，我们开始注意到它的架构的失败之处。看看这张照片。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/12fcea9f2321daa9841d7c2febe83a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*b5Gj44sqne2Cu6Xra5EJpg.jpeg"/></div></div></figure><p id="99f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它看起来不太适合一张脸，尽管它拥有组成一张脸的所有必要成分。我们知道这不是人脸应该有的样子，但是因为 CNN 只在图像中寻找特征，而不注意他们的<strong class="kk iu">姿势</strong>，所以他们很难注意到那张脸和真实脸之间的区别。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3d23f7b5cb06f924d2a18adf4c04cf66.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*t2Qx7SrOFNFvz9Nv_aITeA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">How a CNN would classify this image. <a class="ae ll" rel="noopener" target="_blank" href="/a-simple-and-intuitive-explanation-of-hintons-capsule-networks-b59792ad46b1">Source</a>.</figcaption></figure><p id="2735" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">胶囊网络如何解决这个问题是通过实现神经元的<strong class="kk iu">组来编码空间信息以及物体存在的概率。</strong>胶囊向量的长度是特征存在于图像中的概率，向量的方向将代表其<a class="ae ll" href="https://en.wikipedia.org/wiki/Pose_(computer_vision)" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">姿态信息</strong> </a> <strong class="kk iu">。</strong></p><blockquote class="le lf lg"><p id="c4d0" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">胶囊是一组神经元，其活动向量表示诸如对象或对象部分的特定类型的实体的实例化参数。我们用活动向量的长度来表示实体存在的概率，用它的方向来表示实例化参数。— <a class="ae ll" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></blockquote><p id="4d0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在设计和渲染等计算机图形应用程序中，通常通过给定某种参数来创建对象，对象将根据这些参数进行渲染。然而，在 capsules 网络中，情况正好相反，网络学习如何<strong class="kk iu">反向渲染</strong>图像；观察一幅图像，并试图预测它的实例化参数是什么。</p><p id="b7e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它通过尝试重现它认为它检测到的对象并将其与来自训练数据的标记示例进行比较，来学习如何预测这一点。通过这样做，它在预测实例化参数方面变得越来越好。Geoffrey Hinton 的论文<a class="ae ll" href="https://arxiv.org/pdf/1710.09829.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"/></a><strong class="kk iu"/>提出了使用两个损失函数，而不是只有一个。</p><p id="2ccb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这背后的主要思想是在胶囊之间产生<strong class="kk iu">等变。</strong>这意味着在图像中移动一个特征也会改变它在胶囊中的矢量表示，但不会改变它存在的概率。在较低水平的胶囊检测到特征之后，该信息被向上发送到具有良好拟合的较高水平的胶囊。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b4fea5b62cb8a669d10082f4107fd197.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*LPpIdgcUQiskly8s6zvVbw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">How a Capsule Network would classify this face. <a class="ae ll" rel="noopener" target="_blank" href="/a-simple-and-intuitive-explanation-of-hintons-capsule-networks-b59792ad46b1">Source</a>.</figcaption></figure><p id="fae9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如图所示，特征的所有姿态参数用于确定最终结果。</p><h1 id="891a" class="ni ln it bd lo nj nk nl lr nm nn no lu jz np ka lx kc nq kd ma kf nr kg md ns bi translated">胶囊内的操作</h1><p id="f040" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">您可能已经知道，神经网络中的传统神经元执行以下标量操作:</p><ol class=""><li id="9302" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">投入的加权</li><li id="e706" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">加权输入的总和</li><li id="5da6" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">非线性</li></ol><p id="dacc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这些操作在胶囊内略有变化，执行如下:</strong></p><ol class=""><li id="5180" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">输入向量与权重矩阵的矩阵乘法。这编码了图像中低层特征和高层特征之间非常重要的空间关系。</li><li id="9405" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">加权输入向量。这些权重决定了当前胶囊将把它的输出发送到哪个更高级别的胶囊。这是通过<strong class="kk iu">动态路由、</strong>过程完成的，稍后我会详细介绍。</li><li id="e690" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">加权输入向量之和。(这个没什么特别的)</li><li id="0932" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">使用“挤压”函数的非线性。该函数获取一个向量，并将其“挤压”为最大长度为 1，最小长度为 0，同时保持其方向。</li></ol><h1 id="2aa9" class="ni ln it bd lo nj nk nl lr nm nn no lu jz np ka lx kc nq kd ma kf nr kg md ns bi translated">胶囊之间的动态路由</h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="ab gu cl oh"><img src="../Images/27657ed2f2c42303149d0f2c1dc5b522.png" data-original-src="https://miro.medium.com/v2/format:webp/1*GFxHh3kldywccfJ5AgX6pw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Process of Dynamic Routing. <a class="ae ll" href="https://www.scoop.it/t/data-science-58/p/4090092973/2017/12/03/uncovering-the-intuition-behind-capsule-networks-and-inverse-graphics-part-i" rel="noopener ugc nofollow" target="_blank">Source.</a></figcaption></figure><p id="9819" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个路由过程中，较低级别的胶囊将其输入发送到与其输入“一致”的较高级别的胶囊。对于可以路由到的每个较高的胶囊，较低的胶囊通过将其自己的输出乘以权重矩阵来计算预测向量。如果预测向量与可能较高的胶囊的输出具有大的标量积，则存在自上而下的反馈，其具有增加该高级胶囊的耦合系数并降低其他胶囊的耦合系数的效果。</p><h1 id="35e2" class="ni ln it bd lo nj nk nl lr nm nn no lu jz np ka lx kc nq kd ma kf nr kg md ns bi translated">MNIST 胶囊网络体系结构</h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/b4349978084579df99fcb16ed41cd706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9fvb_xaSSqW7XVb_.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">CapsNet Architecture. <a class="ae ll" href="https://arxiv.org/pdf/1710.09829.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><h2 id="3338" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">编码器</h2><p id="6f87" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">编码器接收图像输入，并学习如何将其表示为 16 维向量，该向量包含渲染图像所需的所有信息。</p><ol class=""><li id="24cf" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">Conv 图层-检测随后由胶囊分析的要素。如论文中所提出的，包含 256 个大小为 9x9x1 的内核。</li><li id="4259" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">初级(较低的)胶囊层——这一层是我之前描述的较低水平的胶囊层。它包含 32 个不同的胶囊，每个胶囊将第八个 9×9×256 卷积核应用于前一卷积层的输出，并产生 4D 矢量输出。</li><li id="1884" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">数字(更高)胶囊层-该层是主胶囊将路由到的更高级别的胶囊层(使用动态路由)。该层输出包含重建对象所需的所有实例化参数的 16D 向量。</li></ol><h2 id="d0e9" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">解码器</h2><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/aa96b46656e0b3d4e1241cf03088b2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X3RYxSSnfDmG1H8l.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Decoder Architecture. <a class="ae ll" href="https://software.intel.com/en-us/articles/understanding-capsule-network-architecture" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="8448" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器从数字胶囊中提取 16D 向量，并学习如何解码其正在检测的对象的图像中给出的实例化参数。解码器与欧几里德距离损失函数一起使用，以确定重建的特征与训练它的实际特征相比有多相似。这确保了胶囊只保留有利于识别其向量中的数字的信息。解码器是一个非常简单的前馈神经网络，如下所述。</p><ol class=""><li id="f37e" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">全连接(密集)第 1 层</li><li id="c772" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">全连接(密集)第 2 层</li><li id="8e72" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">完全连接(密集)的第 3 层—包含 10 个类的最终输出</li></ol><h2 id="d708" class="lm ln it bd lo lp lq dn lr ls lt dp lu kr lv lw lx kv ly lz ma kz mb mc md me bi translated">为什么我们不用胶囊网络呢？</h2><p id="15de" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">虽然 CapsNet 在 MNIST 等简单数据集上取得了一流的性能，但它在 CIFAR-10 或 Imagenet 等数据集上可能存在的更复杂的数据上却举步维艰。这是因为在图像中发现的过量信息从胶囊中释放出来。</p><p id="432c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">胶囊网仍处于研究和开发阶段，还不足以可靠地用于商业任务，因为很少有证实的结果。然而，这个概念是合理的，这个领域的更多进展可能会导致深度学习图像识别的胶囊网络的标准化。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="a1a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如果你喜欢我的文章或者学到了新东西，请务必:</strong></p><ul class=""><li id="e04a" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ok nz oa ob bi translated">在<a class="ae ll" href="https://www.linkedin.com/in/aryan-misra/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。</li><li id="7cf0" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ok nz oa ob bi translated">给我发一些反馈和评论(aryanmisra@outlook.com)。</li><li id="1634" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ok nz oa ob bi translated">查看提出这个想法的<a class="ae ll" href="https://arxiv.org/pdf/1710.09829.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>。</li></ul></div></div>    
</body>
</html>