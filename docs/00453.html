<html>
<head>
<title>Deep Learning Approach for Separating Fast and Slow Components</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于分离快分量和慢分量的深度学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-approach-for-separating-fast-and-slow-components-2e6142d041dc?source=collection_archive---------19-----------------------#2019-01-20">https://towardsdatascience.com/deep-learning-approach-for-separating-fast-and-slow-components-2e6142d041dc?source=collection_archive---------19-----------------------#2019-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="18ac" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">一些背景</h1><p id="10ac" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">(这项工作的幻灯片可以在 https://speaker deck . com/jchin/decompositing-dynamics-from-different-time-scale-for-time-lapse-image-sequences-with-A-deep-CNN 找到)</p><p id="bd3b" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在经历了 9 年帮助单分子测序对科学界变得有用的冒险后，我辞去了在 PacBio 的研究员工作(见<a class="ae lj" href="https://medium.com/@infoecho/the-end-of-a-chapter-a-memoir-on-bioinformatics-development-work-for-single-molecule-sequencing-19455c288280" rel="noopener">我在 PacBio 头几年的故事</a>)。我的大部分技术/科学工作都与 DNA 序列有关。虽然有一些令人兴奋的深度学习方法可以解决一些有趣的问题，但我确实喜欢探索 DNA 测序空间之外的一些东西。</p><p id="7759" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我不久前加入了 DNAnexus。该公司已成为生物数据/测序数据处理云计算平台的领导者。我认为用该平台演示开发除 DNA 序列之外的生物数据的深度学习模型是有用的。怀着这样的目标，前首席安全官 Andrew Carroll 和我决定看看我们能为一些生物成像相关的工作做些什么。</p><p id="e916" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">当我们在寻找一些例子的时候，<a class="ae lj" href="https://en.wikipedia.org/wiki/Eugene_Myers" rel="noopener ugc nofollow" target="_blank"> Gene Myers </a>(是的，第一个完成<a class="ae lj" href="http://science.sciencemag.org/content/291/5507/1304" rel="noopener ugc nofollow" target="_blank">全人类基因组鸟枪法组装</a>的人)发表了一个工具<a class="ae lj" href="http://csbdeep.bioimagecomputing.com" rel="noopener ugc nofollow" target="_blank"> CSBdeep </a>来自德国德累斯顿<a class="ae lj" href="https://www.mpi-cbg.de/home/" rel="noopener ugc nofollow" target="_blank">马普分子细胞生物学和遗传学研究所(MPI-CBG) </a>的他的实验室<a class="ae lj" href="https://www.mpi-cbg.de/research-groups/current-groups/gene-myers/group-leader/" rel="noopener ugc nofollow" target="_blank">用于从光片共焦图像中创建超分辨率图像，用于研究生物发育过程。</a></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/09acb1364829f8fe0bd0082567dd5cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkGTwMNw7ecyAglpP6JWlg.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Example of using CSBDeep to archive super-resolution: Left: the original images. Right: the super-resolution images generated by CSBdeep. The image is provided by <a class="ae lj" href="http://www.gulilab.org" rel="noopener ugc nofollow" target="_blank">Ying Gu Lab</a>.</figcaption></figure><p id="8b6d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">受 CSBDeep 论文的启发，Andrew 联系了一位正式的合作者，<a class="ae lj" href="http://www.gulilab.org" rel="noopener ugc nofollow" target="_blank"> Ying Gu </a>，看看她是否有一些有趣的图片让我们在我们的平台上演示使用 CSBDeep。利用 DNAnexus 云计算平台，重现 CSBDeep 结果并将其应用于新图像相对容易。尽管如此，我在想我们是否可以做一些不同的，新的事情，至少对我来说。</p><h1 id="8853" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">一部电影不仅仅是静态图像，它更有趣</h1><p id="3dc5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">原来，英古的研究工作是跟踪涉及纤维素合成的特定蛋白质，解决生物能源的重要问题。我们得到的图像是追踪细胞内分子的延时电影。最初，我认为我们可能能够通过使用多帧的深度学习来实现超分辨率。虽然我们在这个问题上取得了一些初步的成功，但我却“分心”去解决另一个不同的问题。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mf"><img src="../Images/26766e4c97094fd73a5db9fe1ea2e91d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VkelHKRSFcu6Suo2XDWNg.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">In the image stack, the slow changing background contributes to the non-zero auto-correlation at longer timescales.</figcaption></figure><p id="6d13" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">当我看延时电影时，很难不注意到一些背景成分(例如，植物细胞的主干微管)，以及以不同速度移动的不同斑点或粒子。我认为可以使用深度学习(作为一种无监督的学习方法)来分离背景，慢速组件和激活组件。</p><p id="48fb" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir">在这样的延时电影中，我们怎样才能把运动部分和静止部分分开呢？</strong></p><p id="adb8" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">第一，背景图的获取其实并不太难。我们可以对堆栈中的所有图像的每个像素取平均值或中值来获得背景。为了得到前景图像，我们可以从每幅图像中减去平均背景。如果背景真的是静态的，这应该是最容易做到的事情。然而，这种方法也假设只有一个有趣的“前景”事实上，潜在的生物过程可能具有不同的组件，这些组件具有不同的动态范围，我们可能能够使用深度学习架构来分解不同的组件。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mg"><img src="../Images/eb35001fde7d0e320082dc16be9e1dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZ2K_9swwUl1_4EGoJfNYw.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk"><strong class="bd mh">From (T — </strong>∆<strong class="bd mh">t) to T: </strong>If ∆t is longer than the typically “faster” components, then we can<br/> catch the slow component using such autoencoder architecture.</figcaption></figure><h1 id="d850" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">使用多个自动编码器预测不同时间尺度的未来</h1><p id="53c2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这样的延时电影有什么背景？在深度学习神经网络架构中，自动编码器可以学习具有隐藏层的简化表示，以再现输入。训练期间的损失函数通常是输出和输入之间的 L2 差。如果我们认为背景是电影的不变部分，我们应该希望我们可以使用这样的自动编码器来学习简化的表示，该表示可以从早期的输入预测后期的输出。图像的不变部分<strong class="kn ir">在不同的时间点</strong>应该可以被自动编码器学习。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mi"><img src="../Images/9ae5132343da42c719b0504a30ab144d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KGZmw20mFqPisBAPuU5Ekg.jpeg"/></div></div></figure><p id="6adc" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们可以认为，一幅特定时间的图像可以从更早时间不同尺度的特征或成分中重建出来。我们使用自动编码器从(t-∆t)预测时间 t 的图像。如果 t 很大，那么我们希望自动编码器能够学习背景部分。我们可以用更小的 t 来学习更快的部分等等。例如，我们可以将时间 t 的图像构建为 t-8(帧)、t-4(帧)、t-2(帧)和 t-1(帧)的图像的预测组合，以捕捉不同时间尺度的贡献。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/55df466f40c23ea545361ebb2bc8c9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XZJEJQwdus2cKteto8KxGw.jpeg"/></div></div></figure><p id="cb08" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">沿着这一思路，我们测试了下面的一个建筑展示，用于分解英姑小组生成的延时电影。我认为我们得到了相当好的结果。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mk"><img src="../Images/51b3593532644d08c5c1adfb29ca4648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lB05Qx4lt7rMTsGdT38nJQ.gif"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">From left to right: (a) Original (b) Slow Components (c ) Fast Component (d) Pseudo-color composition from the slow and fast components</figcaption></figure><h1 id="b704" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">其他相关作品</h1><p id="e964" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">虽然我认为我们提出的方法很有趣，并且很容易用 PyTorch 实现，但肯定有一些以前的工作解决了类似的问题。例如，Mikael Henaff、Junbo Zhao 和 Yann LeCun 的论文“<a class="ae lj" href="https://arxiv.org/abs/1711.04994" rel="noopener ugc nofollow" target="_blank">使用误差编码网络</a> (EEN)在不确定性下的预测”使用从预测误差到潜在空间的反馈机制来获得更好的预测结果。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ml"><img src="../Images/bb61654ebfdaa71e1cea92855f4bb023.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBcjZ063xlxO9Ylwy8Cewg.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">ENN Model Architecture</figcaption></figure><p id="5679" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">视频背景去除是图像处理领域的一个重要研究课题，对此我们并不感到奇怪。我要感谢来自<a class="ae lj" href="https://grail.com" rel="noopener ugc nofollow" target="_blank"> Grail </a>的<a class="ae lj" href="https://www.linkedin.com/in/earl-hubbell-28a5634/" rel="noopener ugc nofollow" target="_blank"> Earl Hubbell </a>，当我在 2018 年末的一次当地生物信息学会议上介绍这项工作时，他向我指出了用于视频背景去除的<a class="ae lj" href="https://sites.google.com/site/backgroundsubtraction/recent-background-modeling/background-modeling-via-rpca" rel="noopener ugc nofollow" target="_blank">鲁棒 PCA 方法</a>。</p><h1 id="9320" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">利用 DNAnexus 云平台构建深度学习模型</h1><p id="efab" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我在这里的一部分练习也是为了作为 DNAnexus 平台的新手练习“<a class="ae lj" href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food" rel="noopener ugc nofollow" target="_blank">吃自己的狗粮</a>”。下面是一个原型的截图，我和我的同事在 DNAnexus 平台上开发了一个支持云的 Jupyter 实验室工作站的集成解决方案。通过这样的集成，我们可以无缝集成数据管理、模型构建和评估。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/19ed4f2adae8fce033cbbac1f4ba6d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*KBpFK2QeNRdEdoVoBWGQhg.png"/></div></figure><p id="64bd" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们学到了许多关于在 GPU 实例上使用 Jupyter Lab 和 Docker 后端的利弊的经验，并希望我们学到的经验可以帮助我们尽快更好地改进 DNAnexus 产品。</p><h1 id="7003" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">承认</h1><p id="73c2" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我要感谢小然·辛和应谷与我们分享了他们的研究成果来验证这个想法。我也要感谢安德鲁·卡罗尔把我们连接到英姑的实验室。当然，对于我在 DNAnexus 的同事们帮助我快速入门，以便我可以开始利用这个平台进行有趣的 ML/AI 工作，我永远不会足够感谢。</p></div></div>    
</body>
</html>