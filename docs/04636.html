<html>
<head>
<title>Semantic Segmentation using Deep Separable Residual Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度可分离残差神经网络的语义分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-segmentation-using-deep-separable-residual-neural-networks-ab96184f291f?source=collection_archive---------24-----------------------#2019-07-15">https://towardsdatascience.com/semantic-segmentation-using-deep-separable-residual-neural-networks-ab96184f291f?source=collection_archive---------24-----------------------#2019-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/7e67a5550c462f6d0a8352dbc405afa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*4NdKKltWnXTTQygqsDaSuQ.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Semantic segmentation using Google deep-lab v3</figcaption></figure><p id="56df" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇博客中，我们将看到如何使用深度可分离残差神经网络来执行语义分割。使用可分离的卷积神经网络代替传统的 2D 卷积也被阐明。</p><h1 id="1243" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">数据集描述</strong></h1><p id="3af2" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">对于图像分割任务，我们将使用由<a class="ae lz" href="https://github.com/divamgupta" rel="noopener ugc nofollow" target="_blank"> Divam Gupta </a>准备的示例数据。分割数据可从- <a class="ae lz" href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view" rel="noopener ugc nofollow" target="_blank">驱动器</a>获得。总共有 12 个分割类和总共 367 个图像及其相应的注释。</p><h1 id="1d32" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">数据的可视化</h1><p id="279d" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">使用下面用 python 3 编写的代码块可以很容易地将数据可视化。</p><p id="1ed4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面提到的代码代表下载数据的目录。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="6f0b" class="mj kx iq mf b gy mk ml l mm mn">dir_data = “dataset1/”<br/>dir_seg = dir_data + “/annotations_prepped_train/”<br/>dir_img = dir_data + “/images_prepped_train/”</span></pre><p id="c9f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可视化部分如下。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="4bf2" class="mj kx iq mf b gy mk ml l mm mn">import cv2, os<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="2972" class="mj kx iq mf b gy mo ml l mm mn">## seaborn has white grid by default so I will get rid of this.<br/>sns.set_style(“whitegrid”, {‘axes.grid’ : False})</span><span id="cb94" class="mj kx iq mf b gy mo ml l mm mn">ldseg = np.array(os.listdir(dir_seg))<br/>## pick the first image file<br/>fnm = ldseg[300]<br/>print(fnm)</span><span id="cc82" class="mj kx iq mf b gy mo ml l mm mn">## read in the original image and segmentation labels<br/>seg = cv2.imread(dir_seg + fnm ) # (360, 480, 3)<br/>img_is = cv2.imread(dir_img + fnm )<br/>print(“seg.shape={}, img_is.shape={}”.format(seg.shape,img_is.shape))</span><span id="0825" class="mj kx iq mf b gy mo ml l mm mn">## Check the number of labels<br/>mi, ma = np.min(seg), np.max(seg)<br/>n_classes = ma — mi + 1<br/>print(“minimum seg = {}, maximum seg = {}, Total number of segmentation classes = {}”.format(mi,ma, n_classes))</span><span id="2f21" class="mj kx iq mf b gy mo ml l mm mn">fig = plt.figure(figsize=(5,5))<br/>ax = fig.add_subplot(1,1,1)<br/>ax.imshow(img_is)<br/>ax.set_title(“original image”)<br/>plt.show()</span><span id="5b0f" class="mj kx iq mf b gy mo ml l mm mn">fig = plt.figure(figsize=(15,10))<br/>for k in range(mi,ma+1):<br/> ax = fig.add_subplot(3,n_classes/3,k+1)<br/> ax.imshow((seg == k)*1.0)<br/> ax.set_title(“label = {}”.format(k))</span><span id="1e9b" class="mj kx iq mf b gy mo ml l mm mn">plt.show()</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/f4d0d609a5852c201c9f0f36cbec3551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bgP5y9XOS2Td6zp2Cp81uQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The visualization output consisting of 12 classes of annotations for a single image.</figcaption></figure><h1 id="998c" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">数据预处理</h1><p id="4860" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">数据预处理步骤仅包括将图像的尺寸调整到(224，224)的形状。这是用于 ResNet -50 网络的常规形状，因此也用于我们的可分离残差神经网络。</p><p id="66a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">调整大小的图像的可视化。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="00b7" class="mj kx iq mf b gy mk ml l mm mn">import random</span><span id="d2d7" class="mj kx iq mf b gy mo ml l mm mn">def give_color_to_seg_img(seg,n_classes):<br/> ‘’’<br/> seg : (input_width,input_height,3)<br/> ‘’’<br/> <br/> if len(seg.shape)==3:<br/> seg = seg[:,:,0]<br/> seg_img = np.zeros( (seg.shape[0],seg.shape[1],3) ).astype(‘float’)<br/> colors = sns.color_palette(“hls”, n_classes)<br/> <br/> for c in range(n_classes):<br/> segc = (seg == c)<br/> seg_img[:,:,0] += (segc*( colors[c][0] ))<br/> seg_img[:,:,1] += (segc*( colors[c][1] ))<br/> seg_img[:,:,2] += (segc*( colors[c][2] ))</span><span id="8774" class="mj kx iq mf b gy mo ml l mm mn">return(seg_img)</span><span id="9c02" class="mj kx iq mf b gy mo ml l mm mn">input_height , input_width = 224 , 224<br/>output_height , output_width = 224 , 224</span><span id="d8b1" class="mj kx iq mf b gy mo ml l mm mn">ldseg = np.array(os.listdir(dir_seg))<br/>for fnm in ldseg[np.random.choice(len(ldseg),3,replace=False)]:<br/> fnm = fnm.split(“.”)[0]<br/> seg = cv2.imread(dir_seg + fnm + “.png”) # (360, 480, 3)<br/> img_is = cv2.imread(dir_img + fnm + “.png”)<br/> seg_img = give_color_to_seg_img(seg,n_classes)</span><span id="1abc" class="mj kx iq mf b gy mo ml l mm mn">fig = plt.figure(figsize=(20,40))<br/> ax = fig.add_subplot(1,4,1)<br/> ax.imshow(seg_img)<br/> <br/> ax = fig.add_subplot(1,4,2)<br/> ax.imshow(img_is/255.0)<br/> ax.set_title(“original image {}”.format(img_is.shape[:2]))<br/> <br/> ax = fig.add_subplot(1,4,3)<br/> ax.imshow(cv2.resize(seg_img,(input_height , input_width)))<br/> <br/> ax = fig.add_subplot(1,4,4)<br/> ax.imshow(cv2.resize(img_is,(output_height , output_width))/255.0)<br/> ax.set_title(“resized to {}”.format((output_height , output_width)))<br/> plt.show()</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mu"><img src="../Images/b9018e971d581c12ec81bf8b174de1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Udg0yn1vApDmD2j1aNFJmQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The resized images.</figcaption></figure><p id="2141" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从上面的图片中我们可以看到，调整图片的大小确实改变了图片的长宽比，但是对文件的注释图片没有太大的影响。</p><p id="0e6f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，让所有的图像调整到(224，224)的大小。下面的代码可以用来做这件事。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="ce36" class="mj kx iq mf b gy mk ml l mm mn">def getImageArr( path , width , height ):<br/> img = cv2.imread(path, 1)<br/> img = np.float32(cv2.resize(img, ( width , height ))) / 127.5–1<br/> img1 = cv2.cvtColor(img,cv2.COLOR_BGR2LAB)<br/> img = cv2.merge((img,img1))<br/> #print(img.shape)<br/> return img</span><span id="c737" class="mj kx iq mf b gy mo ml l mm mn">def getSegmentationArr( path , nClasses , width , height ):</span><span id="b549" class="mj kx iq mf b gy mo ml l mm mn">seg_labels = np.zeros(( height , width , nClasses ))<br/> img = cv2.imread(path, 1)<br/> img = cv2.resize(img, ( width , height ))<br/> img = img[:, : , 0]</span><span id="a744" class="mj kx iq mf b gy mo ml l mm mn">for c in range(nClasses):<br/> seg_labels[: , : , c ] = (img == c ).astype(int)<br/> ##seg_labels = np.reshape(seg_labels, ( width*height,nClasses ))<br/> return seg_labels</span><span id="b28d" class="mj kx iq mf b gy mo ml l mm mn">images = os.listdir(dir_img)<br/>images.sort()<br/>segmentations = os.listdir(dir_seg)<br/>segmentations.sort()<br/> <br/>X = []<br/>Y = []<br/>for im , seg in zip(images,segmentations) :<br/> X.append( getImageArr(dir_img + im , input_width , input_height ) )<br/> Y.append( getSegmentationArr( dir_seg + seg , n_classes , output_width , output_height ) )</span><span id="7670" class="mj kx iq mf b gy mo ml l mm mn">X, Y = np.array(X) , np.array(Y)<br/>print(X.shape,Y.shape)</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mv"><img src="../Images/142b6bd3374e102e4bf23d131af9da38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0zGZAmUjsg0yU3KDn68jg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The data and the label shape.</figcaption></figure><h1 id="b6ab" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">模型开发</h1><p id="f6c0" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">该模型是一个深度可分离的残差神经网络。用可分离卷积范式代替传统的 2D 卷积运算，降低了模型的参数复杂度。</p><p id="a3c4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">传统的卷积方法使用基于梯度的学习，因此消失梯度和精度下降是卷积范例的两个主要问题，卷积范例指出，随着参数复杂性的增加，由于模型损失的增加，模型的精度降低。</p><h2 id="10b4" class="mj kx iq bd ky mw mx dn lc my mz dp lg kj na nb lk kn nc nd lo kr ne nf ls ng bi translated">残余建筑</h2><p id="9a6c" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">残差架构类似于 ResNet-50 模型，其避免了基于梯度的学习，但是使得残差学习范式能够通过模型获得最佳结果。所提出的残差模型具有 4 个残差块，其中每个残差块依次具有 3 个可分离卷积层以及与单个可分离卷积方法的快捷连接。</p><p id="8adb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">剩余网络定义如下。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="5765" class="mj kx iq mf b gy mk ml l mm mn">def residual_block(mod_, f_in, f_out, strides_ = (1,1), use_shortcut_ = False): <br/> shortcut_ = mod_<br/> <br/> k_ = (3,3)<br/> <br/> mod_ = SeparableConv2D(f_in, kernel_size=k_, strides=(1,1), padding = “same”)(mod_)<br/> mod_ = BatchNormalization()(mod_)<br/> mod_ = ELU()(mod_)<br/> <br/> mod_ = SeparableConv2D(f_in, kernel_size=k_, strides=strides_, padding = “same”)(mod_)<br/> mod_ = BatchNormalization()(mod_)<br/> mod_ = ELU()(mod_)<br/> <br/> mod_ = SeparableConv2D(f_out, kernel_size=k_, strides=(1,1), padding = “same”)(mod_)<br/> mod_ = BatchNormalization()(mod_)<br/> mod_ = ELU()(mod_)<br/> <br/> if use_shortcut_ == True or strides_ != (1,1):<br/> shortcut_ = SeparableConv2D(f_out, kernel_size=k_, strides=strides_, padding = “same”)(shortcut_)<br/> shortcut_ = BatchNormalization()(shortcut_)<br/> <br/> mod_ = Add()([shortcut_, mod_])<br/> mod_ = ReLU()(mod_)<br/> <br/> return mod_</span></pre><p id="3f33" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">剩余网络可以图示如下。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nh"><img src="../Images/f41827144a737db54e215b900b7a3f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W-dpYPDk7WEvmtreWBd5ZA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Residual block of our proposed network.</figcaption></figure><h2 id="74ca" class="mj kx iq bd ky mw mx dn lc my mz dp lg kj na nb lk kn nc nd lo kr ne nf ls ng bi translated">上采样反卷积层</h2><p id="6a35" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">上采样层将低分辨率图像提高到高分辨率。有各种各样的上采样方法。所提出的方法将转置卷积层用于上采样，这可以被最好地描述如下。该方法简单地反转卷积的向前和向后传递，并在 Keras 的<a class="ae lz" href="https://keras.io/layers/convolutional/" rel="noopener ugc nofollow" target="_blank"> Conv2DTranspose </a>中实现。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="191d" class="mj kx iq mf b gy mk ml l mm mn">def model_build(in_):<br/> <br/> IMAGE_ORDERING = “channels_last”<br/> <br/> k_=(3,3)<br/> <br/> mod_ = Conv2D(16, kernel_size=k_, strides = (1,1), padding = “same”)(in_)<br/> <br/> mod_ = BatchNormalization()(mod_)<br/> <br/> mod_ = ReLU()(mod_)<br/> <br/> mod_ = MaxPooling2D()(mod_)<br/> <br/> mod_ = residual_block(mod_, 16, 32, use_shortcut_=True)<br/> <br/> mod_ = MaxPooling2D()(mod_)<br/> <br/> mod_ = residual_block(mod_, 32, 64, use_shortcut_=True)<br/> <br/> mod_ = MaxPooling2D()(mod_)<br/> <br/> pool3 = mod_<br/> <br/> mod_ = residual_block(mod_, 64, 96, use_shortcut_=True)<br/> <br/> mod_ = MaxPooling2D()(mod_)<br/> <br/> pool4 = mod_<br/> <br/> mod_ = residual_block(mod_, 96, 128, use_shortcut_=True)<br/> <br/> mod_ = MaxPooling2D()(mod_)<br/> <br/> pool5 = mod_<br/> <br/> n = 2048<br/> nClasses = 12<br/> o = ( Conv2D( n , ( 7 , 7 ) , activation=’relu’ , padding=’same’, name=”conv6", data_format=IMAGE_ORDERING))(pool5)<br/> conv7 = ( Conv2D( n , ( 1, 1 ) , activation=’relu’ , padding=’same’, name=”conv7", data_format=IMAGE_ORDERING))(o)<br/> <br/> <br/> ## 4 times upsamping for pool4 layer<br/> conv7_4 = Conv2DTranspose( nClasses , kernel_size=(4,4) , strides=(4,4) , use_bias=False, data_format=IMAGE_ORDERING )(conv7)<br/> ## (None, 224, 224, 10)<br/> ## 2 times upsampling for pool411<br/> pool411 = ( Conv2D( nClasses , ( 1 , 1 ) , activation=’relu’ , padding=’same’, name=”pool4_11", data_format=IMAGE_ORDERING))(pool4)<br/> pool411_2 = (Conv2DTranspose( nClasses , kernel_size=(2,2) , strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING ))(pool411)<br/> <br/> pool311 = ( Conv2D( nClasses , ( 1 , 1) , activation=’relu’ , padding=’same’, name=”pool3_11", data_format=IMAGE_ORDERING))(pool3)<br/> <br/> o = Add(name=”add”)([pool411_2, pool311, conv7_4 ])<br/> o = Conv2DTranspose( nClasses , kernel_size=(8,8) , strides=(8,8) , use_bias=False, data_format=IMAGE_ORDERING )(o)<br/> o = (Activation(‘softmax’))(o)<br/> <br/> return o</span><span id="0084" class="mj kx iq mf b gy mo ml l mm mn">## The model input and summary##</span><span id="5399" class="mj kx iq mf b gy mo ml l mm mn">in_ = Input((224,224,3))<br/>model_f = model_build(in_)<br/>model = Model(input = in_, output = model_f)<br/>model.compile(optimizer = RMSprop(), loss = "categorical_crossentropy", metrics=["accuracy"])</span><span id="2262" class="mj kx iq mf b gy mo ml l mm mn">model.summary()</span></pre><p id="7210" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过执行上面的代码，可以绘制和可视化模型摘要。RMSprop 已经被用作具有分类交叉熵损失的优化器。</p><h2 id="d8f0" class="mj kx iq bd ky mw mx dn lc my mz dp lg kj na nb lk kn nc nd lo kr ne nf ls ng bi translated">数据集的训练和测试分割。</h2><p id="1e84" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">关于训练和测试数据，数据集已经以 85:15 的比例分割。训练和测试数据没有任何重叠图像。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="8a87" class="mj kx iq mf b gy mk ml l mm mn">from sklearn.utils import shuffle<br/>train_rate = 0.85<br/>index_train = np.random.choice(X.shape[0],int(X.shape[0]*train_rate),replace=False)<br/>index_test = list(set(range(X.shape[0])) — set(index_train))<br/> <br/>X, Y = shuffle(X,Y)<br/>X_train, y_train = X[index_train],Y[index_train]<br/>X_test, y_test = X[index_test],Y[index_test]<br/>print(X_train.shape, y_train.shape)<br/>print(X_test.shape, y_test.shape)</span></pre><h2 id="c5d6" class="mj kx iq bd ky mw mx dn lc my mz dp lg kj na nb lk kn nc nd lo kr ne nf ls ng bi translated">模特培训</h2><p id="8f8e" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">训练参数如下。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="eb96" class="mj kx iq mf b gy mk ml l mm mn">nb_epochs = 180<br/>nb_batch = 32</span><span id="3aa2" class="mj kx iq mf b gy mo ml l mm mn">earlyStopping=EarlyStopping(monitor=’val_loss’, patience=10, verbose=0, mode=’auto’)</span><span id="7133" class="mj kx iq mf b gy mo ml l mm mn">lr_reduce = ReduceLROnPlateau(monitor=’val_acc’, factor=0.01, epsilon=0.0001, patience=2, verbose=1)</span><span id="dda4" class="mj kx iq mf b gy mo ml l mm mn">save_path=”weights/sep_kernel3_res4_85_lab.h5"</span><span id="50fe" class="mj kx iq mf b gy mo ml l mm mn">checkpoint = ModelCheckpoint(save_path, monitor=’val_acc’, verbose=1, save_best_only=True, mode=’max’)</span></pre><p id="fbdf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">模特培训</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="169e" class="mj kx iq mf b gy mk ml l mm mn">hist1=model.fit(X_train, y_train, epochs = nb_epochs, batch_size = nb_batch, callbacks=[checkpoint,earlyStopping,lr_reduce], validation_data=(X_test, y_test), verbose = 1)</span></pre><h2 id="3184" class="mj kx iq bd ky mw mx dn lc my mz dp lg kj na nb lk kn nc nd lo kr ne nf ls ng bi translated">模型评估</h2><p id="47f3" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">绘制培训损失曲线</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="c5e8" class="mj kx iq mf b gy mk ml l mm mn">for key in [‘loss’, ‘val_loss’]:<br/> plt.plot(hist1.history[key],label=key)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/62debf55c14fdb6ed74e393ce92c8efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*WcpDAHwrKVQ0NNTNCjtO5A.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Training Loss</figcaption></figure><p id="560a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">并集或 IOU 的交集计算如下:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="4b1b" class="mj kx iq mf b gy mk ml l mm mn">def IoU(Yi,y_predi):<br/> ## mean Intersection over Union<br/> ## Mean IoU = TP/(FN + TP + FP)</span><span id="292d" class="mj kx iq mf b gy mo ml l mm mn">IoUs = []<br/> Nclass = int(np.max(Yi)) + 1<br/> for c in range(Nclass):<br/> TP = np.sum( (Yi == c)&amp;(y_predi==c) )<br/> FP = np.sum( (Yi != c)&amp;(y_predi==c) )<br/> FN = np.sum( (Yi == c)&amp;(y_predi != c)) <br/> IoU = TP/float(TP + FP + FN)<br/> print(“class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}”.format(c,TP,FP,FN,IoU))<br/> IoUs.append(IoU)<br/> mIoU = np.mean(IoUs)<br/> print(“_________________”)<br/> print(“Mean IoU: {:4.3f}”.format(mIoU))<br/> <br/>IoU(y_testi,y_predi)</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9fb85e7cba7621232094799bb5700848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*TpjQ2LGwwh6LOyPjlp6ZOw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The classwise IOU and the mean IOU</figcaption></figure><h1 id="0536" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">模型性能的可视化</h1><p id="035a" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">该模型的预测能力通过下面这段代码来可视化。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="a6dd" class="mj kx iq mf b gy mk ml l mm mn">shape = (224,224)<br/>n_classes= 10</span><span id="617c" class="mj kx iq mf b gy mo ml l mm mn">for i in range(10):<br/> img_is = (X_test[i] + 1)*(255.0/2)<br/> seg = y_predi[i]<br/> segtest = y_testi[i]</span><span id="1494" class="mj kx iq mf b gy mo ml l mm mn">fig = plt.figure(figsize=(10,30)) <br/> ax = fig.add_subplot(1,3,1)<br/> ax.imshow(img_is/255.0)<br/> ax.set_title(“original”)<br/> <br/> ax = fig.add_subplot(1,3,2)<br/> ax.imshow(give_color_to_seg_img(seg,n_classes))<br/> ax.set_title(“predicted class”)<br/> <br/> ax = fig.add_subplot(1,3,3)<br/> ax.imshow(give_color_to_seg_img(segtest,n_classes))<br/> ax.set_title(“true class”)<br/> plt.show()</span></pre><p id="317e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下代码的输出如下。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/78d06c9d74ab4d6f91e43146fe6aae5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*gdy5a4c_gFkqN98ih5MVQw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The prediction ability of the model is fine but can be optimized.</figcaption></figure></div></div>    
</body>
</html>