<html>
<head>
<title>Mining and Classifying Medical Documents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">医学文档的挖掘和分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mining-and-classifying-medical-text-documents-1876462f73bc?source=collection_archive---------8-----------------------#2019-10-25">https://towardsdatascience.com/mining-and-classifying-medical-text-documents-1876462f73bc?source=collection_archive---------8-----------------------#2019-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8597" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/clinical-data-science" rel="noopener" target="_blank">临床数据科学</a></h2><div class=""/><div class=""><h2 id="c1c1" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用 Scikit-Learn 和 Streamlit 为自然语言处理开发和部署机器学习应用程序。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d5142f4293fde8ac92e4178bdf2d4b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LcKuRcFRYsxhtMfE"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@anniespratt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Annie Spratt</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="295e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="9870" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在医疗领域，会产生来自多个专业的大量(数字)文本文档，无论是患者健康记录、信件还是临床研究文档。事实上，文本数据，通常是<a class="ae lh" href="https://en.wikipedia.org/wiki/Unstructured_data" rel="noopener ugc nofollow" target="_blank">非结构化</a>，促成了全球数据量的巨大增长——仅社交媒体一项。因此，从文本中检索信息成为一项越来越重要的任务。</p><p id="9c85" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这篇文章的目的有两个:</p><ol class=""><li id="6a93" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">演示方法为<a class="ae lh" href="https://en.wikipedia.org/wiki/Text_mining" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">文本挖掘</strong> </a>和<strong class="mc jd">文档分类</strong>用于<a class="ae lh" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">自然语言处理</strong> </a></li><li id="13dc" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">展示使用<a class="ae lh" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>和<a class="ae lh" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>在 Python 中自动生成文本分类器的应用的<strong class="mc jd">开发</strong>和<strong class="mc jd">部署</strong>。</li></ol><p id="17df" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最终产品看起来像这个应用程序，名称为<a class="ae lh" href="http://ml-ml.herokuapp.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">医学语言模型学习者(MLML) </strong> </a>。(原始数据可在<a class="ae lh" href="https://www.kaggle.com/tboyle10/medicaltranscriptions" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> kaggle </strong> </a>上获得)。)</p><blockquote class="np"><p id="523c" class="nq nr it bd ns nt nu nv nw nx ny mv dk translated"><strong class="ak">自然语言处理</strong> ( <strong class="ak"> NLP </strong>)是<a class="ae lh" href="https://en.wikipedia.org/wiki/Linguistics" rel="noopener ugc nofollow" target="_blank">语言学</a>、<a class="ae lh" href="https://en.wikipedia.org/wiki/Computer_science" rel="noopener ugc nofollow" target="_blank">计算机科学</a>、<a class="ae lh" href="https://en.wikipedia.org/wiki/Information_engineering_(field)" rel="noopener ugc nofollow" target="_blank">信息工程</a>、<a class="ae lh" href="https://en.wikipedia.org/wiki/Artificial_intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>的一个子领域，涉及计算机与人类(自然)语言之间的交互，特别是如何编程计算机处理和分析大量的<a class="ae lh" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank">自然语言</a>数据。</p></blockquote></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/14ad5c20c559a5e532c47a6421c63342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gt8Q0ZLDT-I1ZPeUAM9X0Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Possible look of a Streamlit application. (image by author)</figcaption></figure><h1 id="180c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">语言建模</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Andrew Ng explaining embeddings, sequence models, recurrent neural networks.</figcaption></figure><h2 id="f84e" class="oj lj it bd lk ok ol dn lo om on dp ls mj oo op lu mn oq or lw mr os ot ly iz bi translated">嵌入</h2><p id="91ba" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">一个重要的考虑是，文本由<a class="ae lh" href="https://en.wikipedia.org/wiki/String_(computer_science)" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">字符串</strong> </a>组成，但是数学模型依赖于数字，有不同的方式将文字转换成数字向量。另外，根据问题的定义，对语言建模有不同的可能性。一个常见的任务是将来自某个领域的序列(例如某种语言的文本/语音)映射到另一个领域的序列(例如另一种语言的文本/语音)，称为<a class="ae lh" href="https://en.wikipedia.org/wiki/Seq2seq" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">序列到序列模型</strong> </a>。例如，为了从文本(<a class="ae lh" href="http://An important consideration is the fact that text consists of strings, but mathematical models rely on numbers." rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">文本到语音</strong> </a>)生成语音，使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">递归神经网络</strong> </a>以及预训练的单词<a class="ae lh" href="https://en.wikipedia.org/wiki/Embedding" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd"/></a>(例如用于单词表示的<a class="ae lh" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">全局向量</strong> </a>、<em class="ou">手套</em>)。在那种情况下，一个单词就变成了一个<em class="ou"> n </em>维空间中的向量，类似于具有<em class="ou"> n </em>个主成分的空间中的数据点。</p><p id="0f16" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">不同的维度(也称为<a class="ae lh" href="https://en.wikipedia.org/wiki/Latent_variable" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">、潜在变量</strong> </a>)可以被认为是描述单词的属性/特性，例如面包、水果和苹果等单词的属性“可食用”；像椅子、汽车和动物这样的词会出现在相反的方向。因此，在这个维度上，“苹果”和“椅子”会相距更远，但是“苹果”和“水果”会更近。至于“动物”这个词，这应该是中间的某个地方，因为有些动物是被吃掉的。请注意，通常不可能提取不同潜在变量的含义。</p><p id="acd8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">由于这些嵌入，与 1 选 1 的<em class="ou"> K </em>编码方法相比，维数降低了。例如，具有<em class="ou"> K </em>个单词的字典在 1-out-of-t 10】K 空间中具有<em class="ou"> K </em>个维度(每个单词都是与所有其他单词正交的向量)，但是嵌入可以将维度降低到 100，从而降低计算复杂度。(如果<em class="ou"> K </em> =10.000 且<em class="ou"> n </em> =100，则压缩系数为 100。)</p><p id="bdf3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">预训练的嵌入可以下载，不必先构建。然而，每一个可能的单词都必须已经在嵌入中可用——或者嵌入必须首先被训练。嵌入是与上下文无关的，即单词“bank”在类似于“bank account”或“he sat on a bank”的表达中被视为相似。</p><h2 id="4a09" class="oj lj it bd lk ok ol dn lo om on dp ls mj oo op lu mn oq or lw mr os ot ly iz bi translated">词汇袋</h2><p id="04f5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">或者，可以忽略语言的顺序方面，将文本建模为单词的集合(<a class="ae lh" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">单词袋</strong> </a>方法)。这是文本挖掘中最常见的表现形式，是从<a class="ae lh" href="https://en.wikipedia.org/wiki/Plain_text" rel="noopener ugc nofollow" target="_blank">文本</a>中获取高质量信息的过程。为了以这种方式描述文本，通过使用<strong class="mc jd">最频繁出现的单词</strong>或<a class="ae lh" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd">n-grams</strong></a><strong class="mc jd"/>(根据文本中的单词总数进行原始计数或归一化)来构建字典，但是这样的字典可能不太具有区分性，因为每个文本都包含像“a”、“the”、“is”等表达。</p><p id="f41e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了解决这个问题，开发了一种叫做<a class="ae lh" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">词频-逆文档频率</strong> </a> <strong class="mc jd"> </strong> ( <strong class="mc jd"> tf-idf </strong>或<strong class="mc jd"> TFIDF </strong>)的解决方案；tf-idf 是一个<strong class="mc jd">数字统计量</strong>，旨在反映一个单词对文档集合中的一个文档有多重要。tf-idf 值与单词在文档中出现的次数成比例地增加，并根据语料库中包含该单词的文档数量进行修改，这有助于调整某些单词通常更频繁出现的事实。tf-idf 是当今最流行的术语加权方案之一；<a class="ae lh" href="https://link.springer.com/article/10.1007%2Fs00799-015-0156-0" rel="noopener ugc nofollow" target="_blank">数字图书馆中 83%的基于文本的推荐系统使用 TF–IDF</a>。</p><h2 id="e2a5" class="oj lj it bd lk ok ol dn lo om on dp ls mj oo op lu mn oq or lw mr os ot ly iz bi translated">术语频率-逆文档频率</h2><p id="d63e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><strong class="mc jd">词频</strong>(针对每个文档进行计算)通常是特定单词<em class="ou"> i </em>(例如“子宫颈”)的计数除以文本<em class="ou"> L </em>中的总单词量，从而调整文档长度。</p><p id="1e87" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">逆文档频率</strong>通过计算包含单词<em class="ou"> i </em>的文档的逆分数(<em class="ou"> N/ni </em>)并应用对数变换来获得。出现在几乎每个文档中的单词在对数变换之前将具有接近 1 的值，因此在对数变换之后接近 0。</p><p id="87f0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">tf-idf 是为一组<em class="ou"> N </em>文档中的每个文档<em class="ou"> k </em>中的每个单词<em class="ou"> i </em>定义的。它由 tf 和 idf 相乘计算得出。这样，几乎每个文档<em class="ou"> k </em>中都出现的一个字<em class="ou"> i </em>降低了 tf-idf 值。对于文本挖掘，在字典中保留具有低 tf-idf 值的单词是没有意义的，因为它们对于特定的文档类没有区别。</p><p id="1647" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">想象一下，一个数据科学家想要建立一个区分生物和法律文档的模型；他应该关注哪些单词？像“是”或“a”这样无处不在的词一点帮助都没有，但是像“蛋白质”或“合同”这样的术语有帮助——这些是有趣的。如果模型应该进一步区分生物文档的子类(更高的分辨率)，它需要细化字典。例如，“DNA”、“酶”或“途径”等词在微生物学中比在生态学中使用得更频繁，而“生物多样性”、“栖息地”或“种群”等词则相反。如果期望所述更高的分辨率，可能需要更大的字典。或者，人们必须更有选择性地将最大和最小 tf-idf 值包括在尺寸为 d 的字典中。</p><p id="42dc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">tf-idf 计算的结果是一个维数为 N <em class="ou"> x </em> D 的矩阵，即文档数乘以字典大小，其中填充了用作特征的 tf-idf 值<strong class="mc jd">T3。注意，这很可能是一个<a class="ae lh" href="https://en.wikipedia.org/wiki/Sparse_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">稀疏矩阵</strong> </a>。此外，在矩阵构造过程中存在可调参数，并且显然 tf-idf 变换对于模型性能具有一些影响。</strong></p><p id="778e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">sci kit-作为文本文档的内置<strong class="mc jd">转换器</strong>学习。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="1704" class="oj lj it ow b gy pa pb l pc pd"><strong class="ow jd">import</strong> pandas <strong class="ow jd">as</strong> pd<strong class="ow jd"><br/>from</strong> sklearn.feature_extraction.text <strong class="ow jd">import</strong> TfidfVectorizer<br/><strong class="ow jd">from</strong> sklearn.preprocessing <strong class="ow jd">import</strong> LabelEncoder<br/>enc = LabelEncoder()</span><span id="a39a" class="oj lj it ow b gy pe pb l pc pd"># Load data.<br/>data = pd.read_csv("mtsamples.csv", index_col=0, usecols=[0,1,2,4])<br/>data = data.dropna()<br/>samples = data.transcription<br/>text_labels  = [label_name.lower() for label_name in data.medical_specialty]<br/>labels = enc.fit_transform(np.array(text_labels))</span><span id="a6b5" class="oj lj it ow b gy pe pb l pc pd"># Transform data.<br/>max_df = 0.2<br/>min_df = 0.001<br/>max_features = 1000<br/>ngram_range = (1,1)</span><span id="5b88" class="oj lj it ow b gy pe pb l pc pd">tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english', ngram_range=ngram_range)</span><span id="04ac" class="oj lj it ow b gy pe pb l pc pd">tfidf = tfidf_vectorizer.fit_transform(samples)<br/>feature_names = tfidf_vectorizer.get_feature_names()</span></pre><p id="ec57" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">由于有几个自由参数，Streamlit 可用于构建滑块来更改值，并手动尝试不同的组合；这样，可以快速评估许多不同的选项。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="166c" class="oj lj it ow b gy pa pb l pc pd"><strong class="ow jd">import</strong> streamlit <strong class="ow jd">as</strong> st<br/><strong class="ow jd">from</strong> streamlit.logger <strong class="ow jd">import</strong> get_logger</span><span id="43a5" class="oj lj it ow b gy pe pb l pc pd"># Title.<br/>st.sidebar.header("Constructing dictonary of words.")</span><span id="11ed" class="oj lj it ow b gy pe pb l pc pd"># Upper bound for tf-idf value.<br/>max_df = st.sidebar.slider("Maximum tf-idf value for a word to be considered.", min_value=0.05, max_value=0.4, value=0.3, step=0.01)</span><span id="4ff3" class="oj lj it ow b gy pe pb l pc pd"># Lower bound for tf-idf value.<br/>min_df = st.sidebar.slider("Minimum tf-idf value for a word to be considered.", min_value=0.00, max_value=0.05, value=0.01, step=0.01)</span><span id="eb34" class="oj lj it ow b gy pe pb l pc pd"># Size of dictionary.<br/>max_features = st.sidebar.slider("Size of dictionary.", min_value=100, max_value=1000, value=500, step=100)</span></pre><p id="74b8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最终产品看起来应该是这样的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/e5525b63fc1966207cd0b04892cbfad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*wnXl34WJfokCPI4Mx75ugg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Checkboxes and sliders. (image by author)</figcaption></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="7307" class="li lj it bd lk ll pg ln lo lp ph lr ls ki pi kj lu kl pj km lw ko pk kp ly lz bi translated">降维</h1><p id="20bf" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">一个可能的应用是根据医学文档的医学专业对其进行分类。一旦构建了文档-单词矩阵，并且将文本转换成数学表示，就可以使用维数减少方法，例如<a class="ae lh" href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">截断奇异值分解</strong> </a>和图表库，例如<a class="ae lh" href="https://altair-viz.github.io/index.html" rel="noopener ugc nofollow" target="_blank"> Altair </a>来可视化它们。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="7aef" class="oj lj it ow b gy pa pb l pc pd"><strong class="ow jd">from</strong> sklearn.decomposition <strong class="ow jd">import</strong> TruncatedSVD<br/><strong class="ow jd">import</strong> altair <strong class="ow jd">as</strong> alt</span><span id="07f8" class="oj lj it ow b gy pe pb l pc pd"># Dimensionality reduction.<br/>dim_red = TruncatedSVD(n_components=2)<br/>data_red = dim_red.fit_transform(tfidf)</span><span id="8a8b" class="oj lj it ow b gy pe pb l pc pd"># Plot.<br/>scatter = alt.Chart(data_red,title="dimensionality reduction",height=400).mark_circle().encode(x='principal component 1', y='principal component 2', color=alt.Color('class', scale=alt.Scale(scheme='blues')),tooltip=["class"]).interactive()</span><span id="5574" class="oj lj it ow b gy pe pb l pc pd">st.altair_chart(scatter)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/a175cb847cd949870ad786edbdc6bdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gftdKWgHrCZag5K0YYltrA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Dimensionality reduction of medical documents. (image by author)</figcaption></figure><p id="e167" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">该图显示了两个主成分空间中的不同文档，解释了原始 tf-idf 空间中的最高方差。已经可以看出，这些类之间有很多重叠——至少对于构造的字典来说是这样——并且分类器的性能会很低。</p><p id="bbb1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">数据集包含 40 个类。然而，还不知道一个特定的类在数据集中出现的频率。例如，可以预期在倾斜的数据集中，低丰度类被错误分类的频率更高，因此丢弃或合并稀有类可能是合理的。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="ce98" class="li lj it bd lk ll pg ln lo lp ph lr ls ki pi kj lu kl pj km lw ko pk kp ly lz bi translated">模型结构</h1><p id="873d" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">为了建立文本模型，可以使用任何分类算法，例如<a class="ae lh" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">随机森林</strong></a>；与<a class="ae lh" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">逻辑回归</strong> </a>相比，随机森林的优势在于可以构造任意形状的决策边界，而无需基扩展。为了找到最佳的随机森林超参数，可以执行网格搜索。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="74e8" class="oj lj it ow b gy pa pb l pc pd"><strong class="ow jd">from</strong> sklearn.ensemble <strong class="ow jd">import</strong> RandomForestClassifier</span><span id="599b" class="oj lj it ow b gy pe pb l pc pd"># Number of trees.<br/>n_estimators = 1000</span><span id="3312" class="oj lj it ow b gy pe pb l pc pd"># Define classifier.<br/>forest_clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=None, max_leaf_nodes=None, class_weight='balanced', oob_score=True, n_jobs=-1, random_state=0)</span><span id="2d2c" class="oj lj it ow b gy pe pb l pc pd"># Define grid.<br/>parameters = {'max_leaf_nodes':np.linspace(20,35,14,dtype='int')}</span><span id="494e" class="oj lj it ow b gy pe pb l pc pd"># Balanced accuracy as performance measure.<br/>clf = RandomizedSearchCV(forest_clf, parameters, n_iter=10, cv=3,iid=False, scoring='accuracy',n_jobs=-1)</span><span id="a431" class="oj lj it ow b gy pe pb l pc pd"># Train/optimize classifier.<br/>classifier = clf.fit(tfidf, labels)</span><span id="ebd7" class="oj lj it ow b gy pe pb l pc pd"># Retrieve optimum.<br/>forest = classifier.best_estimator_<br/>feature_importances = forest.feature_importances_<br/>indices = np.argsort(feature_importances)[::-1]</span></pre><p id="e4c7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">手动调整这些超参数可能也很有趣。(在当前版本中，只有一个文本输入容器，所以必须将字符串转换为整数。)</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="36b6" class="oj lj it ow b gy pa pb l pc pd">st.sidebar.header("Customizing the model.")</span><span id="1d7e" class="oj lj it ow b gy pe pb l pc pd">n_estimators = st.sidebar.text_input('Number of trees in random forest.', '1000')</span><span id="78e9" class="oj lj it ow b gy pe pb l pc pd">max_leaf_nodes = st.sidebar.text_input('Maximum number of leaf nodes in a tree.', '25')</span><span id="1d42" class="oj lj it ow b gy pe pb l pc pd">max_depth = st.sidebar.text_input('Maximum depth of a tree.', '5')</span><span id="0f6f" class="oj lj it ow b gy pe pb l pc pd">class_weight = st.sidebar.selectbox("Class weights for the model.",('balanced','balanced_subsample'))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f76f96de273b454ef3d83adb9cc577cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*Skm4lIBjhDdNqb66qm6w-w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Input and selection fields. (image by author)</figcaption></figure><p id="ba29" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了解释该模型，从随机森林获得的特征重要性应该在用条形图训练之后进行评估。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="45e7" class="oj lj it ow b gy pa pb l pc pd"># Retrieve values.<br/>feature_importance = classifier.feature_importances_</span><span id="c301" class="oj lj it ow b gy pe pb l pc pd"># Plot.<br/>bars = alt.Chart(feature_importance, height=400, title="discriminative power of features").mark_bar(color='steelblue', opacity=0.7).encode(y='features:N', x='feature importance:Q', tooltip="feature importance")</span><span id="3655" class="oj lj it ow b gy pe pb l pc pd">st.altair_chart(bars)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/f78fe04464519c7c9c58a68e07725fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kXcGRBvAVltorMpw1h5cQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Feature importance of words to discriminate between documents. (image by author)</figcaption></figure><p id="e08e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">可以看出，大多数重要程度是相似的。由<strong class="mc jd">外袋得分</strong>确定的整体表现较低(0.41)。很可能在类别之间有太多的重叠，这由维度减少来支持。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="2716" class="li lj it bd lk ll pg ln lo lp ph lr ls ki pi kj lu kl pj km lw ko pk kp ly lz bi translated">模型评估</h1><p id="1e39" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">评估模型相对于<a class="ae lh" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd"/><strong class="mc jd"/></a><a class="ae lh" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd">F1 得分</strong></a><a class="ae lh" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd">混淆矩阵</strong> </a> <strong class="mc jd"> </strong>进行计算。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="f01c" class="oj lj it ow b gy pa pb l pc pd"><strong class="ow jd">from</strong> sklearn.metrics <strong class="ow jd">import</strong> f1_score, confusion_matrix</span><span id="fd1d" class="oj lj it ow b gy pe pb l pc pd"># Retrieve values.<br/>y_true = labels<br/>y_pred = classifier.predict(tfidf)</span><span id="cffa" class="oj lj it ow b gy pe pb l pc pd"># Compute scores.<br/>f1_score_ = f1_score(y_true,y_pred,average="weighted")<br/>cm = confusion_matrix(y_true,y_pred)</span><span id="6fb9" class="oj lj it ow b gy pe pb l pc pd"># Plot.<br/>heat = alt.Chart(source, height=500, title="confusion matrix").mark_rect(opacity=0.7).encode(x='predicted class:N', y='true class:N', color=alt.Color('count:Q', scale=alt.Scale(scheme='blues')), tooltip="count")<br/>st.altair_chart(heat)</span></pre><p id="7637" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">F1 分数为 0.49，因此精确度和/或召回率较低。从混淆矩阵中可以看出，该模型在区分医学文档方面存在困难。目前，文档之间使用的单词似乎过于相似，即许多相同的单词在不同的医学专业中使用，或者根本找不到任何模式。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/ada9c2a71c3b802583b146fe306ad295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frqGh0v1W70zlINDfchLiA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Confusion matrix for all classes. (image by author)</figcaption></figure><p id="4bb6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">显然，该模型现在可以用于预测。在这种情况下，必须提供一个样本作为输入，将其转换为 tf-idf 格式，并提供给分类器，然后分类器将返回该样本属于任何类别的概率预测。</p><h1 id="af5a" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">部署</h1><p id="51e5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">应用程序中的代码应该如下所示:</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="c4e3" class="oj lj it ow b gy pa pb l pc pd">LOGGER = get_logger(__name__)</span><span id="9f3e" class="oj lj it ow b gy pe pb l pc pd">def run():<br/>   # code for app</span><span id="9f06" class="oj lj it ow b gy pe pb l pc pd">if __name__ == "__main__":<br/>    run()</span></pre><p id="bb23" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了在本地测试它，终端被打开并被引导到带有“app.py”文件的位置。键入以下代码启动本地测试会话。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="7f3a" class="oj lj it ow b gy pa pb l pc pd">streamlit run app.py</span></pre><p id="5f7f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">要在云提供商如<a class="ae lh" href="http://heroku.com" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> Heroku </strong> </a>上部署应用程序，需要一个帐户和<a class="ae lh" href="https://git-scm.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> git </strong> </a> <strong class="mc jd"> </strong>。首先，创建一个文件名为“setup.sh”、内容如下的<strong class="mc jd"> shell 脚本</strong>。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="fe5b" class="oj lj it ow b gy pa pb l pc pd">mkdir -p ~/.streamlit/</span><span id="ca7b" class="oj lj it ow b gy pe pb l pc pd">echo "\<br/>[general]\n\<br/>email = \"email@website.com\"\n\<br/>" &gt; ~/.streamlit/credentials.toml</span><span id="ac7e" class="oj lj it ow b gy pe pb l pc pd">echo "\<br/>[server]\n\<br/>headless = true\n\<br/>enableCORS=false\n\<br/>port = $PORT\n\<br/>" &gt; ~/.streamlit/config.toml</span></pre><p id="ed7e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">接下来，创建一个<strong class="mc jd"> requirements.txt </strong>文件。它的内容应该是这样的。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="9a72" class="oj lj it ow b gy pa pb l pc pd">pandas<br/>sklearn<br/>numpy<br/>streamlit<br/>altair</span></pre><p id="202a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">对于 Heroku 来说，必须编写一个所谓的<strong class="mc jd"> Procfile </strong>，其中包含以下代码行——假设该应用程序名为“app.py”。请注意，Procfile 没有类型规范。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="d506" class="oj lj it ow b gy pa pb l pc pd">web: sh setup.sh &amp;&amp; streamlit run app.py</span></pre><p id="6a03" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">该应用程序可以部署在终端中(这里是在 Unix/Mac 操作系统上)。为此，shell 和所有文件都是定向的。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="e1b1" class="oj lj it ow b gy pa pb l pc pd">$ cd Documents/Projects/App</span></pre><p id="b614" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">接下来，必须在 Heroku 上创建一个应用程序(这里命名为“myapp”)。之后，使用“app.py”文件在目录中初始化 git 存储库。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="e0fc" class="oj lj it ow b gy pa pb l pc pd">$ git init<br/>$ heroku git:remote -a myapp</span></pre><p id="1b47" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">代码被提交到存储库并部署在 Heroku 上。</p><pre class="ks kt ku kv gt ov ow ox oy aw oz bi"><span id="066d" class="oj lj it ow b gy pa pb l pc pd">$ git add .<br/>$ git commit -am "make it better"<br/>$ git push heroku master</span></pre><p id="5f70" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最后，应用程序应该是在线的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/4a87637c379052ba8a83ad2a0276c9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5lzTkKcpLb1SjDRB"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@cdr6934?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Chris Ried</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>