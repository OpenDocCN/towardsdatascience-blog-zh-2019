<html>
<head>
<title>Untangling UK politics with Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用数据科学理清英国政治</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/untangling-uk-politics-with-data-science-a5afe9a86923?source=collection_archive---------22-----------------------#2019-11-29">https://towardsdatascience.com/untangling-uk-politics-with-data-science-a5afe9a86923?source=collection_archive---------22-----------------------#2019-11-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="338a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们能通过分析问题找到答案吗？</h2></div><p id="12c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TL；在这篇文章中，我们使用 spaCy、BERT 和新颖的聚类算法提取了英国议会中讨论的关键主题</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/f7adc2b2f07cf9890521c982b11ddbda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cr9IbtK1K8J9ujSC49baiA.jpeg"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">The House of Commons in the early 19th Century, it was around this time (1833) that the practice of giving a Minister notice of a question by printing it in the House’s Notice Paper of future business began</figcaption></figure><blockquote class="lu lv lw"><p id="27ef" class="ki kj lx kk b kl km ju kn ko kp jx kq ly ks kt ku lz kw kx ky ma la lb lc ld im bi translated">"判断一个人要看他的问题，而不是他的回答。"</p><p id="093a" class="ki kj lx kk b kl km ju kn ko kp jx kq ly ks kt ku lz kw kx ky ma la lb lc ld im bi translated"><em class="it">伏尔泰</em></p></blockquote><p id="d446" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在过去的三年里，英国政治变得令人困惑。除了高度动荡的政治环境之外，误导性信息在网上和社交媒体上的传播显著增加。</p><p id="def2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将试图通过分析英国政治引擎源头的数据来“穿透”这种噪音；议会问题。</p><h2 id="ba39" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">什么是议会问题？</h2><p id="9628" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">议会问题是议员用来寻找信息或敦促采取行动的工具。他们要求部长们解释和维护他们部门的工作、政策决定和行动。[1]</p><p id="9bc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些问题可以使用 http://www.data.parliament.uk/<a class="ae mz" href="http://www.data.parliament.uk/" rel="noopener ugc nofollow" target="_blank">提供的 API 下载。</a></p><p id="8b11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从 API 下载所有问题、预处理和清理数据后，从 2013 年至今，我们剩下 255，000 个问题:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi na"><img src="../Images/f204c2f7c29556c14d473be5e569c4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mnZZ0Qm7UNVakKTBHryp6w.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">A sample of the data set for analysis</figcaption></figure><p id="a47c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据显示:</p><ul class=""><li id="bc1c" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">提出问题的议员</li><li id="371c" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">它所针对的部门</li><li id="0795" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">问题和答案的日期</li><li id="40ea" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">问题本身的文本。</li></ul><p id="2b3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于我们需要的大多数信息都包含在问题本身的文本中，我们需要找到一种方法来提取这些问题中传达的关键主题，然后才能更详细地分析这些主题。</p><p id="dafc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">输入伯特</strong></p><p id="6487" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模型在 NLP 中的发展速度之快令人惊讶。不仅新的和更强大的模型正在定期开发，开源社区也在不断进步，使得这些模型的使用越来越直接。</p><p id="81ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用先进的<a class="ae mz" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html]" rel="noopener ugc nofollow" target="_blank"> BERT </a>模型来帮助我们从问题数据中提取信息。在过去的几个月里，这个模型的部署变得越来越简单，最近它被集成到了 spaCy 库中，使得在 NLP 管道中使用它变得很容易。</p><p id="9ef5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用 BERT 将单词转换为上下文相关的单词向量，然后我们可以对每个问题进行加权，以产生代表每个问题语义的向量。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi np"><img src="../Images/f6a8d2750c7bbf177c4f760a8adfcacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TpkPIKA88u0z0iB6hk3JnA.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">2019, the year that NLP became more popular than the Muppets [2]</figcaption></figure><p id="aaed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 1。用 BERT </strong>创建单词向量</p><p id="69a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">spaCy NLP 库为我们做了大量的工作。它最近发布了对许多 transformer 模型的支持，这些模型允许最先进的功能，同时仍然使用简单的 spaCy API，这使得它非常受欢迎。只需几行代码就可以启动并运行 BERT:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="b4d2" class="mb mc it nr b gy nv nw l nx ny">!pip install spacy-transformers --quiet</span><span id="cd64" class="mb mc it nr b gy nz nw l nx ny">!python -m spacy download en_trf_bertbaseuncased_lg #requires the runtime to be restarted if using Colab...</span><span id="ae6d" class="mb mc it nr b gy nz nw l nx ny">import spacy<br/>import torch</span><span id="2bc2" class="mb mc it nr b gy nz nw l nx ny">is_using_gpu = spacy.prefer_gpu()</span><span id="c5eb" class="mb mc it nr b gy nz nw l nx ny">if is_using_gpu:</span><span id="1e24" class="mb mc it nr b gy nz nw l nx ny">    torch.set_default_tensor_type("torch.cuda.FloatTensor")</span><span id="c05e" class="mb mc it nr b gy nz nw l nx ny">nlp = spacy.load("en_trf_bertbaseuncased_lg")</span></pre><p id="3908" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们加载了模型，使用<em class="lx">张量</em>属性从模型中获取单词向量就非常简单了:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="de1f" class="mb mc it nr b gy nv nw l nx ny">doc = nlp("This is an example question")</span><span id="a104" class="mb mc it nr b gy nz nw l nx ny">doc.tensor[0] #returns the vector for the first word (This)</span></pre><p id="4b41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2。使用 TF-IDF 为每个问题创建一个加权向量</strong></p><p id="6c96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有能力从 BERT 中获取问题中每个单词的向量，但我们现在需要一种方法来将这些向量组合成每个问题的单个向量。有不同的技术来做到这一点，平均和求和向量已被证明工作良好。</p><p id="333a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，我们在这里需要小心。伯特是一个非常强大的模型，但它不能读取我们的思想！从数据来看，回答问题的方式有不同的模式，例如:</p><ul class=""><li id="fcaa" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated"><em class="lx">“他的部门过去采用什么标准…”</em></li><li id="7716" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="lx">“她的部门正在采取什么措施来……”</em></li><li id="5b72" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated"><em class="lx">“他的部门正在采取什么措施来……”</em></li></ul><p id="dddc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们不小心，我们的分析会被问题的结构而不是内容所迷惑。我们需要一种方法来过滤掉这些噪音，这样我们就可以把重点放在代表问题本身的单词上。</p><p id="fc6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> TF-IDF 前来救援</strong></p><p id="8f01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于 TF-IDF 与词向量的结合，我之前已经写过<a class="ae mz" rel="noopener" target="_blank" href="/supercharging-word-vectors-be80ee5513d"><strong class="kk iu"/></a>了，所以这里就不赘述了。这是一种简单却非常强大的聚合单词向量的方法，同时还能过滤掉噪音。</p><p id="ea36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了证明这个概念，我们来举个例子:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="4115" class="mb mc it nr b gy nv nw l nx ny">import IPython</span><span id="67fd" class="mb mc it nr b gy nz nw l nx ny">tkn = tfidf.build_tokenizer()sent = df.questionText.values[236178].lower()<br/>sent = tkn(sent)html=''for wrd in sent:  <br/>   try:    <br/>      weight = (tfidf.idf_[tfidf.vocabulary_[wrd]])*10<br/>      print(weight/10)  <br/>   except:    <br/>      weight = 1    <br/>      print('{} not found'.format(wrd))  <br/>   html+='&lt;span style="font-size:{}px"&gt;{}&lt;/span&gt;'.format(weight,wrd) <br/>IPython.display.HTML(html)</span></pre><p id="7fbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这从我们的数据框架中随机抽取一个问题，并对每个单词进行加权。然后将它添加到 HTML 中，这样我们就可以看到 TF-IDF 对每个单词的影响:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oa"><img src="../Images/46d0767cd33de451b750c8e8f05efee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mskaTSg57r69RAgg2qdGAw.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">The results from TF-IDF. Notice how it is able to reduce the importance of ‘noisy’ words whilst amplifying words which have greater meaning to the question.</figcaption></figure><p id="8fec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由此我们可以看出，这种方法在过滤噪声的同时放大更有见地的单词是非常有效的。</p><p id="50fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">把这个和来自 spaCy 的 BERT 向量放在一起，我们得到下面的，它为每个问题创建一个向量，然后把这个向量保存到一个 numpy 数组中。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="21a2" class="mb mc it nr b gy nv nw l nx ny">import spacy<br/>import torch<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>import IPython<br/>from tqdm import tqdm</span><span id="0ffa" class="mb mc it nr b gy nz nw l nx ny">is_using_gpu = spacy.prefer_gpu()<br/>if is_using_gpu:<br/>   torch.set_default_tensor_type("torch.cuda.FloatTensor")</span><span id="fdad" class="mb mc it nr b gy nz nw l nx ny">nlp = spacy.load("en_trf_bertbaseuncased_lg")</span><span id="ec52" class="mb mc it nr b gy nz nw l nx ny">vectorizer = TfidfVectorizer(min_df=0.0,lowercase=True)<br/>tfidf = vectorizer.fit(df.questionText.values)<br/>tkn = tfidf.build_tokenizer()</span><span id="4b97" class="mb mc it nr b gy nz nw l nx ny">print('creating a lookup dictionary') #this speeds up the script significantly...</span><span id="6d8a" class="mb mc it nr b gy nz nw l nx ny">tfidf_lookup = {}<br/>for key,value in tfidf.vocabulary_.items():<br/>   tfidf_lookup[key]=tfidf.idf_[value]</span><span id="e615" class="mb mc it nr b gy nz nw l nx ny">vect = []</span><span id="62bd" class="mb mc it nr b gy nz nw l nx ny">for doc in tqdm(nlp.pipe(df.questionText.values,batch_size=5000)):<br/>   weighted_doc_tensor = []<br/>   try:<br/>      for cnt, wrd_vec in enumerate(doc.tensor):<br/>         word = doc[cnt].text<br/>         try:<br/>            weight = tfidf_lookup[word.lower()]<br/>         except:<br/>            weight = 0.5<br/>         pass<br/>         doc.tensor[cnt] = doc.tensor[cnt]*weight<br/>      vect.append(np.mean(doc.tensor,axis=0))<br/>   except:<br/>      vect.append(np.zeros(768,))#if it is blank...<br/>   pass</span><span id="71bd" class="mb mc it nr b gy nz nw l nx ny">vect = np.vstack(vect)<br/>np.save('question_vects_tfidf.npy', vect)</span></pre><p id="9a50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3。分组问题以确定关键主题</strong></p><p id="fd18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有了 255，000 个问题的向量，我们可以通过使用降维和聚类的组合从这些向量中识别关键主题:</p><p id="e11b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.1 降维:UMAP </strong></p><p id="66fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我以前使用过<a class="ae mz" rel="noopener" target="_blank" href="/elmo-contextual-language-embedding-335de2268604"> TSNE </a>来降低文档嵌入的维度，但是在本文中我想尝试 UMAP 算法。这类似于 TSNE，但有一个额外的好处，即保持数据的“宏观”结构不变，并识别本地集群。它也适用于大型数据集。一个 Python 库以优秀的<a class="ae mz" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">UMAP-学习</a>的形式提供。</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="294f" class="mb mc it nr b gy nv nw l nx ny">!pip install umap-learn --quiet</span><span id="e134" class="mb mc it nr b gy nz nw l nx ny">import umap</span><span id="73c8" class="mb mc it nr b gy nz nw l nx ny">reducer = umap.UMAP(n_neighbors=25)</span><span id="635d" class="mb mc it nr b gy nz nw l nx ny">y = reducer.fit_transform(vect)</span></pre><p id="a8dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该库以符合数据的方式紧密遵循 Scikit Learn。在该算法中选择的邻居的数量可以用于平衡对局部聚类和数据集的整体结构的强调。为了创建对数据进行聚类的特性，我使用该参数的值 5 和 25 运行了该算法两次。</p><p id="84b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.2 聚类:HDBSCAN </strong></p><p id="922b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">HDBSCAN 与 DBSCAN 相关。这两种算法都被设计成基于密度来寻找聚类，并且可以自动识别聚类的数量。此外，它们可以突出不属于任何聚类的离群点。</p><p id="f572" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lx">注意:</em> </strong> <em class="lx">在使用基于密度的聚类算法如 HDBSCAN 或 DBSCAN 时，使用 UMAP 或 TSNE 来降维可能是危险的。其原因是原始数据集的密度没有被保留[3]。因此，仔细审查这种方法的结果以确保所创建的集群符合分析的目标是很重要的。</em></p><p id="b614" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">HDBScan 的额外优势是能够处理不同密度的集群。与大多数聚类算法一样，调整参数(min_cluster_size 和 min_samples)更像是一门艺术，而不是科学，您需要检查结果并迭代以找到这些参数的最佳值:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="b7dc" class="mb mc it nr b gy nv nw l nx ny">!pip install hdbscan --quiet</span><span id="8438" class="mb mc it nr b gy nz nw l nx ny">db = hdbscan.HDBSCAN(min_cluster_size=40, min_samples=1).fit(df[['x_tfidf_umap_5','y_tfidf_umap_5','y_tfidf_umap_25','x_tfidf_umap_25']])</span><span id="a39c" class="mb mc it nr b gy nz nw l nx ny">labels = db.labels_</span><span id="ad94" class="mb mc it nr b gy nz nw l nx ny">n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)<br/>n_noise_ = list(labels).count(-1)</span><span id="d609" class="mb mc it nr b gy nz nw l nx ny">print('Estimated number of clusters: %d' % n_clusters_)<br/>print('Estimated number of noise points: %d' % n_noise_)</span><span id="113e" class="mb mc it nr b gy nz nw l nx ny">df['cluster'] = db.labels_</span></pre><p id="3782" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.3 获取集群标签</strong></p><p id="6b0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用 TF-IDF(再次！)来更好地理解集群是如何工作的。</p><p id="d38c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过查找每个聚类的关键词，我们不仅可以了解这些聚类代表什么，还可以为每个聚类生成一些用户友好的标签:</p><pre class="lf lg lh li gt nq nr ns nt aw nu bi"><span id="f6ae" class="mb mc it nr b gy nv nw l nx ny">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="6848" class="mb mc it nr b gy nz nw l nx ny">vectorizer = TfidfVectorizer(stop_words='english')<br/>tfidf = vectorizer.fit_transform(df.questionText.values)</span><span id="da50" class="mb mc it nr b gy nz nw l nx ny">totals = 0</span><span id="cee1" class="mb mc it nr b gy nz nw l nx ny">for cluster in df.cluster.value_counts()[0:10].index:<br/>   stg = " ".join(df.loc[df.cluster==cluster].questionText.values)<br/>   response = vectorizer.transform([stg])<br/>   count = df.cluster.value_counts().loc[cluster]<br/>   totals += count<br/>   feature_array = np.array(vectorizer.get_feature_names())<br/>   tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]<br/>   n = 10<br/>   print("Cluster Label: {}, Items in Cluster: {}".format(cluster,count))<br/>   print(feature_array[tfidf_sorting][:n])</span></pre><p id="13c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这返回(去除噪声后的前 5 个聚类):</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ob"><img src="../Images/1999d4538b819b539cd9d0f1aa86899d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ub9zXdMD5EboyEzNiGcTzQ.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Success! We can clearly see that each cluster represents a specific political theme.</figcaption></figure><p id="f9d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太棒了，我们已经成功地建立了一种将每一个项目聚集成核心主题的方法。</p><h2 id="9bd9" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">将这一切结合在一起</h2><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oc"><img src="../Images/8f4104c80dc34b1c6b8dddda4d502ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xU45U1KQroTcS4zeMqBJ1w.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Viewing the key themes over time we see that these these are very reactive to key events. Brexit (green) dominates from mid 2016 onward.</figcaption></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bbdcc4c27d586c9f08ac0d16e2a3a9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/0*fj_-IGXakgIADbSF"/></div></figure><p id="00c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，这个练习告诉了我们关于英国政治的什么？随着时间的推移，对 10 大主题进行可视化显示，大多数问题都是针对当天的时事提出的。对 2016 年年中退出欧盟的大量关注并不令人惊讶，但图表显示了这在多大程度上扭曲了议会辩论，使卫生、能源和交通等话题黯然失色。</p><h2 id="d4dc" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">未来的工作</h2><p id="009c" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">当然，这只是触及了这个数据集所能做的事情的表面。此数据的其他可能用途包括:</p><ul class=""><li id="ede9" class="nb nc it kk b kl km ko kp kr nd kv ne kz nf ld ng nh ni nj bi translated">根据议员在议会中提出的问题对他们进行剖析。</li><li id="0a3c" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">理解政府的机构是否反映了下议院辩论的焦点。</li><li id="0029" class="nb nc it kk b kl nk ko nl kr nm kv nn kz no ld ng nh ni nj bi translated">在议会提问之前预测问题。</li></ul><h2 id="b6a0" class="mb mc it bd md me mf dn mg mh mi dp mj kr mk ml mm kv mn mo mp kz mq mr ms mt bi translated">参考资料:</h2><p id="b3ae" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">[1]来自 parliment . uk:<a class="ae mz" href="https://www.parliament.uk/documents/commons-information-office/p01.pdf" rel="noopener ugc nofollow" target="_blank">https://www . parliament . uk/documents/commons-information-office/P01 . pdf</a></p><p id="365e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]谷歌趋势 2019 年 11 月 24 日</p><p id="ef40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]https://umap-learn.readthedocs.io/en/latest/clustering.html<a class="ae mz" href="https://umap-learn.readthedocs.io/en/latest/clustering.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>