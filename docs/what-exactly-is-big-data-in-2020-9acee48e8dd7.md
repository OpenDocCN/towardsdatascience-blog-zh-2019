# 2020 年的大数据到底是什么？

> 原文：<https://towardsdatascience.com/what-exactly-is-big-data-in-2020-9acee48e8dd7?source=collection_archive---------22----------------------->

在本文中，我们将分解“大数据”的组成部分，什么使大数据具有挑战性，以及目前如何解决这个问题。大数据通常被用作一个时髦词，但它本质上描述了三类数据:快速、大量和复杂。让我们浏览一下这些类别，浏览一下示例，看看如何使用最新的解决方案来应对这些挑战。
“*大数据是一个术语，描述大量高速*、*复杂多变的数据，这些数据需要先进的技巧和技术来实现对信息的*、*存储*、*分发、*、*和分析*。”( [TechAmerica 基金会的联邦大数据委员会，2012 年](https://www.sciencedirect.com/science/article/pii/S0268401214001066#bib0175))

![](img/4f8407e2b08bb233c8bd21eda60f8c93.png)

# **快速数据**

那么什么是快数据呢？快速数据是在短时间内大量产生的数据。例子包括来自汽车工厂的传感器、心脏监视器、飞行仪器等的数据..

快速数据通常被视为实时或半实时数据流。数据流本质上是在产生时发送给消费者的数据包(想象一下通过天线接收的电视信号或通过电话中的固定电话发送的语音呼叫)。实时数据的定义是一个更深入的话题，然而，它本质上归结为需要在毫秒内更新的数据。数据流面临许多挑战。其中包括能够在不丢失数据的情况下捕获数据，处理流中的重复记录，将流数据与更大的历史数据集集成，以及执行实时分析。

传统的流服务，如 Rabbit MQ 和 Active MQ，是在单独的服务器上运行的，因此受限于服务器的容量。Apache Kafka 引入了分布式流服务的概念，其中数据流可以在多个节点(商用服务器)之间拆分。这实现了高吞吐量的流处理，允许每秒处理数百万条消息。随着向云迁移的到来，开发人员不再需要管理底层硬件。亚马逊 Kinesis 和谷歌 Pub/Sub 完全抽象出流媒体服务背后的服务器，使开发人员只需专注于业务逻辑和数据。

为了对流数据执行实时分析，通常使用 [ELK 栈](https://www.elastic.co/what-is/elk-stack)(弹性搜索、日志存储、Kibana)。ELK stack 是一个工具组合，使用 Log Stash 将各种来源的数据传输到弹性搜索中。弹性搜索以一种方式存储数据，在这种方式下可以很容易地搜索到某些短语和关键字。Kibana 是一个位于弹性搜索之上的工具。它在数据流入时实时分析数据，并允许用户查询数据和围绕数据创建指标。

# **超大数据**

“大数据”的第二个定义是数据足够大。

一个非常大的数据集的例子包括一家主要投资银行 20 年的股票交易历史，过去 20 年的所有信用卡交易，或者在一个非常受欢迎的网站(如 buzz feed)上的用户交互，等等..通常，这些数据的大小在万亿字节或千兆字节的范围内。

![](img/a0672e1ae017d51393777c74c4918e30.png)

由于存储和处理数据需要计算资源，因此处理非常大的数据集非常具有挑战性。根据存储容量，典型的商用硬盘可以存储大约 1tb 的数据。其中一些数据集的大小为数 Pb(数千个硬盘)。要处理这些数据，需要将它们存储在一个数据中心内的多个服务器(或者传统的 RAID 存储驱动器)上。此外，处理数据需要大量的物理内存和 CPU 内核。

随着 Hadoop(一种用于处理数据的开源分布式大数据生态系统)的发布，大部分数据处理成本都降低了。这个系统的核心是 Hive，一个类似 SQL 的处理系统，允许像数据库一样存储数据。数据处理在本地商用硬件上执行，使用 Map-Reduce 算法(为分布式数据处理而编写)。这些服务器中有许多存储在企业数据中心。然而，管理数据中心和 Hadoop 生态系统需要大量的成本和维护。

最近，大部分存储和处理已经转移到云中，亚马逊网络服务(AWS)和谷歌云平台(GCP)是主要参与者。AWS 和 GCP 都提供受管 map-reduce/spark 解决方案——AWS 弹性 Map Reduce 和 GCP 云数据 proc。他们还拥有大规模可扩展的数据库，分别是 Google Big Query 和 AWS Redshift，它们在 Pb 级数据集上提供关系数据库功能。这些解决方案的最大优势是用户无需管理底层硬件，而是专注于最重要的任务，即存储/移动数据和编写 SQL。

这个生态系统中另一个值得注意的参与者是 Snowflake，它提供了一个基于 AWS 环境的预付费数据处理生态系统。

# **复杂数据**

“大数据”的第三个方面是复杂数据。为了解释是什么让数据变得复杂，让我来描述一下理想的数据集是什么样子的。理想的数据集有一个定义好的模式，每个数据类型(字符串、数字、日期)都用一个主键和一个定义良好的数据字典来定义。该数据集中没有重复项，并且该数据集中的所有值都准确无误。

复杂数据代表的是与这一理想截然相反的数据。所有数据集都有一定程度的复杂性，但是，有些数据集处理起来本来就比较混乱。通常，这些复杂的数据集是非结构化的(不遵循行、列结构的数据)，不断变化，并且很差或没有文档记录。

![](img/c314a3ab44dcf212177fed1aef860fc0.png)

示例包括手动输入的表单，如医生的说明、来自博客帖子的书面文本、不断更新的 web 应用程序日志，以及深度嵌套的 XML 或 JSON 文件。

关于复杂数据的一个小提示是，大多数坏数据应该从源头上解决。数据工程师需要建立足够强大的系统来快速捕捉和过滤这些数据集，但是，数据最终应该由数据生产者进行校正。此外，建议尽可能与上游数据生产者达成定义明确的服务水平协议，以限制坏数据的数量，并使数据更易于管理。

解开复杂的数据集，并使它们符合一个结构化和干净的格式需要做大量的工作。每个数据质量问题都需要单独解决，例如重复记录、不存在的主键以及格式错误的字符和字符串。

有许多工具可以帮助理清这些数据。数据科学家和工程师通常使用 Jupyter 笔记本电脑和 Pandas 或 Spark 来处理、分析和可视化这些数据集。Jupyter notebook 为开发人员提供了一个交互式环境，他们可以在其中一行一行地遍历数据集并保存结果，最终节省了大量时间。Databricks 是另一个解决方案，它提供了一个现成的数据分析和处理环境，包括一个强大的 Jupyter 笔记本用户界面、专有的 Spark 功能和易于使用的节点管理功能。

还有第三方工具，如 Trifacta，它通过提供用户界面和现成的功能来自动化一些数据清理任务，并加快清理和分析工作，从而取代从头编写数据清理代码的需要。谷歌云有一个版本的云数据准备工具，作为 GCP 生态系统的一部分提供。

# 摘要

可以看出，这些挑战中的任何一类都很难解决。然而，大多数公司都在处理这三个问题的变种。这里的挑战变得更加巨大，因为人们必须权衡不同的工具和方法来解决他们的具体问题。

向云计算的转变大大降低了处理和管理大数据的障碍，因为公司不再需要数据中心和大量预算来购买专有软件。然而，很难雇佣到具备理解如何应用正确的工具来解决特定大数据问题的技能的人。

进一步降低使用大数据的障碍的下一步是应用机器学习来自动理解数据的结构并检测数据周围的变化。随着熟练人才成为大数据工作的瓶颈，未来 5-10 年内推出的下一代工具极有可能将围绕这一类别展开。

希望这篇文章为您提供了关于大数据的全面而简洁的概述！