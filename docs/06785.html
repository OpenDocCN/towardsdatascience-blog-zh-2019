<html>
<head>
<title>Introduction to Principal Component Analysis (PCA) — with Python code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA)简介-使用 Python 代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57?source=collection_archive---------15-----------------------#2019-09-27">https://towardsdatascience.com/introduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57?source=collection_archive---------15-----------------------#2019-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4497" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PCA 介绍——原因、时间和方式</h2></div><h2 id="aef3" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">动机</strong></h2><p id="e85a" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">想象一个有 100000 个句子的文本数据。文本中唯一单词的总数是 10000(词汇量)。如果每个句子由长度等于词汇大小的向量表示。在这个长度为 10000 的向量中，每个位置属于一个唯一的单词。对于我们这个句子的向量，如果某个词出现<strong class="lg iu"> <em class="lx"> k </em> </strong>次，那么这个词的索引对应的向量中元素的值就是<strong class="lg iu"> <em class="lx"> k </em> </strong>。对于词汇表中没有出现在我们句子中的每一个单词，向量元素是 0。这被称为一键编码。</p><p id="d760" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在，如果我们为这些句子中的每一个创建向量，我们有一组 100000 个向量。在矩阵形式中，我们有一个 100000*10000 大小的矩阵。这意味着我们的矩阵中有 10⁹元素。在 python 中，要估计占用的内存，</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="7f06" class="ki kj it mi b gy mm mn l mo mp">import numpy as np<br/>import sys<br/>sys.getsizeof(np.zeros((100000,10000)))<br/>&gt;&gt;&gt; 8000000112</span></pre><p id="6413" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这个数字以字节为单位，转换成 8000 兆字节。</p><p id="3fdd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如果我们能够在一个更小的矩阵上工作，一个具有更少但更重要的特征的矩阵，会怎么样？比方说，大约 100 个特征可以捕获数据的大部分信息。拥有更多功能通常是好事。但是更多的特征通常也包括噪声。噪声会降低任何机器学习算法的鲁棒性，尤其是在小数据集上(阅读<strong class="lg iu">维度的诅咒</strong>了解更多)。此外，我们将节省 100-1000 倍的内存消耗。当您将应用程序部署到生产环境中时，这一点非常重要。</p><p id="2d05" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">带着这个目的，我们来讨论一下 PCA 是如何帮助我们解决这个问题的。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="aec9" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">方差作为信息</h2><p id="3058" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在机器学习中，我们需要算法的功能来找出有助于区分数据类别的模式。特征的数量越多，差异(数据的变化)就越大，因此模型发现很容易做出“分割”或“边界”。但并不是所有的功能都能提供有用的信息。他们也会有噪音。如果我们的模型开始适应这种随机噪声，它将失去其鲁棒性。事情是这样的— <em class="lx">我们希望从可用的特征空间中组合出</em> <strong class="lg iu"> <em class="lx"> m 个</em> </strong> <em class="lx">特征，以获得最大的方差。注意，我们要组合而不是仅仅选择</em> <strong class="lg iu"> <em class="lx"> m </em> </strong> <em class="lx">特征。</em></p><h2 id="7f65" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">从数学角度来说</h2><p id="e847" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">假设一个数据点由矢量<strong class="lg iu"> <em class="lx"> x </em> </strong>表示，原本具有<strong class="lg iu"> <em class="lx"> n </em> </strong>特征。</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/8c562d8afd7481dd8201f8a678f783e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*IIWDRxyu-sFgxaK5AeVxTA.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">vector x</figcaption></figure><p id="659a" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我们的工作是将它转换成一个向量<strong class="lg iu"> <em class="lx"> z </em> </strong>，维数比<strong class="lg iu"><em class="lx"/></strong>n 少，比如说<strong class="lg iu"> <em class="lx"> m </em> </strong></p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/63d916467c9b1c7c0e1f744974f2fe7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*s44LzfFXadycWrKa3f7u2A.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">vector z</figcaption></figure><p id="c8d4" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这是通过<strong class="lg iu">线性变换</strong>实现的。具体来说，我们将一个矩阵乘以<strong class="lg iu"> <em class="lx"> x </em> </strong>，将其映射到一个<strong class="lg iu"> <em class="lx"> m- </em> </strong>维空间。考虑一个大小为<strong class="lg iu"> <em class="lx"> n*m </em> </strong>的矩阵<strong class="lg iu"><em class="lx"/></strong>。使用<strong class="lg iu"> <em class="lx"> W </em> </strong>，我们希望将<strong class="lg iu"> <em class="lx"> x </em> </strong>转换为<strong class="lg iu"> <em class="lx"> z </em> </strong>为—</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/2f24b3a926b9ca7bf2e9e6fd8dfeae73.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*5L_At6XXFIIC-z0bBe3rWA.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">PCA transformation. Wt is the transpose of W</figcaption></figure><p id="f163" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">对上述等式的简单解释—假设您有一个包含列的原始数据框—</p><figure class="md me mf mg gt my gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/714364057b4cf6ee34bc9cf29e26b19e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vq3myjeH4JVDJUugrxQZzQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Original Dataframe</figcaption></figure><p id="bc40" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在 PCA 之后，如果我们选择，比方说 3 个分量，数据帧看起来像这样—</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/631c28cc0f80d3dd5ef8af0b2d7e9855.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*J455srG50Zr2LVM-l0w33g.png"/></div></figure><p id="1ee4" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在哪里，</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/086a39f173c6c00509b8344e83e68728.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*sf_waW2gkEu2Nvrqh6jVYA.png"/></div></figure><p id="cdba" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">c1，c2，..而 d1，d2，…是标量。w 矩阵是</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi no"><img src="../Images/88c73727bc0601920b7e948c56a55720.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*r392seJs48O5p9zRb7P7Ug.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">W — a 5*2 matrix</figcaption></figure><p id="d622" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">考虑下面的示例图。指向东北的轴沿其方向覆盖了更多的分布。那么第二大“覆盖”轴指向西北。</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi np"><img src="../Images/42390be470d7b8f35d6ffa9ad1fae32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/0*OQP5dk7ZhsCGVQA-"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Image credits: <a class="ae nq" href="https://i1.wp.com/enhancedatascience.com/wp-content/uploads/2017/05/PCA1.png?resize=629%2C424" rel="noopener ugc nofollow" target="_blank">https://i1.wp.com/enhancedatascience.com/wp-content/uploads/2017/05/PCA1.png?resize=629%2C424</a></figcaption></figure><h2 id="d88f" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">如何获得 W？</h2><p id="013a" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">上面的描述对于一个关于 PCA 的小对话来说已经足够好了。但是如果你对 W 的计算很好奇，这一节是给你的。如果你对数学不感兴趣，你可以跳过它。</p><p id="0746" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我们讨论过 PCA 是关于计算方差最大化的轴。考虑一个<strong class="lg iu">单轴 w </strong>，我们希望沿着它最大化方差。数学上，</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/82f6688efb3dc96a215d521dbec0e83e.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*C8Gm6DzDX6LdhYLNo_7DEw.png"/></div></figure><p id="e9f0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">为了计算方差，我们需要均值。在统计学中，最好称之为期望，用<em class="lx"> E 表示。</em>随机变量的期望就是它的均值。在我们的例子中，我们将原始特征的期望值 x 表示为μ。对于我们转换后的特征，<em class="lx"> z </em>我们的期望值是<em class="lx"> wt* </em> μ(见下图)</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d737af9cf448c41593d14221118bf547.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*hn_9JeH3rdvbdDshV0tBfg.png"/></div></figure><p id="8b83" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如果你感到困惑，参考下面的等式，取 LHS 和 RHS 的平均值</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3844b44415301a0fa98a2054a4e35469.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*b4dGpgnTUGFjAm1TExF3bw.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">matrix multiplication is commutative</figcaption></figure><p id="1732" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">设变换特征的平均值为</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/81dbeffd77d24c858b16266d1960320c.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*-DjOimW8HjPCUCn1qY24Hg.png"/></div></figure><p id="5263" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">随机变量 z 的样本方差由下式给出:</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d708513338f9fc81e20e1ba1d48330e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*eKscm3Zifhn2UIHvdlzsyg.png"/></div></figure><p id="a15a" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">用期望和从期望操作中取出<em class="lx"> w </em>来表示(因为它是一个常数，不依赖于<em class="lx"> z </em></p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/25a8376c7c5108ce3132b2adbca8be95.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*Chq9Ab8OJXEQ0-S2HRlZCw.png"/></div></figure><p id="49c5" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">原来，<em class="lx"> z </em>的方差就是<em class="lx">wt * x(sigma)* w</em>的协方差。协方差是多维随机变量的方差，在我们的例子中是<em class="lx"> x </em>。它是一个大小为<em class="lx"> k*k </em>的方阵，其中<em class="lx"> k </em>是<em class="lx"> x </em>的尺寸。是一个<strong class="lg iu">对称矩阵</strong>。<a class="ae nq" href="https://stattrek.com/matrix-algebra/covariance-matrix.aspx" rel="noopener ugc nofollow" target="_blank">关于协方差矩阵的更多信息</a></p><p id="2762" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">在我们最大化方差之前，我们对我们的<em class="lx"> w </em>矩阵的范数施加一个重要的条件。标准应该是 1</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ab4d53416c3eec612d632713e9045d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*PqtRd5Q78DpVbwe_6GTUHQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Norm = 1</figcaption></figure><p id="8ca0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">为什么？否则，最大化方差的简单方法就是增加范数，而不是以最大化方差的方式重新调整主轴。这个条件包括使用拉格朗日乘数</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/4f8ff560687aeb7f556a2855c621c639.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*V4V10IUyDAQ98tLqJWJvDA.png"/></div></figure><p id="bafd" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如果我们对上面的表达式求导，我们得到—</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/301f99b387991c74ce2bf7c7d6c6b116.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*9gpexqEJ47bhJQh-ROCxgQ.png"/></div></figure><p id="9964" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">看看最后一个表情。协方差矩阵乘以<em class="lx"> w </em>等于标量α乘以<em class="lx"> w </em>。这意味着，<em class="lx"> w </em>是协方差矩阵的<strong class="lg iu">特征向量</strong>，α是对应的<strong class="lg iu">特征值</strong>。利用这个结果，</p><figure class="md me mf mg gt my gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/c06e847514cac01668cd8ff335c271dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*pECdnJR6JacD56glRvWEeg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">var(z) is the eigenvalue</figcaption></figure><p id="a42e" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如果要使<em class="lx"> var(z) </em>最大化，那么α必须是所有特征值中最大的。并且对应的特征向量是主分量</p><p id="2c41" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated"><em class="lx">这为我们提供了第一个主成分，与任何其他成分相比，沿着该主成分解释的方差是最大的。以上是第一个轴的推导。类似地导出其他轴。重点是深入研究 PCA 背后的数学。</em></p><h2 id="ee4d" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数学说够了！给我看看代码</h2><p id="9ac3" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">我们使用的数据集是 Coursera 的课程评论，可在<a class="ae nq" href="https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上获得。这个练习的代码可以在我的<a class="ae nq" href="https://github.com/DhruvilKarani/PCA-blog-notebook/blob/master/PCA.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a> repo 上找到。</p><p id="22af" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">顾名思义，由评论(句子)和评分(1，2，3，4，5)组成。我们收集了 50，000 条评论，使用<strong class="lg iu"> TF-IDF </strong>对它们进行清理和矢量化。</p><p id="a71c" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated"><strong class="lg iu"> TF-IDF </strong>是一种给句子中的每个单词分配分数的评分方法。如果该词在所有评论中不常出现，但在总体上频繁出现，则得分高。比如<em class="lx">课程</em>这个词出现频率很高，但是几乎每隔一个复习就出现一次。所以这个词的 TF-IDF 分数会在下面。另一方面，<em class="lx">差</em>这个词出现频率很高，而且只针对负面评论。所以它的分数更高。</p><p id="108c" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">因此，对于每次审查，我们有固定长度的 TF-IDF 向量。向量的长度就是词汇量的大小。在我们的例子中，TF-IDF 矩阵的大小为 50000*5000。而且 99%以上的元素都是 0。因为如果一个句子有 10 个单词，5000 中只有 10 个元素是非零的。</p><p id="f69c" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我们使用主成分分析来降低维数。<em class="lx">但是怎么决定要带多少组件呢？</em></p><p id="e2b0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">让我们来看一下解释的方差图与组件或轴数的关系</p><figure class="md me mf mg gt my gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ob"><img src="../Images/99de7d76e2cca7a275b9e5d6acc9770a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmhgWHufhaQUoGvwdJIYnw.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Variance explained vs the number of components</figcaption></figure><p id="5b1e" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如您所见，前 250 个组件解释了 50%的变化。前 500 个组件占 70%。如您所见，增量收益减少了。当然，如果你取 5000 个组件，解释的方差将是 100%。但是那么做 PCA 有什么意义呢？</p><p id="18b0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi">— — — —</p><p id="92c3" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">我已经开始了我的个人博客，我不打算在媒体上写更多令人惊叹的文章。通过订阅<a class="ae nq" href="https://thenlp.space/" rel="noopener ugc nofollow" target="_blank">帮助空间</a>来支持我的博客</p></div></div>    
</body>
</html>