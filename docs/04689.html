<html>
<head>
<title>Explorations in Named Entity Recognition, and was Eleanor Roosevelt right?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索命名实体识别，埃莉诺·罗斯福是对的吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218?source=collection_archive---------14-----------------------#2019-07-17">https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218?source=collection_archive---------14-----------------------#2019-07-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e0b6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 spaCy 自然语言处理库从新闻文章中获取洞察力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/66283449a0a473f99411f495220a6a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jASAUQ-O4WEY5m5US8_cqg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image credit: unsplash</figcaption></figure><p id="48b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">据说埃莉诺·罗斯福说过:</p><blockquote class="lu"><p id="ea63" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">伟大的头脑讨论想法；普通人讨论事件；心胸狭窄的人讨论人。</p></blockquote><p id="4d3e" class="pw-post-body-paragraph ky kz it la b lb mf ju ld le mg jx lg lh mh lj lk ll mi ln lo lp mj lr ls lt im bi translated">虽然这可能是<a class="ae mk" href="https://quoteinvestigator.com/2014/11/18/great-minds/" rel="noopener ugc nofollow" target="_blank">的误解，</a>这种说法似乎引起了许多人的直觉共鸣，但它有多真实呢？经得起推敲吗？</p><p id="76d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多方法可以对此进行调查，一个有趣的方法可能是在一堆报纸中寻找<strong class="la iu">思想</strong>、<strong class="la iu">事件</strong>和<strong class="la iu">人物</strong>，看看它们出现的比例是否与读者的“思想大小”(伟大、普通、渺小)相关。</p><p id="8752" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了从报纸文章中挖掘信息，我决定使用一种称为命名实体识别(NER)的自然语言处理技术，这种技术用于识别句子中称为“命名实体”的东西。命名实体是诸如产品、国家、公司、数字之类的东西。为此，我将使用自然语言处理库。下面是他们的文档中的一个例子，展示了 NER 标签的外观:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/2a2017d33da614bacf3a03e76dcd70f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9o6i47NIW_qTLqdhSvXe3w.png"/></div></div></figure><p id="b035" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">spaCy 识别以下实体:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="8fcf" class="mr ms it mn b gy mt mu l mv mw">PERSON:      People, including fictional.<br/>NORP:        Nationalities or religious or political groups.<br/>FAC:         Buildings, airports, highways, bridges, etc.<br/>ORG:         Companies, agencies, institutions, etc.<br/>GPE:         Countries, cities, states.<br/>LOC:         Non-GPE locations, mountain ranges, bodies of water.<br/>PRODUCT:     Objects, vehicles, foods, etc. (Not services.)<br/>EVENT:       Named hurricanes, battles, wars, sports events, etc.<br/>WORK_OF_ART: Titles of books, songs, etc.<br/>LAW:         Named documents made into laws.<br/>LANGUAGE:    Any named language.<br/>DATE:        Absolute or relative dates or periods.<br/>TIME:        Times smaller than a day.<br/>PERCENT:     Percentage, including ”%“.<br/>MONEY:       Monetary values, including unit.<br/>QUANTITY:    Measurements, as of weight or distance.<br/>ORDINAL:     “first”, “second”, etc.<br/>CARDINAL:    Numerals that do not fall under another type.</span></pre><p id="23f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以看出，我们有人物和事件，但非常缺乏想法。为了纠正这一点，我们需要从其他人中选择一个来扮演思想的代理人。为此，我选择了百分比。这样做的原因是百分比通常是一种描述抽象概念的方式，人们在谈论例如作为一个整体的人类时使用它，而不是这个或那个人。从想法上来说，这不是一张完美的地图，但我们必须利用现有的资料。</p><p id="fe93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至于读者的“思维规模”，我会选择科尔曼-廖可读性指数。这是一种量化读者必须达到何种教育水平才能理解文本的方法，使用以下公式计算:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="9e84" class="mr ms it mn b gy mt mu l mv mw">Coleman_Liau = 0.0588*L–0.296*S-15.8</span><span id="ebd9" class="mr ms it mn b gy mx mu l mv mw">L = Average number of letters per 100 characters<br/>S = Average number of sentences per 100 characters</span></pre><p id="b73a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，这个类比并不完美，但希望足够好。</p><p id="5138" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来我们已经建立了所有的方法，让我们开始处理数据吧！</p><h1 id="c58d" class="my ms it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">获取和清理数据</h1><p id="45cf" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">我们将使用免费订阅的《newsapi.org》杂志提供的新闻提要。这意味着我们将获得一个<strong class="la iu">标题</strong>，一个<strong class="la iu">描述</strong>(文章的摘要)和文章的<strong class="la iu">内容</strong>的前 260 个字符。我决定挑选一些主要来自美国和英国的流行英文报纸:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="136e" class="mr ms it mn b gy mt mu l mv mw">sources = ['abc-news', 'cnn', 'fox-news', 'cbs-news', 'the-new-york-times', 'reuters', 'the-wall-street-journal', 'the-washington-post', 'bloomberg', 'buzzfeed', 'bbc-news', 'daily-mail']</span></pre><p id="784e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在获得数据(2019 年 6 月的 13，368 篇文章)后，我检查了这些数据，发现有几篇中文和阿拉伯文的文章会给 spaCy 带来问题。我用在<a class="ae mk" href="https://stackoverflow.com/questions/3094498/how-can-i-check-if-a-python-unicode-string-contains-non-western-letters" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>上找到的一个函数清理了它:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="ed25" class="mr ms it mn b gy mt mu l mv mw">latin_letters= {}</span><span id="f324" class="mr ms it mn b gy mx mu l mv mw">def is_latin(uchr):<br/>    try: <br/>        return latin_letters[uchr]<br/>    except KeyError:<br/>        try:<br/>             return latin_letters.setdefault(<br/>             uchr, 'LATIN' in ud.name(uchr))<br/>        except:<br/>            print(uchr)<br/>            raise Exception()</span><span id="4eaa" class="mr ms it mn b gy mx mu l mv mw">def only_roman_chars(unistr):<br/>    return all(is_latin(uchr) for uchr in unistr if uchr.isalpha())</span></pre><p id="d3eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">清理之后，我们剩下 11458 个帖子，分布在不同的来源:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="5014" class="mr ms it mn b gy mt mu l mv mw">df.groupby('source').count()['title']</span><span id="4075" class="mr ms it mn b gy mx mu l mv mw">abc-news                   1563<br/>bbc-news                   1076<br/>bloomberg                    56<br/>buzzfeed                    295<br/>cbs-news                    780<br/>cnn                         809<br/>daily-mail                 1306<br/>fox-news                   1366<br/>reuters                     916<br/>the-new-york-times         1467<br/>the-wall-street-journal     590<br/>the-washington-post        1234</span></pre><p id="1de6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我决定使用<strong class="la iu">描述</strong>作为 NER 标签的基础，因为我们希望根据文章的内容来标记文章，而描述似乎是最适合的。</p><p id="a429" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 Coleman-Liau，我们将使用<strong class="la iu">内容</strong>，因为这样可以更好地反映文章的整体写作风格。</p><p id="9c6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完成后，我们可以开始提取我们的实体:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="220a" class="mr ms it mn b gy mt mu l mv mw">ners = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']</span><span id="09ad" class="mr ms it mn b gy mx mu l mv mw"># The ners we are most interested in<br/>ners_small = ['PERSON', 'EVENT', 'PERCENT']</span><span id="bb8f" class="mr ms it mn b gy mx mu l mv mw">nlp = spacy.load("en_core_web_sm")</span><span id="23b9" class="mr ms it mn b gy mx mu l mv mw">df['ner'] = df['Description'].apply(lambda desc: dict(Counter([ent.label_ for ent in nlp(desc).ents])))</span><span id="e25d" class="mr ms it mn b gy mx mu l mv mw">for ner in ners:<br/>    df[ner] = df['ner'].apply(lambda n: n[ner] if ner in n else 0)</span></pre><p id="2e64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们按来源对它们进行分组，并对它们进行标准化:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="b533" class="mr ms it mn b gy mt mu l mv mw">df_grouped_mean = df.groupby('source').mean()</span><span id="69dd" class="mr ms it mn b gy mx mu l mv mw"># Normalize <br/>df_grouped = df_grouped_mean[ners].div(<br/>    df_grouped_mean[ners].sum(axis=1), axis=0)<br/>df_grouped['coleman_content'] = df_grouped_mean['coleman_content']</span><span id="f741" class="mr ms it mn b gy mx mu l mv mw"># Do the same for the smaller ners-set<br/>df_grouped_small = df_grouped_mean[ners_small].div(<br/>   df_grouped_mean[ners_small].sum(axis=1), axis=0)<br/>df_grouped_small['coleman_content'] = <br/>   df_grouped_mean['coleman_content']</span></pre><h1 id="8f38" class="my ms it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">看着结果</h1><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="b0a0" class="mr ms it mn b gy mt mu l mv mw">fig, axes = plt.subplots(nrows=3, ncols=1)</span><span id="9e5d" class="mr ms it mn b gy mx mu l mv mw">df_grouped[ners].iloc[:4].plot(kind='bar', figsize=(20,14), rot=10, ax=axes[0], legend=False);<br/>df_grouped[ners].iloc[4:8].plot(kind='bar', figsize=(20,14), rot=10, ax=axes[1]);<br/>df_grouped[ners].iloc[8:].plot(kind='bar', figsize=(20,14), rot=10, ax=axes[2], legend=False);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/950c0b2317ce132ad3559c82b12f7056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2QqrXoClb_HkHBkVYEE_Qw.png"/></div></div></figure><p id="ead9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个条形图可能有点难以解释，所以让我们来看看不同的新闻来源重点领域，或者他们拥有最多的实体:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="a68b" class="mr ms it mn b gy mt mu l mv mw">focus = []</span><span id="f052" class="mr ms it mn b gy mx mu l mv mw">for source in df_grouped[ners].values:<br/>    focus.append(sorted([(ners[i],x) for i,x in enumerate(source)], key=lambda x: x[1], reverse=True)[:3])<br/>        <br/>df_grouped['focus'] = [' '.join([y[0] for y in x]) for x in focus]<br/>df_grouped['focus']</span><span id="48df" class="mr ms it mn b gy mx mu l mv mw">abc-news                          ORG GPE DATE<br/>bbc-news                        GPE PERSON ORG<br/>bloomberg                       ORG GPE PERSON<br/>buzzfeed                   ORG PERSON CARDINAL<br/>cbs-news                        PERSON ORG GPE<br/>cnn                             PERSON ORG GPE<br/>daily-mail                     PERSON DATE GPE<br/>fox-news                        ORG PERSON GPE<br/>reuters                           DATE GPE ORG<br/>the-new-york-times              PERSON GPE ORG<br/>the-wall-street-journal         ORG GPE PERSON<br/>the-washington-post             GPE ORG PERSON</span></pre><p id="879b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，让我们列出在某个主题中所占比例最大的新闻来源:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="32bf" class="mr ms it mn b gy mt mu l mv mw">largest_in_topic = {}</span><span id="2f54" class="mr ms it mn b gy mx mu l mv mw">for n in ners:<br/>    largest_in_topic[n] = list(df_grouped.sort_values(n,ascending=False).index[:3])</span><span id="a244" class="mr ms it mn b gy mx mu l mv mw">largest_in_topic</span><span id="c2ed" class="mr ms it mn b gy mx mu l mv mw">{'PERSON': ['cnn', 'daily-mail', 'the-new-york-times'],<br/> 'NORP': ['the-washington-post', 'the-new-york-times', 'fox-news'],<br/> 'FAC': ['the-new-york-times', 'abc-news', 'fox-news'],<br/> 'ORG': ['buzzfeed', 'the-wall-street-journal', 'bloomberg'],<br/> 'GPE': ['abc-news', 'the-washington-post', 'bbc-news'],<br/> 'LOC': ['bloomberg', 'abc-news', 'the-washington-post'],<br/> 'PRODUCT': ['the-wall-street-journal', 'daily-mail', 'buzzfeed'],<br/> 'EVENT': ['bbc-news', 'bloomberg', 'reuters'],<br/> 'WORK_OF_ART': ['cbs-news', 'fox-news', 'the-new-york-times'],<br/> 'LAW': ['bloomberg', 'the-wall-street-journal', 'cnn'],<br/> 'LANGUAGE': ['bbc-news', 'fox-news', 'the-new-york-times'],<br/> 'DATE': ['reuters', 'daily-mail', 'cbs-news'],<br/> 'TIME': ['bbc-news', 'daily-mail', 'cbs-news'],<br/> 'PERCENT': ['bloomberg', 'buzzfeed', 'cbs-news'],<br/> 'MONEY': ['bloomberg', 'the-wall-street-journal', 'cbs-news'],<br/> 'QUANTITY': ['buzzfeed', 'bbc-news', 'cnn'],<br/> 'ORDINAL': ['bbc-news', 'cbs-news', 'reuters'],<br/> 'CARDINAL': ['bloomberg', 'cbs-news', 'abc-news']}</span></pre><p id="7e22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有一些有趣的事情需要注意:</p><ul class=""><li id="0de9" class="nv nw it la b lb lc le lf lh nx ll ny lp nz lt oa ob oc od bi translated">几乎每个人都喜欢谈论国家、公司和人物。</li><li id="32a0" class="nv nw it la b lb oe le of lh og ll oh lp oi lt oa ob oc od bi translated">正如所料，华尔街日报和彭博喜欢金钱和组织。</li><li id="c7cd" class="nv nw it la b lb oe le of lh og ll oh lp oi lt oa ob oc od bi translated">路透社喜欢精确日期。</li></ul><p id="e6d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们只看较小的 NER 集，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/182c270d6948ee060612713343fb9de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nth2pm0QaRKpT98RZeoT_A.png"/></div></div></figure><p id="39a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好的，看起来不错。是时候计算一下科尔曼-廖指数了。为此，我们需要能够分解成句子，这比我们想象的要困难得多。我将使用来自<a class="ae mk" href="https://stackoverflow.com/questions/4576077/python-split-text-on-sentences" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>的函数:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="3f7a" class="mr ms it mn b gy mt mu l mv mw">import re<br/>alphabets= "([A-Za-z])"<br/>prefixes = "(Mr|St|Mrs|Ms|Dr)[.]"<br/>suffixes = "(Inc|Ltd|Jr|Sr|Co)"<br/>starters = "(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"<br/>acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"<br/>websites = "[.](com|net|org|io|gov)"<br/><br/>def split_into_sentences(text):<br/>    text = " " + text + "  "<br/>    text = text.replace("\n"," ")<br/>    text = re.sub(prefixes,"\\1&lt;prd&gt;",text)<br/>    text = re.sub(websites,"&lt;prd&gt;\\1",text)<br/>    if "Ph.D" in text: text = text.replace("Ph.D.","Ph&lt;prd&gt;D&lt;prd&gt;")<br/>    text = re.sub("\s" + alphabets + "[.] "," \\1&lt;prd&gt; ",text)<br/>    text = re.sub(acronyms+" "+starters,"\\1&lt;stop&gt; \\2",text)<br/>    text = re.sub(alphabets + "[.]" + alphabets + "[.]" + alphabets + "[.]","\\1&lt;prd&gt;\\2&lt;prd&gt;\\3&lt;prd&gt;",text)<br/>    text = re.sub(alphabets + "[.]" + alphabets + "[.]","\\1&lt;prd&gt;\\2&lt;prd&gt;",text)<br/>    text = re.sub(" "+suffixes+"[.] "+starters," \\1&lt;stop&gt; \\2",text)<br/>    text = re.sub(" "+suffixes+"[.]"," \\1&lt;prd&gt;",text)<br/>    text = re.sub(" " + alphabets + "[.]"," \\1&lt;prd&gt;",text)<br/>    if "”" in text: text = text.replace(".”","”.")<br/>    if "\"" in text: text = text.replace(".\"","\".")<br/>    if "!" in text: text = text.replace("!\"","\"!")<br/>    if "?" in text: text = text.replace("?\"","\"?")<br/>    text = text.replace(".",".&lt;stop&gt;")<br/>    text = text.replace("?","?&lt;stop&gt;")<br/>    text = text.replace("!","!&lt;stop&gt;")<br/>    text = text.replace("&lt;prd&gt;",".")<br/>    sentences = text.split("&lt;stop&gt;")<br/>    sentences = sentences[:-1]<br/>    sentences = [s.strip() for s in sentences]<br/>    return sentences</span></pre><p id="5734" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">进行计算:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="c7ec" class="mr ms it mn b gy mt mu l mv mw">def calculate_coleman(letter_count, word_count, sentence_count):<br/>    return 0.0588 * letter_count*100/word_count - 0.296 *   <br/>           sentence_count*100/word_count - 15.8</span><span id="f191" class="mr ms it mn b gy mx mu l mv mw">df['coleman'] = df['split_content'].apply(lambda x: calculate_coleman(<br/>    len(' '.join(x).replace(' ', '').replace('.', '')),<br/>    len([y for y in ' '.join(x).replace('’', '').split() if not y.isnumeric()]),<br/>    len(x)))</span><span id="5b21" class="mr ms it mn b gy mx mu l mv mw">df_grouped['coleman'].sort_values(ascending=False)</span><span id="5b15" class="mr ms it mn b gy mx mu l mv mw">bloomberg                  14.606977<br/>reuters                    13.641115<br/>bbc-news                   13.453002<br/>fox-news                   13.167492<br/>abc-news                   13.076667<br/>the-washington-post        13.025180<br/>the-wall-street-journal    12.762103<br/>cbs-news                   12.753429<br/>daily-mail                 12.030524<br/>cnn                        11.988568<br/>the-new-york-times         11.682979<br/>buzzfeed                   10.184662</span></pre><p id="da6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这有点令人惊讶；举例来说，我本以为《纽约时报》会更高一点，但从另一方面来看，这可能是对的。如果我有超过 260 个字符的内容，这可能会更准确，但下一层的 newsapi 是 449 美元/月。只是为了确保稍后我会对照<a class="ae mk" href="http://www.adamsherk.com/publishing/news-sites-google-reading-level/" rel="noopener ugc nofollow" target="_blank">外部源</a>再次检查可读性分数。</p><h1 id="1aae" class="my ms it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">寻找相关性</h1><p id="76c5" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">让我们根据人员、事件和百分比绘制可读性图:</p><div class="kj kk kl km gt ab cb"><figure class="ok kn ol om on oo op paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/6472f9f02d3ffb13f6a0b54b88e106c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*ToVGVHAwprQomikIPC1h2w.png"/></div></figure><figure class="ok kn oq om on oo op paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7b32fe8a104890260914c9f7b724e739.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*v96ro_0YIFVHRMux0JaMLw.png"/></div></figure><figure class="ok kn or om on oo op paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/1532f43e50dac5f2d6019a812351830f.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*Yowrn-2a9jvLu1WDWDTYvw.png"/></div></figure></div><p id="3ad6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的是，实际上似乎有一点关联，至少在人和事件上。让我们计算相关分数:</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="839c" class="mr ms it mn b gy mt mu l mv mw">df_grouped_small.corr()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5702ced20be9569ebd344fb62411e637.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*jcSSa02QmRxJ-YW9teijQw.png"/></div></figure><p id="ff83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看 coleman_content 专栏，埃莉诺·罗斯福(Eleanor Roosevelt)的话可能真的有些道理！至少在科尔曼-廖和人之间存在负相关，而在科尔曼-廖和事件之间存在正相关。</p><p id="195b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于事件是为“普通”人设计的，我们预计散点图会移动到高事件值的中间，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/1381560e9871e95aeca26a3aae536220.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*9QQYjkPnTN6ag8J887_BEg.png"/></div></figure><p id="b125" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这不是我们真正看到的，但人/事的负/正相关性仍然为引用提供了一些可信度。</p><p id="5e38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，这要用一桶盐来吃。除了到目前为止我们所做的所有让步，我们没有足够的样本来达到统计学意义。实际上，我们来看看 p 值(来自<a class="ae mk" href="https://stackoverflow.com/questions/25571882/pandas-columns-correlation-with-statistical-significance" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>的函数):</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="3281" class="mr ms it mn b gy mt mu l mv mw">from scipy.stats import pearsonr</span><span id="a75d" class="mr ms it mn b gy mx mu l mv mw">def calculate_pvalues(df):<br/>    df = df.dropna()._get_numeric_data()<br/>    dfcols = pd.DataFrame(columns=df.columns)<br/>    pvalues = dfcols.transpose().join(dfcols, how='outer')<br/>    for r in df.columns:<br/>        for c in df.columns:<br/>            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)<br/>    return pvalues</span><span id="97fa" class="mr ms it mn b gy mx mu l mv mw">calculate_pvalues(df_grouped_small)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b87ef85af2a82c96fd71569feef0b0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*_iw_yAsGWNC8SqJmIlGd1A.png"/></div></figure><p id="f0e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如所料，p 值很低，除了百分比。</p><p id="6432" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于计算出的 Coleman-Liau 等级似乎有点偏差，我决定用以下可读性等级进行测试，这些等级摘自<a class="ae mk" href="http://www.adamsherk.com/publishing/news-sites-google-reading-level/" rel="noopener ugc nofollow" target="_blank">http://www . adamsherk . com/publishing/news-sites-Google-reading-level/</a></p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="f78d" class="mr ms it mn b gy mt mu l mv mw">reading_level = {'abc-news': (41,57,1),'cnn': (27,69,2),<br/>   'fox-news': (23,73,2),'cbs-news': (28,70,0),<br/>   'the-new-york-times': (7,85,7),'reuters': (6,85,7),<br/>   'the-wall-street-journal': (9,88,2),<br/>   'the-washington-post': (24,72,2),'bloomberg': (6,81,11)}</span></pre><p id="a383" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们给出了 3 个值(初级、中级、高级)，我用不同的权重(-1，0，1)来计算一个值。</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="804a" class="mr ms it mn b gy mt mu l mv mw">df_grouped_small['external_reading_level'] = df_grouped_small.index.map(<br/>    lambda x: reading_level[x][2]-reading_level[x][0] if x in reading_level else 0)</span></pre><p id="1991" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看这种相关性</p><pre class="kj kk kl km gt mm mn mo mp aw mq bi"><span id="7293" class="mr ms it mn b gy mt mu l mv mw">df_grouped_small[df_grouped_small['external_reading_level'] != 0][ners_small + ['external_reading_level']].corr()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/42cf9ffd64537ec2dd612d8bea1c21c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*wZkTe_RmPEctNxOjTslb6Q.png"/></div></figure><p id="09ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们发现相关性和我们之前得到的相似，除了我们实际上和百分比有更高的正相关性。</p><h1 id="7bb3" class="my ms it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">结论</h1><p id="d738" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">我们的结果表明，引用的话实际上可能有一定的真实性，但统计意义如此之低，以至于需要进一步的研究。此外，事实证明，无论思想的大小，人们都喜欢谈论别人，非常喜欢。即使是最聪明的新闻(彭博，高得 14.6 分)谈论人的次数也是谈论事件或百分比的 7 倍。</p><p id="f200" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">观察柱状图，另一件突出的事情是报纸在内容选择上是多么的相似。因此，尽管人们的利益有所不同，但最终我们的相似之处要多于不同之处。</p></div></div>    
</body>
</html>