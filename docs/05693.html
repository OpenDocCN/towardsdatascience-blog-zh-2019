<html>
<head>
<title>Shall we build transparent models right away?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们要马上建立透明模型吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/shall-we-build-transparent-models-right-away-196db0eeba6c?source=collection_archive---------22-----------------------#2019-08-20">https://towardsdatascience.com/shall-we-build-transparent-models-right-away-196db0eeba6c?source=collection_archive---------22-----------------------#2019-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5220" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可解释的人工智能(xAI)是新的酷小子，xAI 方法(建立一个黑盒，然后解释它)现在是机器学习从业者最珍视的工作方式。这真的是最好的路线吗？为什么我们不马上建立一个可解释的模型呢？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/1af4d196c62ad638c4b37bd40c2f7667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKmOjsYY8HSqcLjZCP-wog.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd le"><em class="lf">Rashomon</em> </strong>(羅生門 <em class="lf">Rashōmon</em>) is a 1950 <a class="ae lg" href="https://en.wikipedia.org/wiki/Jidaigeki" rel="noopener ugc nofollow" target="_blank">Jidaigeki</a> film directed by <a class="ae lg" href="https://en.wikipedia.org/wiki/Akira_Kurosawa" rel="noopener ugc nofollow" target="_blank">Akira Kurosawa</a>. The film is known for a plot device that involves various characters providing subjective, alternative, self-serving, and contradictory versions of the same incident. (<a class="ae lg" href="https://en.wikipedia.org/wiki/Rashomon" rel="noopener ugc nofollow" target="_blank">wikipedia</a>)</figcaption></figure><h1 id="7631" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">可解释与可解释的人工智能</h1><p id="b501" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">可解释性和可解释性是两个不同的概念，尽管在不同的来源中，这两个概念似乎被错误地互换使用。在这篇博文中，我将根据以下定义[7]进行推理，至少从我的角度来看，这些定义似乎被最广泛地采用:</p><ul class=""><li id="e504" class="mk ml it js b jt ju jx jy kb mm kf mn kj mo kn mp mq mr ms bi translated">可解释的 ML:使用黑盒并在事后解释它</li><li id="402f" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">可解释的 ML:使用透明的模型，即不是黑盒</li></ul><p id="06b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，基于这些定义，可解释性是一个模型属性，而可解释的 ML 指的是旨在解释黑盒模型的工具和方法论。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a556e31efa59b25f8f355e9d9633c1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*nUfSE9A-WFsQbHLuzzxbiA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">xkcd: curve fitting [0]</figcaption></figure><h1 id="a830" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">我们应该首先建立一个黑盒子吗？</h1><p id="be7f" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">辛西娅·鲁丁[1]的采访很好听，让人耳目一新，她的文章也是[2]。在众多观点中，辛西娅提出了两个有趣的观点:</p><h2 id="1d2c" class="mz li it bd lj na nb dn ln nc nd dp lr kb ne nf lv kf ng nh lz kj ni nj md nk bi translated">1.可解释的 ML 方法提供了不可靠的解释</h2><p id="6885" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">当前主流的可解释 AI / Ml 的解释没有多大意义:可解释 Ml 方法提供的解释并不忠实于原始模型计算的内容。根据定义，这是正确的，即使对于本地代理人也是如此；最重要的是，目前可用的局部替代方法是不稳定的，即不健壮[3]</p><h2 id="6912" class="mz li it bd lj na nb dn ln nc nd dp lr kb ne nf lv kf ng nh lz kj ni nj md nk bi translated">2.准确性和可解释性之间没有权衡</h2><p id="99cd" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">福布斯写道"<em class="nl">更复杂，但也可能更强大的算法，如神经网络，包括随机森林在内的集成方法，以及其他类似的算法牺牲了透明性和可解释性</em>【可解释性，根据以上定义】<em class="nl">以获得功率、性能和准确性</em>【4】。按照同样的思路，DARPA [6]。真的吗？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/995d2d2954c33bc3e10425ec4a122d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLH6wT5O6GnzpjsFPsiDXw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Learning Performance vs Explainability (Interpretability according to the definitions above) [6]</figcaption></figure><p id="4f2c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不。准确性和可解释性之间没有权衡。罗生门集提供了一个证明:<em class="nl">认为数据允许一个相当准确的预测模型的大集合存在。因为这组精确的模型很大，所以它通常包含至少一个可解释的模型。这个模型既可解释又准确。</em>【2】</p><p id="d91a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为什么我们不马上建立一个可解释的模型呢？</p><h1 id="be5f" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">回到我们对可解释性的定义</h1><p id="c059" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">在这篇博文的开始，我们介绍了可解释 ML 的概念。根据我们的定义，<em class="nl">可解释的</em>是那些不是<strong class="js iu"> </strong>黑箱的算法。这到底是什么意思？</p><p id="fcbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们看看罗生门集[2，10]:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/8165d6e222e8847e16dc8724b77c17ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwR1FhsC9IeUSxZz_VNh8Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Rashomon sets: existence of a simple but accurate model [10]</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/040dbf233bd8fcb34409c09a8838b1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnQrrV92A-_aOaBJIEBjMg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Rashomon sets: classes of functions F2 that can be approximated with functions from classes F1 within δ using a specified norm [10]</figcaption></figure><p id="bcab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，可解释性是数学家标记为<em class="nl">透明</em>(黑盒的反义词)[9]的函数类的一个属性。</p><p id="e303" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，线性模型类比深度神经网络更容易解释的事实似乎完全没有争议。然而，从实践的角度来看，<em class="nl">线性模型并不比深度神经网络</em>、<em class="nl">更具可解释性，特别是对于高维度模型或存在大量工程特征的情况</em>。<em class="nl">线性模型的权重可能看起来很直观，但它们在特征选择和预处理方面可能很脆弱</em>【9】。</p><p id="9993" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，从业者对<em class="nl">可解释性</em>的定义中没有任何内容表明，属于可解释模型类别的模型将被关键决策者(例如:法官)或关键利益相关者(例如:申请假释的囚犯)理解。在我们对<em class="nl">可解释性</em>的定义中，没有任何东西暗示一个模型在可解释性方面最终会满足它的需求。</p><blockquote class="np"><p id="1d1c" class="nq nr it bd ns nt nu nv nw nx ny kn dk translated">“但是在地下的某个地方，将会有各种各样的计算不可约性，我们永远也不能真正把它们带入人类理解的领域”——史蒂夫·沃尔夫勒姆[8]</p></blockquote><h1 id="f277" class="lh li it bd lj lk ll lm ln lo lp lq lr ls nz lu lv lw oa ly lz ma ob mc md me bi translated">可解释性的许多方面</h1><p id="5bd5" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">从利益相关者的角度来看，使用一个可解释的模型类别，其中可解释的意思是透明的(与黑箱相反),并没有带来太多的东西。那么可解释性的定义应该是什么呢？不幸的是，可解释性有很多方面。</p><p id="d163" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">不同的任务可能需要不同的解释需求。[12]提供了一个非详尽的假设列表，说明是什么使任务在解释需求上相似:</p><ul class=""><li id="169f" class="mk ml it js b jt ju jx jy kb mm kf mn kj mo kn mp mq mr ms bi translated">全局与局部:<em class="nl">全局可解释性意味着知道总体上存在什么模式(比如控制星系形成的关键特征)，而局部可解释性意味着知道特定决策的原因(比如为什么某个贷款申请被拒绝)。前者在以科学理解或偏见检测为目标时可能很重要；后者是当一个人需要一个具体决定的理由时。</em></li><li id="ed41" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">面积，不完整的严重程度:<em class="nl">问题公式化的哪一部分不完整，有多不完整？一方面，人们可能会对自动驾驶汽车如何做出决策感到好奇。另一方面，可能希望检查特定的场景列表(例如，导致汽车偏离道路 10 厘米的传感器输入集)。在这两者之间，人们可能想要检查一个一般属性—安全的城市驾驶—而不需要场景和安全标准的详尽列表。不完整的严重程度也可能影响解释需求。</em></li><li id="52ec" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">时间限制。<em class="nl">用户能花多长时间来理解解释？</em></li><li id="fe9e" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">用户专业知识的性质。<em class="nl">用户在任务中的经验如何？</em></li></ul><p id="c3ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之，我看不到我们有任何机会在短期内就可解释性的一个独特的、银弹定义达成一致。</p><p id="100b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一方面，我们有奥卡姆剃刀和泛化理论方法，旨在将算法的泛化属性表达为模型复杂性定义的函数[13]。一般化理论有一个严格的、公认的问题定义；是的，复杂性很可能与可解释性负相关(不管定义是什么)。为什么我们不把奥卡姆剃刀作为我们的指导原则呢？</p><h1 id="c310" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">结果</h1><ul class=""><li id="e393" class="mk ml it js b jt mf jx mg kb oc kf od kj oe kn mp mq mr ms bi translated">当前对<em class="nl">可解释性</em>(可解释的模型不是黑盒)的定义并没有给模型的涉众带来任何价值</li><li id="233b" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">奥卡姆剃刀原则应该仍然是指导原则</li><li id="0f51" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">可解释的 ML 方法论(例如:LIME [11])提供了不可靠的解释。然而，在数据科学家手中，它们仍然是模型理解和调试的有价值的工具</li></ul><h1 id="bcba" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">参考</h1><p id="5d76" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">[0] xkcd，曲线拟合，【https://xkcd.com/2048/ T2】</p><p id="8974" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]<a class="ae lg" href="https://twimlai.com/twiml-talk-290-the-problem-with-black-boxes-with-cynthia-rudin/" rel="noopener ugc nofollow" target="_blank">https://twimlai . com/twiml-talk-290-the-problem-with-black-box-with-Cynthia-rudin/</a></p><p id="713b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]辛西娅·鲁丁。"请停止解释高风险决策的黑箱模型."<em class="nl"> arXiv 预印本 arXiv:1811.10154 </em> (2018)。</p><p id="eae8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3] Alvarez-Melis、David 和 Tommi S. Jaakkola。"论可解释性方法的稳健性."<em class="nl"> arXiv 预印本 arXiv:1806.08049 </em> (2018)。</p><p id="0994" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4]理解可解释的 AI，福布斯，2019 ( <a class="ae lg" href="https://www.forbes.com/sites/cognitiveworld/2019/07/23/understanding-explainable-ai/#630f69af7c9e" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><p id="8cdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5]奥卡姆的威廉</p><p id="b287" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[6] XAI 计划更新，DARPA，(<a class="ae lg" href="https://www.darpa.mil/attachments/XAIProgramUpdate.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><p id="d6b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[7] Keith O'Rourke，可解释的 ML 与可解释的 ML</p><p id="1b7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[8]史蒂夫·沃尔夫勒姆，逻辑，可解释性和理解的未来，(<a class="ae lg" href="https://blog.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><p id="c96d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[9]利普顿，扎卡里·c .〈模型可解释性的神话〉。arXiv 预印本 arXiv:1606.03490  (2016)。(<a class="ae lg" href="https://arxiv.org/pdf/1606.03490.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><p id="6dfa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[10] Semenova、Lesia 和 Cynthia Rudin。"罗生门曲线和体积的研究:机器学习中一般化和模型简单性的新视角."<em class="nl"> arXiv 预印本 arXiv:1908.01755 </em> (2019)。</p><p id="8c8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[11]里贝罗、马尔科·图利奥、萨梅尔·辛格和卡洛斯·盖斯特林。“我为什么要相信你？:解释任何分类器的预测。<em class="nl">第 22 届 ACM SIGKDD 知识发现和数据挖掘国际会议论文集</em>。ACM，2016。(<a class="ae lg" href="https://arxiv.org/pdf/1602.04938.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><p id="a01a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[12]多希-维勒兹，大结局，和被金。"迈向可解释机器学习的严谨科学."<em class="nl"> arXiv 预印本 arXiv:1702.08608 </em> (2017)。(<a class="ae lg" href="https://arxiv.org/pdf/1702.08608.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><p id="99e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[13] Mattia Ferrini，一般化界限:依靠你的深度学习模型(<a class="ae lg" rel="noopener" target="_blank" href="/generalization-bounds-rely-on-your-deep-learning-models-4842ed4bcb2a?source=friends_link&amp;sk=eaa4cbccb0662f018b84c8080b7f2596">此处</a>)</p></div></div>    
</body>
</html>