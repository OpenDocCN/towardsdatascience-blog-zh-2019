<html>
<head>
<title>Beautiful visual model interpretation of classification strategies— Kannada MNIST Digits Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类策略的美丽视觉模型解读——卡纳达语 MNIST 数字识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kannada-visual-model-952cc6ec3a72?source=collection_archive---------28-----------------------#2019-09-11">https://towardsdatascience.com/kannada-visual-model-952cc6ec3a72?source=collection_archive---------28-----------------------#2019-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="99c7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">此处为副标题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d641562cfb25ecd618ca1e81a1a6c47c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2tyhY_sgaHcL-YoxyrpIQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Kannada handwritten digits</figcaption></figure><p id="a90a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">卡纳达语 MNIST 数据集是一个很棒的近期作品(<a class="ae lu" href="https://arxiv.org/abs/1908.01242" rel="noopener ugc nofollow" target="_blank">细节在此</a>)，我很高兴它也对公众开放。我相信很快这里的社区就会在这个数据集上发布最先进的准确性数据。所以，我在做一些不同的事情。</p><p id="31bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，我们将尝试可视化，尝试看到模型所看到的，逐个像素地评估事物。我们的目标是可解释性。在本文中，我将从“最简单的”、最容易解释的算法开始。希望我会在后面的文章中公布其他建模技术的结果。</p><p id="aa19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">重申并澄清:我不会专注于获得最佳表现。相反，我将着重于可视化输出，理解模型的意义，并理解它失败的地方和原因。当模型运行得不太好时，评估哪个更有趣。:)</strong></p><h1 id="add1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">可视化数字数据</h1><h2 id="fda5" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">函数绘制一个随机数字及其标签</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cce3" class="mn lw it na b gy ne nf l ng nh">def plot_random_digit():<br/>    random_index = np.random.randint(0,X_train.shape[0])<br/>    plt.imshow(X_train[random_index], cmap='BuPu_r')<br/>    plt.title(y_train[random_index])<br/>    plt.axis("Off")</span><span id="f317" class="mn lw it na b gy ni nf l ng nh">plt.figure(figsize=[2,2])<br/>plot_random_digit()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1b33046e9257c310e6a2618dc24da9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*FZe5kDGIwclevx46vvFY7g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">A random Kannada digit plotted as image</figcaption></figure><h2 id="fc8d" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">一口气看 50 个样本</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ef0c" class="mn lw it na b gy ne nf l ng nh">plt.figure(figsize=[10,6])<br/>for i <strong class="na iu">in</strong> range(50):<br/>    plt.subplot(5, 10, i+1)<br/>    plt.axis('Off')<br/>    if i &lt; 10:<br/>        plt.title(y_train[i])<br/>    plt.imshow(X_train[i], cmap='BuPu_r')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a9ab884ee34dac4a8b144367d5a1e9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*ZyqEcvv0IShIljVfbvSTLw.png"/></div></figure><p id="4eb8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作为一个不擅长阅读卡纳达文字的人，对我来说这些符号看起来有些相似</p><ul class=""><li id="c9c5" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">第三和第七条</li><li id="dc48" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt nq nr ns nt bi translated">第 6 和第 9 条</li></ul><p id="e25a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在开始时，我期望这些预测者在这两对之间会有些混淆。尽管这不一定是真的——也许我们的模型能比我更好地识别数字。</p><h1 id="b74a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">重塑数据集以构建预测模型</h1><p id="bec1" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">个别例子是 28 X 28。对于 scikit learn 中的大多数预测建模方法，我们需要将示例展平为 1D 数组。<br/>我们将使用 numpy 数组的整形方法。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="493b" class="mn lw it na b gy ne nf l ng nh">X_train_reshape = X_train.reshape(X_train.shape[0], 784)<br/>X_test_reshape = X_test.reshape(X_test.shape[0], 784)</span></pre><h1 id="653d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">建立和理解逻辑回归模型</h1><p id="e390" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">让我们为我们的多类分类问题建立一个逻辑回归模型。</p><p id="9773" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">请再次注意，我们不会关注获得最佳性能，而是关注如何理解模型所学的内容。</strong></p><p id="92b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归模型将很容易和有趣地分析系数，以了解模型学到了什么。<br/>在 SciKit-learn 中，可以通过多种方式制定多类分类。他们是-</p><ul class=""><li id="0dc9" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">一对休息</li><li id="09d3" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt nq nr ns nt bi translated">多项式</li></ul><p id="e406" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 1。一人 vs 其余:</strong></p><p id="5855" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也称为一对一，这种策略包括为每个类安装一个分类器。对于每个分类器，该类与所有其他类相匹配。这种方法的一个优点是它的可解释性。</p><p id="3e98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为每个类仅由一个分类器表示，所以可以通过检查相应的分类器来获得关于该类的知识。这是多类分类最常用的策略，也是一个公平的默认选择。</p><p id="4c9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于我们的例子，这意味着构建 10 个不同的分类器。</p><p id="454c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里了解更多:<br/><a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . multi class . onevsrestclassifier . html</a></p><p id="2981" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 2。多项式:</strong></p><p id="42ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在此策略中，我们使用线性预测器对看到给定输出的概率的对数进行建模。<br/>对于<code class="fe oe of og na b">multinomial</code>，最小化的损失是整个概率分布的多项式损失拟合。softmax 函数用于查找每个类别的预测概率。</p><p id="a326" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里阅读更多相关内容:<br/><a class="ae lu" href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Multinomial _ logistic _ regression # As _ a _ log-linear _ model</a></p><p id="8f54" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">注意</strong>:这种区别很重要，需要你为模型解释不同的系数。</p><h1 id="7afe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">首先，让我们使用 One vs. Rest 方案来构建我们的模型</h1><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2df8" class="mn lw it na b gy ne nf l ng nh">from sklearn.linear_model import LogisticRegression<br/>lr1 = LogisticRegression(solver="liblinear", multi_class="ovr")<br/><br/><em class="oh"># Fitting on first 10000 records for faster training  </em><br/>lr1.fit(X_train_reshape[:10000], y_train[:10000])</span></pre><h2 id="86cd" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">评估列车组的性能</h2><p id="2bf8" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">模型对训练数据的预测</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="4562" class="mn lw it na b gy ne nf l ng nh">from sklearn.metrics import confusion_matrix, accuracy_score, classification_report<br/>y_train_pred = lr1.predict(X_train_reshape[:10000])</span><span id="1213" class="mn lw it na b gy ni nf l ng nh">cm = confusion_matrix(y_train[:10000], y_train_pred[:10000])<br/><br/>plt.figure(figsize=[7,6])<br/>sns.heatmap(cm, cmap="Reds", annot=True, fmt='.0f')<br/>plt.show()</span></pre><h2 id="8b83" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">那可是非常高的训练准确率！过度拟合？</h2><p id="8336" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">还有，看起来模型是<strong class="la iu">不是</strong>在 3 和 7，6 和 9 之间非常混淆，至少在火车布景上不是。</p><h1 id="ac93" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">错误分析:检查错误分类的案例</h1><p id="e228" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">为了便于索引，我们将转换到熊猫系列，隔离错误分类的案例，绘制一些示例。</p><h2 id="d06d" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">11 例分类错误</h2><ul class=""><li id="1e67" class="nl nm it la b lb nz le oa lh oi ll oj lp ok lt nq nr ns nt bi translated">研究一些案例</li><li id="dea6" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt nq nr ns nt bi translated">选择 9 个随机案例——我们将绘制数字，以及真实和预测的标签</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/22b6178685fe4aa05e8c6dba6902e18c.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*9CBhzWeAszdWTR3qt-WWJQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The mis-classified cases</figcaption></figure><p id="07ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你能看出为什么这个模型是混乱的吗？让我们看看模型在测试集上表现如何。</p><h2 id="d0b9" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">测试集上的混淆矩阵</h2><p id="c452" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">对测试数据进行预测，并绘制混淆矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/71ae11f15570f2f7453ef31f7c82136f.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*dR2-IDglg12eP4QXXBRyLg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Confusion Matrix on test set — smells like over-fitting</figcaption></figure><h2 id="1860" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">看着混乱矩阵和分类报告-</h2><p id="9016" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">3、7 的回忆最少—模型明显混淆了它们。同样，4 和 5 之间也有混淆。还有，很多 0 被误认为 1 和 3。</p><p id="13ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好吧！所以看起来在测试集上性能急剧下降。很有可能我们在列车上过度适应了。</p><p id="6f2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们承认这一模式可以改进。</p><p id="a1ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，我们现在先不要担心这个。<strong class="la iu">让我们关注理解模型所学内容的方式。</strong></p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><h1 id="1bcb" class="lv lw it bd lx ly ou ma mb mc ov me mf jz ow ka mh kc ox kd mj kf oy kg ml mm bi translated">模型解释</h1><h2 id="4a64" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">了解每个像素的贡献</h2><p id="2b0e" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">我们现在学习的每个像素的系数都是基于一对静止方案的。</p><p id="5ab3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们继续分析我们的 OVR 模型的系数。</p><p id="fdf2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">系数 lr1.coef_ 的形状。“形状”是(10。784)，即每个标签有 784 个系数——每个数字的每个像素有 784 个系数！</p><p id="41b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正系数意味着该像素上的高值增加了该标签的机会，<strong class="la iu">与所有其他类</strong>相比。因此，系数告诉我们这个像素如何将这个标签与所有其他标签区分开来。</p><h2 id="4088" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">提取像素系数并绘制在标签 0 的热图上</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="195a" class="mn lw it na b gy ne nf l ng nh">plt.figure(figsize=[3,3])<br/>coefs = lr1.coef_[0].reshape(28,28)<br/>plt.imshow(coefs,cmap="RdYlGn",vmin=-np.max(coefs),vmax=np.max(coefs)) <em class="oh">#setting mid point to 0</em><br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/259a75785ec75e75ba39da79d31eb06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*rzVhJinLdvboN9yqwjdtPA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Heatmap for 0 — OVR</figcaption></figure><p id="5529" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我用了一种不同的颜色来区分正负符号。</p><p id="eeb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上图中，绿色像素是正值像素。图像告诉我们，某些像素中的值有助于将数字归类为 0。不出所料，中间的红色表示该范围内的值意味着该数字为零的可能性较低。黄色接近于 0，这意味着像素对区分没有任何帮助。</p><h2 id="c210" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated"><strong class="ak">制作所有数字的像素热图</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/f371200a8de33219070b7e59be3c8d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*H7I7yR75A4TNeA8Crjp8bw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Heatmap — all digits — OVR scheme</figcaption></figure><p id="6bcc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好好看看这些热图。这将揭示模型已经学习了什么。请注意，我们有“一与其余”的公式，特别是在与其他数字的热图进行比较时。</p><h1 id="ebfe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">现在，让我们使用多项式方案建立一个模型。</h1><ul class=""><li id="23d4" class="nl nm it la b lb nz le oa lh oi ll oj lp ok lt nq nr ns nt bi translated">我们需要将<code class="fe oe of og na b">multi_class</code>参数指定为“多项式”</li><li id="badb" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt nq nr ns nt bi translated">“liblinear”解算器不支持这一点，所以我们选择“sag”解算器。</li></ul><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9b69" class="mn lw it na b gy ne nf l ng nh">lr2 = LogisticRegression(random_state=42, multi_class="multinomial", solver="sag")<br/>lr2.fit(X_train_reshape[:10000], y_train[:10000])</span></pre><h2 id="459e" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">在测试集上评估性能</h2><p id="108b" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">绘制混淆矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6b5a9a34d8ee22c2ed6aa9f923878f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*JoK7HFob96EVBsRL16QOoQ.png"/></div></figure><h1 id="e7fb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">了解每个像素的贡献</h1><p id="5273" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">每个标签有 784 个系数——每个像素的系数。</p><p id="3774" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，一个正的系数意味着是什么使得这个标签成为现在这个样子！但是，如果 3 个标签在特定像素中具有相似的存在，则所有 3 个标签的系数可能具有相似的值。</p><h2 id="c2a6" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">提取像素系数并绘制在标签 0 的热图上</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/7601a3bcda6c12cc6f7f5eb1d6c6404e.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*9m_zHD8GNteuUe5q_uR0vg.png"/></div></figure><p id="f41a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这与 OVR 模型的热图有多不同/相似？<br/>让我们制作所有像素的热图。</p><h2 id="17cf" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">为所有数字制作这样的像素热图</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/63049c11af690a0332ed222f7c4a6fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*_Jz1EX8KWIwLmMCEAtQXsA.png"/></div></figure><h2 id="44f4" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">这些热图与每个标签的平均图像相比如何？</h2><p id="d330" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">绘制每个数字的平均图像。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ba42" class="mn lw it na b gy ne nf l ng nh">plt.figure(figsize=(10, 4))<br/>for i <strong class="na iu">in</strong> range(10):<br/>    plt.subplot(2,5,i+1), plt.title(i)<br/>    plt.imshow(np.mean(X_train[y_train==i],axis=0),cmap='gray')<br/>plt.suptitle('Mean images for each digit')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/aa20803db89812e3e2141bb95ad4c91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*JBQFSWZiQOnadIZ6cuiSyA.png"/></div></figure><h1 id="e0f7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">把它们都画在一起——好好看看。</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/371aabbf50d6f8bafa1f870c59216c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*dTczfKHRGKbogiEpxZGk4g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Mean images vs. OVR vs. Mutinomial</figcaption></figure><h1 id="d37b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">锻炼-</h1><p id="488a" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">你已经看到了 OVR 方法和多项式方法的热图。你也有每个标签的平均图像。</p><ul class=""><li id="c8d1" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">将热图与平均图像进行比较和对比。</li><li id="087b" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt nq nr ns nt bi translated">你认为这是怎么回事？你能试着理解每个数字的模型都学到了什么吗？</li><li id="b618" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt nq nr ns nt bi translated">为什么模型在某些数字上表现不佳？热图有助于理解吗？</li></ul><h1 id="1f71" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">对于那些有兴趣尝试更多东西的人来说</h1><p id="4ff2" class="pw-post-body-paragraph ky kz it la b lb nz ju ld le oa jx lg lh ob lj lk ll oc ln lo lp od lr ls lt im bi translated">我建议你试试下面的-</p><ol class=""><li id="0a3a" class="nl nm it la b lb lc le lf lh nn ll no lp np lt pd nr ns nt bi translated">使用带有正则化(ridge、lasso、elasticnet)的逻辑回归和使用交叉验证的超参数优化来减少过度拟合。</li><li id="9ca3" class="nl nm it la b lb nu le nv lh nw ll nx lp ny lt pd nr ns nt bi translated">利用奇异值分解/主成分分析对原始数据进行去噪和重构；接下来是一个优化的逻辑回归模型。</li></ol><p id="dcfe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，这就是我们这里的小演示！我将很快分享更多不同建模技术的演示，解释它们的方法，以及更多使用监督和非监督技术的相同数据集的实验。</p><h1 id="4583" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">觉得这个有意思？请继续关注更多这样的演示。</h1><h2 id="b9a9" class="mn lw it bd lx mo mp dn mb mq mr dp mf lh ms mt mh ll mu mv mj lp mw mx ml my bi translated">请分享您的评论/意见/建议！</h2></div></div>    
</body>
</html>