<html>
<head>
<title>Ridge Regression for Better Usage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更好使用的岭回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db?source=collection_archive---------0-----------------------#2019-01-03">https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db?source=collection_archive---------0-----------------------#2019-01-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b6e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的目标是让你更好地使用岭回归，而不仅仅是使用库提供的东西。那么，<em class="ko">“什么是岭回归？”。</em>回答问题最简单的方法是“<em class="ko">线性回归的变异”</em>。最糟糕的方法是从下面的数学方程式开始，很多人一看就不明白。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/ffa9b165264b8c20280f08a75ceb48db.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*gd9Tzg8lmKLY0ZXWaerU8w.png"/></div></figure><p id="0cc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">坏消息是我们仍然需要处理它，好消息是我们不会从这样的方程开始，尽管不是现在。我想从‘普通最小二乘法<em class="ko"/>(OLS)’开始。如果你碰巧很少或没有线性回归的背景知识，<a class="ae kx" href="https://www.youtube.com/watch?v=Qa2APhWjQPc" rel="noopener ugc nofollow" target="_blank">这段视频</a>将帮助你了解如何使用“最小二乘法”。现在，你知道 OLS 就像我们通常所说的“线性回归”，我将这样使用这个术语。</p><h2 id="fcfb" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">在继续前进之前</h2><p id="91c4" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">在接下来的部分中，我将使用不同的术语和数字采取不同的方法。有两件事你要记住。一是我们不喜欢过度拟合。换句话说，<strong class="js iu">我们总是喜欢捕捉一般模式的模型</strong>。另一个是我们的目标是从新数据中预测，而不是具体的数据。因此，<strong class="js iu">模型评估应该基于新数据(测试集)，而不是给定数据(训练集)</strong>。此外，我将交替使用以下术语。</p><ul class=""><li id="af15" class="lw lx it js b jt ju jx jy kb ly kf lz kj ma kn mb mc md me bi translated">自变量=特征=属性=预测值= <em class="ko"> X </em></li><li id="84a4" class="lw lx it js b jt mf jx mg kb mh kf mi kj mj kn mb mc md me bi translated">系数=β=<em class="ko">β</em></li><li id="48a5" class="lw lx it js b jt mf jx mg kb mh kf mi kj mj kn mb mc md me bi translated">残差平方和= RSS</li></ul><h1 id="ce79" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">为什么是 OLS，为什么不是</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/a6eecdb936aa58ba2ab6816f3f82f233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cEysrHZokqla0tXnZ-5GQ.png"/></div></div></figure><h2 id="6761" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated"><strong class="ak">最小二乘法找到<em class="ng">最佳</em>和<em class="ng">无偏</em>系数</strong></h2><p id="6f99" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">您可能知道最小二乘法可以找到最适合数据的系数。还要加上一个条件，就是它也能找到无偏系数。这里无偏是指<strong class="js iu"> OLS 不考虑哪个自变量比其他的更重要</strong>。它只是找到给定数据集的系数。简而言之，只需要找到一组 betas，从而得到最低的“残差平方和(RSS)”。问题变成了“RSS 最低的模型真的是最好的模型吗？”。</p><h2 id="ebb9" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">偏差与方差</h2><p id="db52" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">以上问题的答案是<em class="ko">“不太会”</em>。正如“无偏”这个词所暗示的，我们也需要考虑“偏见”。偏差是指一个模型对其预测者的关注程度。假设有两个模型用两个预测器“甜度”和“光泽”来预测苹果价格；一个模型无偏，一个模型有偏。</p><p id="b0a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，无偏模型试图找出两个特征和价格之间的关系，就像 OLS 方法一样。该模型将尽可能完美地拟合观测值，以最小化 RSS。然而，这很容易导致<a class="ae kx" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a>的问题。换句话说，该模型在处理新数据时表现不佳，因为它是专门为给定数据构建的，可能不适合新数据。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c51ed46f1327b3e888e926ad3dfe0500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*OkRTcykIzOlmfe4OCJN1hA.png"/></div></figure><p id="ce1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有偏模型不平等地接受其变量，以区别对待每个预测值。回到这个例子，我们想只关心“甜蜜”来建立一个模型，这应该在新数据中表现得更好。原因将在理解<strong class="js iu">偏差与方差</strong>后解释。如果你不熟悉偏差与方差的话题，我强烈推荐你观看<a class="ae kx" href="https://www.youtube.com/watch?v=EuBBz3bI-aA" rel="noopener ugc nofollow" target="_blank">这段视频</a>，它会给你带来深刻的见解。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/02640565a14fc83efea79aee966ca32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*wqDhhG2BjkBCl5WuHojddw.png"/></div></figure><p id="081a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以说<strong class="js iu">偏差与模型不符合训练集有关，方差与模型不符合测试集</strong>有关。偏差和方差在模型复杂性上是一种权衡关系，这意味着一个简单的模型将具有高偏差和低方差，反之亦然。在我们的苹果例子中，只考虑“甜味”的模型不会像同时考虑“甜味”和“光泽”的另一个模型那样符合训练数据，但更简单的模型将更好地预测新数据。</p><p id="9e24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是因为“甜度”是价格的决定因素，而“光泽”不是常识。我们人类都知道这一点，但数学模型不会像我们一样思考，只是计算给定的数据，直到它找到所有预测器和独立变量之间的某种关系，以拟合训练数据。</p><p id="84e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">* <em class="ko">注</em>:我们假设“甜蜜”和“闪亮”不相关</p><h1 id="71a3" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">岭回归开始起作用了</h1><p id="1419" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">查看<em class="ko">偏差与方差</em>图，Y 轴为“误差”，即“偏差和方差之和”。因为这两者基本上都与失败有关，所以我们想尽量减少它们。现在仔细看一下这个图，你会发现总误差最低的地方在中间。这通常被称为“甜蜜点”。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/6110c8554d5efa5b4ca62d0fb391dceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*cB0ESE9z3rB3-rpXPhwgWw.png"/></div></figure><p id="2be5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们回忆一下，OLS 平等地对待所有变量(无偏见)。因此，随着新变量的加入，OLS 模型变得更加复杂。可以说，OLS 模型总是在图的最右边，偏差最小，方差最大。它被固定在那里，从不移动，但我们想把它移到甜蜜点。这是岭回归大放异彩的时候，也称为<em class="ko">正则化</em>。I <strong class="js iu"> n 岭回归，你可以调整λ参数，使模型系数发生变化</strong>。这可以通过最后介绍的编程演示来更好地理解。</p><h1 id="2c29" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">岭回归的几何理解</h1><p id="4117" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">很多时候，图形有助于获得模型如何工作的感觉，岭回归也不例外。下图是比较 OLS 和岭回归的几何解释。</p><h2 id="3beb" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">等高线和 OLS 估计</h2><p id="9d3d" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">每个轮廓是 RSS 相同的点的连接，以 RSS 最低的 OLS 估计为中心。此外，OLS 估计是最适合训练集的点(低偏差)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi gj"><img src="../Images/3e65834c342ef09fd0ccb93739fa0a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pHwPfuhgTDFH8elIh_B2g.png"/></div></div></figure><h2 id="c0ce" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">圆和脊估计</h2><p id="2889" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">与 OLS 估计值不同，山脊估计值会随着蓝色圆圈大小的变化而变化。它仅仅是圆与最外轮廓相遇的地方。岭回归的工作原理就是我们如何调整圆的大小。关键是<strong class="js iu"> <em class="ko"> β </em>的变化在不同的层面</strong>。</p><p id="26d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设<em class="ko"> β </em> 1 是‘闪亮’，而<em class="ko"> β </em> 2 是‘甜蜜’。如您所见，随着圆大小的变化，脊<em class="ko"> β </em> 1 比脊<em class="ko"> β </em> 2 相对更快地下降到零(比较两个图)。之所以会出现这种情况，是因为 RSS 对<em class="ko"> β </em>的改变不同。更直观地说，轮廓不是圆，而是倾斜放置的椭圆。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nk"><img src="../Images/6a0094597d669e7d0c2be93c26db3161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGn5C4Qe2OIKkODiE6Cprw.png"/></div></div></figure><p id="6e75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">山脊<em class="ko"> β </em>永远不可能为零，只有<em class="ko">向其收敛</em>，这将在下面用数学公式解释。虽然像这样的几何表达式很好地解释了主要思想，但也有一个限制，那就是我们不能在三维空间上表达它。所以，这一切都归结为数学表达式。</p><h1 id="7c2a" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">数学公式</h1><p id="3738" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">我们已经看到了多元线性回归方程的一般形式和矩阵形式。可以写成另一个版本如下。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nl"><img src="../Images/661c527c70806df67abf58babf952166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMssBrKdIDKGdZBOvNJRvQ.png"/></div></div></figure><p id="5b98" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里<em class="ko"> argmin </em>的意思是使函数达到最小值的“最小值参数”。在上下文中，它找到最小化 RSS 的<em class="ko"> β </em>。我们知道如何从矩阵公式中得到β。现在，问题变成了“这和岭回归有什么关系？”。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/acf9f8cfb9e8a21b5328b0bef885df65.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*8R8-IckBY6Rw239ruufShg.png"/></div></figure><p id="62df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，岭回归是线性回归的一种变体。上面的项是 OLS 方程的脊约束。我们正在寻找<em class="ko"> β </em>，但是它们现在也必须满足上述约束。回到几何图形，C 相当于圆的半径，因此，<em class="ko"> β </em>应该落在圆的区域，可能在边缘的某个地方。</p><h2 id="d863" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">向量范数</h2><p id="03df" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">我们仍然想理解第一个方程。为此，我们需要温习一下向量范数，它无非是以下定义。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/af363cc9dae7214872922cb9233b1c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*FSvb8xU_eqvjXyXiXg7jrA.png"/></div></figure><p id="2835" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">订阅 2 是作为在' L2 范数'，你可以了解更多关于向量范数<a class="ae kx" href="http://mathworld.wolfram.com" rel="noopener ugc nofollow" target="_blank">在这里</a>。此时我们只关心 L2 范数，所以我们可以构造我们已经看到的方程。下面是最简单的，但仍然和我们一直在讨论的一样。请注意，下面等式中的第一项基本上是 OLS，然后第二项是 lambda，这就是岭回归。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/18abf1c0b670aaedfbd7d22d9827fbbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*LsI3XqHSjNCiteUoFo2zKA.png"/></div></figure><h2 id="261e" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">我们真正想找到的是</h2><p id="a950" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">lambda 项通常被称为“惩罚”,因为它增加了 RSS。我们将某些值迭代到 lambda 上，并用诸如“均方误差(MSE)”之类的度量来评估模型。因此，应该选择最小化 MSE 的λ值作为最终模型。这个<strong class="js iu">岭回归模型在预测上一般比 OLS 模型好</strong>。如下面的公式所示，如果λ等于零(没有惩罚)，脊<em class="ko"> β </em>随λ变化，并且变得与 OLS <em class="ko"> β </em>相同。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9140a55377a6a6feb85e147b529a108c.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*Rnl4jgKCG8oKuH7MgQ_Vxw.png"/></div></figure><h2 id="7e51" class="ky kz it bd la lb lc dn ld le lf dp lg kb lh li lj kf lk ll lm kj ln lo lp lq bi translated">为什么它收敛到零而不是变成零</h2><p id="56c4" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">使用我们之前看到的矩阵公式，λ以分母结束。这意味着如果我们增加 lambda 值，ridge <em class="ko"> β </em>应该会减少。但是不管 lambda 值设置得多大，ridge <em class="ko"> β </em>都不能为零。也就是说，岭回归为特征赋予不同的重要性权重，但不会丢弃不重要的特征。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a2b4e479a2ad331a358406b8b5ca7bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*524ctaHK1BIN9tqhHIOY8Q.png"/></div></figure><h1 id="af9f" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">使用数据集进行演示</h1><p id="a35d" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">来自<em class="ko"> sklearn </em>库的数据集‘波士顿房价’用于演示。在<a class="ae kx" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names" rel="noopener ugc nofollow" target="_blank">这个元数据</a>中解释了十几个特性。在整个演示过程中需要下面的<em class="ko"> python </em>库。</p><p id="feac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">* <em class="ko">完整代码</em>可以在<a class="ae kx" href="https://github.com/Q-shick/fundamentals_of_data_science/blob/master/mathematical%20_model/Ridge%20and%20Lasso.ipynb" rel="noopener ugc nofollow" target="_blank"> my github </a>找到</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="4d2f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在数据集已加载，接下来应该对要素进行标准化。由于岭回归是通过惩罚来缩小系数的，因此应该对要素进行缩放，以保证起始条件的公平性。本帖解释了这个问题的更多细节。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="874b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们可以迭代从 0 到 199 的 lambda 值。注意，λ等于零(<em class="ko"> x </em> = 0)时的系数与 OLS 系数相同。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="5554" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们可以从数据框中绘制绘图。为了更好的可视化，只选择了五个属性。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="1486" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">凭直觉，‘房’应该是房价最好的指标。这就是为什么红色的线在迭代过程中不会收缩。相反，“高速公路通道”(蓝色)显著减少，这意味着当我们寻求更通用的模型时，该特征失去了它的重要性。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ns"><img src="../Images/aa6930fca597e87af2cbf8f6c03eac64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*KeutQ9gUGBhoKbVC3iM9lQ.png"/></div></div></figure><p id="ec4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似的图案从其余部分会聚到零(黑色虚线)可见。如果我们越来越多地增加 lambda(极端偏置)，那么只有“房间”会保持显著，这又是有意义的，因为房间的数量必须解释大多数。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="dbea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的代码片段绘制了 lambda 跟踪的 MSE。由于随着λ值设置得越大，模型变得越简单(=有偏差)，所以 X 轴从左到右表示模型的简单性。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b1b27bc8b90040f055ff9f8c180ad3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*_Fyu956_WKQ6rDShce23yg.png"/></div></figure><p id="a443" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">绿色虚线来自上图中的 OLS，X 轴通过增加λ值绘制。MSE 值在开始时随着 lambda 值的增加而减小，这意味着模型预测在某一点上得到改进(误差更小)。简而言之，<strong class="js iu">一个有一定偏差的 OLS 模型比纯粹的 OLS 模型</strong>更好的预测，我们称这个修正的 OLS 模型为岭回归模型。</p><h1 id="8cee" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">结论</h1><p id="59f6" class="pw-post-body-paragraph jq jr it js b jt lr jv jw jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn im bi translated">我们从不同的角度研究岭回归，从数学公式、矩阵格式到几何表达式。通过这些，我们可以理解岭回归基本上是一个带惩罚的线性回归。通过演示，我们确认了<strong class="js iu">没有找到最佳λ</strong>的方程式。因此，我们需要迭代一系列值，并用 MSE 评估预测性能。通过这样做，我们发现岭回归模型比简单线性回归模型在预测方面表现得更好。</p><ul class=""><li id="4cc6" class="lw lx it js b jt ju jx jy kb ly kf lz kj ma kn mb mc md me bi translated">OLS 只是找到给定数据的最佳拟合</li><li id="9659" class="lw lx it js b jt mf jx mg kb mh kf mi kj mj kn mb mc md me bi translated">特性对 RSS 有不同的贡献</li><li id="f4d6" class="lw lx it js b jt mf jx mg kb mh kf mi kj mj kn mb mc md me bi translated">岭回归偏向于重要的特征</li><li id="9e43" class="lw lx it js b jt mf jx mg kb mh kf mi kj mj kn mb mc md me bi translated">MSE 或 R 平方可用于寻找最佳λ</li></ul><h1 id="eceb" class="mk kz it bd la ml mm mn ld mo mp mq lg mr ms mt lj mu mv mw lm mx my mz lp na bi translated">好的读物</h1><div class="nu nv gp gr nw nx"><a href="https://onlinecourses.science.psu.edu/stat501/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">欢迎来到 STAT 501！统计 501</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">这是 STAT 501 在线课程材料网站。在这方面有很多例子、笔记和讲座材料…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">onlinecourses.science.psu.edu</p></div></div><div class="og l"><div class="oh l oi oj ok og ol kv nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">sklearn.linear_model。ridge-sci kit-了解 0.20.0 文档</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">该模型求解一个回归模型，其中损失函数是线性最小二乘函数，正则化是线性最小二乘函数</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">scikit-learn.org</p></div></div><div class="og l"><div class="om l oi oj ok og ol kv nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://ipython-books.github.io/81-getting-started-with-scikit-learn/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">IPython 食谱- 8.1。scikit 入门-学习</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">IPython 食谱，</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">ipython-books.github.io</p></div></div><div class="og l"><div class="on l oi oj ok og ol kv nx"/></div></div></a></div></div></div>    
</body>
</html>