<html>
<head>
<title>A 5-Step Guide on incorporating Differential Privacy into your Deep Learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将差分隐私融入深度学习模型的 5 步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-5-step-guide-on-incorporating-differential-privacy-into-your-deep-learning-models-7861c6c822c4?source=collection_archive---------19-----------------------#2019-07-15">https://towardsdatascience.com/a-5-step-guide-on-incorporating-differential-privacy-into-your-deep-learning-models-7861c6c822c4?source=collection_archive---------19-----------------------#2019-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="542a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 PyTorch 和差分隐私对 MNIST 数字进行分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e564521429fa528453475cb5e1c79fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSN1a2xVtV1exzcD8fpzhA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@euwars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Farzad Nazifi</a> on <a class="ae ky" href="https://unsplash.com/search/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="a5a5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="86ca" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">到目前为止，我们都知道差分隐私的好处，它如何在保护个人隐私的同时仍然提供对大型数据集的准确查询结果。这篇文章将阐述如何将差分隐私应用于 MNIST 数字分类问题，并使用一种称为<strong class="lt iu">教师群体私人聚合(PATE)的技术对其进行分析。</strong></p><h1 id="7701" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">方法学</h1><p id="aec4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，我们将私有数据分成 N 个集合(在本例中为 100 个)，并在 N 个数据集的每一个上训练一个分类器。这些被称为<strong class="lt iu">师</strong>的量词。然后，我们将使用教师分类器来预测公共数据的标签。对于公共数据集中的每个图像，N 个分类器预测最多的标签将被认为是该图像的真实标签。</p><p id="05c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，使用<strong class="lt iu">老师</strong>分类器的预测作为我们公共数据的真实标签，我们将训练一个<strong class="lt iu">学生</strong>分类器，然后可以用来分类新的看不见的图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d7113b898638d5bd66dbaa3d688fe799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W9Nu6tkIkMAAllg0.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2: Overview of the approach: (1) an ensemble of teachers is trained on disjoint subsets of the sensitive data, (2) a student model is trained on public data labeled using the ensemble.</figcaption></figure><blockquote class="mt mu mv"><p id="1449" class="lr ls mw lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated"><em class="it">MNIST 训练数据将被视为私有数据，我们将在此基础上训练我们的教师模型。然后，通过组合教师模型的预测而获得的学生模型将在 MNIST 测试数据上被训练(90%的测试数据将用于训练模型，而剩余的 10%将用于测试其准确性)</em></p></blockquote><h1 id="d5b8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">好吧，但是隐私部分在哪里起作用？</h1><p id="30e2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">深度学习模型有过度拟合训练数据的趋势。神经网络可以学习个人的特征，而不是学习一般特征，然后对手可以利用这些特征来获取个人的私人信息。</p><p id="1719" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过不直接在私人数据上训练学生模型，我们防止它直接从数据集中学习单个人的关键个人特征。取而代之的是，由教师模型学习的概括特征和趋势被用于训练学生。</p><p id="303f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，有一个小警告。如果图像的标签可以通过移除单个教师的预测来改变，对手可以将搜索范围缩小到该模型。</p><p id="a55f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了避免这种情况，我们在选择预测最多的标签作为公共数据的真实标签之前，向教师模型的预测添加随机的<a class="ae ky" href="https://en.wikipedia.org/wiki/Laplace_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">拉普拉斯噪声</strong> </a> <strong class="lt iu"> </strong>。通过这种方式，我们增加了一点随机性，并扭曲了最终结果，这样真正的标签就不会因为去掉一个老师而轻易改变。</p><h1 id="32fc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用 PyTorch 实现差分隐私</h1><h2 id="a35c" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated"><strong class="ak">第一步:加载数据</strong></h2><p id="82d9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从<em class="mw"> torchvision </em>和<em class="mw">T3】导入 MNIST 数据，定义一个函数生成数据加载器。</em></p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="e966" class="na la it nn b gy nr ns l nt nu"><strong class="nn iu">import</strong> <strong class="nn iu">torch</strong><br/><br/><strong class="nn iu">from</strong> <strong class="nn iu">torchvision</strong> <strong class="nn iu">import</strong> datasets, transforms<br/><strong class="nn iu">from</strong> <strong class="nn iu">torch.utils.data</strong> <strong class="nn iu">import</strong> Subset<br/><br/><em class="mw"># Transform the image to a tensor and normalize it</em><br/>transform = transforms.Compose([transforms.ToTensor(),<br/>                                transforms.Normalize((0.5,), (0.5,))])<br/><br/><em class="mw"># Load the train and test data by using the transform</em><br/>train_data = datasets.MNIST(root='data', train=<strong class="nn iu">True</strong>, download=<strong class="nn iu">True</strong>, transform=transform)<br/>test_data = datasets.MNIST(root='data', train=<strong class="nn iu">False</strong>, download=<strong class="nn iu">True</strong>, transform=transform)</span><span id="4a52" class="na la it nn b gy nv ns l nt nu">num_teachers = 100 <em class="mw"># Define the num of teachers</em><br/>batch_size = 32 <em class="mw"># Teacher batch size</em><br/><br/><strong class="nn iu">def</strong> get_data_loaders(train_data, num_teachers):<br/>    <em class="mw">""" Function to create data loaders for the Teacher classifier """</em><br/>    teacher_loaders = []<br/>    data_size = len(train_data) // num_teachers<br/>    <br/>    <strong class="nn iu">for</strong> i <strong class="nn iu">in</strong> range(data_size):<br/>        indices = list(range(i*data_size, (i+1)*data_size))<br/>        subset_data = Subset(train_data, indices)<br/>        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size)<br/>        teacher_loaders.append(loader)<br/>        <br/>    <strong class="nn iu">return</strong> teacher_loaders<br/><br/>teacher_loaders = get_data_loaders(train_data, num_teachers)</span></pre><p id="2126" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，通过如上所述分割 MNIST 测试集来生成学生训练和测试数据。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="b5c2" class="na la it nn b gy nr ns l nt nu"><em class="mw"># Create the public dataset by using 90% of the Test data as train #data and remaining 10% as test data.</em></span><span id="efc9" class="na la it nn b gy nv ns l nt nu">student_train_data = Subset(test_data, list(range(9000)))<br/>student_test_data = Subset(test_data, list(range(9000, 10000)))<br/><br/>student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)<br/>student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)</span></pre><h2 id="fa9c" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">步骤 2:定义和培训教师模型</h2><p id="dcbe" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">定义一个简单的 CNN 来分类 MNIST 数字。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="f638" class="na la it nn b gy nr ns l nt nu"><strong class="nn iu">import</strong> <strong class="nn iu">torch.nn</strong> <strong class="nn iu">as</strong> <strong class="nn iu">nn</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">torch.nn.functional</strong> <strong class="nn iu">as</strong> <strong class="nn iu">F</strong><br/><strong class="nn iu">import</strong> <strong class="nn iu">torch.optim</strong> <strong class="nn iu">as</strong> <strong class="nn iu">optim</strong><br/><br/><strong class="nn iu">class</strong> <strong class="nn iu">Classifier</strong>(nn.Module):<br/>    <em class="mw">""" A Simple Feed Forward Neural Network. </em><br/><em class="mw">        A CNN can also be used for this problem </em><br/><em class="mw">    """</em><br/>    <strong class="nn iu">def</strong> __init__(self):<br/>        super().__init__()<br/>        <br/>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<br/>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<br/>        self.conv2_drop = nn.Dropout2d()<br/>        self.fc1 = nn.Linear(320, 50)<br/>        self.fc2 = nn.Linear(50, 10)<br/>    <br/>    <strong class="nn iu">def</strong> forward(self, x):<br/>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<br/>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<br/>        x = x.view(-1, 320)<br/>        x = F.relu(self.fc1(x))<br/>        x = F.dropout(x, training=self.training)<br/>        x = self.fc2(x)<br/>        <strong class="nn iu">return</strong> F.log_softmax(x)</span></pre><p id="5f55" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在定义训练和预测函数</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="0971" class="na la it nn b gy nr ns l nt nu"><strong class="nn iu">def</strong> train(model, trainloader, criterion, optimizer, epochs=10):<br/>    <em class="mw">""" This function trains a single Classifier model """</em><br/>    running_loss = 0<br/>    <strong class="nn iu">for</strong> e <strong class="nn iu">in</strong> range(epochs):<br/>        model.train()<br/>        <br/>        <strong class="nn iu">for</strong> images, labels <strong class="nn iu">in</strong> trainloader:<br/>            optimizer.zero_grad()<br/>            <br/>            output = model.forward(images)<br/>            loss = criterion(output, labels)<br/>            loss.backward()<br/>            optimizer.step()<br/>            <br/>            running_loss += loss.item()</span><span id="ee7a" class="na la it nn b gy nv ns l nt nu"><strong class="nn iu">def</strong> predict(model, dataloader):<br/>    <em class="mw">""" This function predicts labels for a dataset </em><br/><em class="mw">        given the model and dataloader as inputs. </em><br/><em class="mw">    """</em><br/>    outputs = torch.zeros(0, dtype=torch.long)<br/>    model.eval()<br/>    <br/>    <strong class="nn iu">for</strong> images, labels <strong class="nn iu">in</strong> dataloader:<br/>        output = model.forward(images)<br/>        ps = torch.argmax(torch.exp(output), dim=1)<br/>        outputs = torch.cat((outputs, ps))<br/>        <br/>    <strong class="nn iu">return</strong> outputs</span><span id="2f3c" class="na la it nn b gy nv ns l nt nu"><strong class="nn iu">def</strong> train_models(num_teachers):<br/>    <em class="mw">""" Trains *num_teacher* models (num_teachers being the number of teacher classifiers) """</em><br/>    models = []<br/>    <strong class="nn iu">for</strong> i <strong class="nn iu">in</strong> range(num_teachers):<br/>        model = Classifier()<br/>        criterion = nn.NLLLoss()<br/>        optimizer = optim.Adam(model.parameters(), lr=0.003)<br/>        train(model, teacher_loaders[i], criterion, optimizer)<br/>        models.append(model)<br/>    <strong class="nn iu">return</strong> models</span><span id="ad89" class="na la it nn b gy nv ns l nt nu">models = train_models(num_teachers)</span></pre><h2 id="dcde" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">步骤 3:通过组合教师模型的预测来生成聚合的教师和学生标签。</h2><p id="86a3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们需要选择ε值，我们首先定义差分隐私的正式定义</p><p id="e030" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个定义并没有<em class="mw">创造</em>差异隐私，相反，它是一个查询 M 提供多少隐私的度量。具体来说，它是在数据库(x)和并行数据库(y)上运行查询 M 之间的比较。正如您所记得的，并行数据库被定义为与一个完整的数据库(x)相同，只是删除了一个条目/人。</p><p id="348b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，该定义指出，对于所有并行数据库，数据库(x)上的查询和数据库(y)上的相同查询之间的最大距离将是 e^epsilon，但有时该约束不适用于概率增量。因此，这个定理被称为“ε-δ”差分隐私。</p><h2 id="110c" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">我们应该增加多少噪音？</h2><p id="01a1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">添加到查询输出中的噪声量是以下四个因素的函数:</p><ul class=""><li id="ab11" class="nw nx it lt b lu mn lx mo ma ny me nz mi oa mm ob oc od oe bi translated">噪声的类型(高斯/拉普拉斯)</li><li id="0c0e" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated">查询/函数的敏感性</li><li id="84db" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated">期望的ε</li><li id="8965" class="nw nx it lt b lu of lx og ma oh me oi mi oj mm ob oc od oe bi translated">所需的增量(δ)</li></ul><p id="9a95" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，对于我们添加的每种噪声，我们有不同的方法来计算添加多少，作为灵敏度、ε和δ的函数。我们将重点讨论拉普拉斯噪声。</p><p id="3ad5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">拉普拉斯噪声根据“比例”参数 b 增加/减少。我们基于以下公式选择“b”。</p><p id="851f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">b =敏感度(查询)/ε</p><p id="0ca2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">换句话说，如果我们将 b 设置为这个值，那么我们知道我们将有一个隐私泄露的&lt;= epsilon. Furthermore, the nice thing about Laplace is that it guarantees this with delta == 0. There are some tunings where we can have very low epsilon where delta is non-zero, but we’ll ignore them for now.</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="d6b1" class="na la it nn b gy nr ns l nt nu"><strong class="nn iu">import</strong> <strong class="nn iu">numpy</strong> <strong class="nn iu">as</strong> <strong class="nn iu">np</strong><br/><br/>epsilon = 0.2</span><span id="ca0c" class="na la it nn b gy nv ns l nt nu"><strong class="nn iu">def</strong> aggregated_teacher(models, dataloader, epsilon):<br/>    <em class="mw">""" Take predictions from individual teacher model and </em><br/><em class="mw">        creates the true labels for the student after adding </em><br/><em class="mw">        laplacian noise to them </em><br/><em class="mw">    """</em><br/>    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)<br/>    <strong class="nn iu">for</strong> i, model <strong class="nn iu">in</strong> enumerate(models):<br/>        results = predict(model, dataloader)<br/>        preds[i] = results<br/>    <br/>    labels = np.array([]).astype(int)<br/>    <strong class="nn iu">for</strong> image_preds <strong class="nn iu">in</strong> np.transpose(preds):<br/>        label_counts = np.bincount(image_preds, minlength=10)<br/>        beta = 1 / epsilon<br/><br/>        <strong class="nn iu">for</strong> i <strong class="nn iu">in</strong> range(len(label_counts)):<br/>            label_counts[i] += np.random.laplace(0, beta, 1)<br/><br/>        new_label = np.argmax(label_counts)<br/>        labels = np.append(labels, new_label)<br/>    <br/>    <strong class="nn iu">return</strong> preds.numpy(), labels</span><span id="4527" class="na la it nn b gy nv ns l nt nu">teacher_models = models<br/>preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)</span></pre><h2 id="34e7" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">Step 4: Create the Student model and train it using the labels generated in step 3.</h2><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4b57" class="na la it nn b gy nr ns l nt nu"><strong class="nn iu">def</strong> student_loader(student_train_loader, labels):<br/>    <strong class="nn iu">for</strong> i, (data, _) <strong class="nn iu">in</strong> enumerate(iter(student_train_loader)):<br/>        <strong class="nn iu">yield</strong> data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])</span><span id="82a7" class="na la it nn b gy nv ns l nt nu">student_model = Classifier()<br/>criterion = nn.NLLLoss()<br/>optimizer = optim.Adam(student_model.parameters(), lr=0.003)<br/>epochs = 10<br/>steps = 0<br/>running_loss = 0<br/><strong class="nn iu">for</strong> e <strong class="nn iu">in</strong> range(epochs):<br/>    student_model.train()<br/>    train_loader = student_loader(student_train_loader, student_labels)<br/>    <strong class="nn iu">for</strong> images, labels <strong class="nn iu">in</strong> train_loader:<br/>        steps += 1<br/>        <br/>        optimizer.zero_grad()<br/>        output = student_model.forward(images)<br/>        loss = criterion(output, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>        running_loss += loss.item()<br/>        <br/>        <strong class="nn iu">if</strong> steps % 50 == 0:<br/>            test_loss = 0<br/>            accuracy = 0<br/>            student_model.eval()<br/>            <strong class="nn iu">with</strong> torch.no_grad():<br/>                <strong class="nn iu">for</strong> images, labels <strong class="nn iu">in</strong> student_test_loader:<br/>                    log_ps = student_model(images)<br/>                    test_loss += criterion(log_ps, labels).item()<br/>                    <br/>                    <em class="mw"># Accuracy</em><br/>                    ps = torch.exp(log_ps)<br/>                    top_p, top_class = ps.topk(1, dim=1)<br/>                    equals = top_class == labels.view(*top_class.shape)<br/>                    accuracy += torch.mean(equals.type(torch.FloatTensor))<br/>            student_model.train()<br/>            print("Epoch: <strong class="nn iu">{}</strong>/<strong class="nn iu">{}</strong>.. ".format(e+1, epochs),<br/>                  "Training Loss: <strong class="nn iu">{:.3f}</strong>.. ".format(running_loss/len(student_train_loader)),<br/>                  "Test Loss: <strong class="nn iu">{:.3f}</strong>.. ".format(test_loss/len(student_test_loader)),<br/>                  "Test Accuracy: <strong class="nn iu">{:.3f}</strong>".format(accuracy/len(student_test_loader)))<br/>            running_loss = 0</span></pre><p id="365d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Here’s a snippet of the <strong class="lt iu">训练损失</strong>和<strong class="lt iu">精度</strong>实现。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="ef90" class="na la it nn b gy nr ns l nt nu">Epoch: 9/10..  Training Loss: 0.035..  Test Loss: 0.206..  Test Accuracy: 0.941<br/>Epoch: 9/10..  Training Loss: 0.034..  Test Loss: 0.196..  Test Accuracy: 0.949<br/>Epoch: 10/10..  Training Loss: 0.048..  Test Loss: 0.204..  Test Accuracy: 0.943<br/>Epoch: 10/10..  Training Loss: 0.046..  Test Loss: 0.203..  Test Accuracy: 0.943<br/>Epoch: 10/10..  Training Loss: 0.045..  Test Loss: 0.203..  Test Accuracy: 0.945<br/>Epoch: 10/10..  Training Loss: 0.049..  Test Loss: 0.207..  Test Accuracy: 0.946<br/>Epoch: 10/10..  Training Loss: 0.032..  Test Loss: 0.228..  Test Accuracy: 0.941<br/>Epoch: 10/10..  Training Loss: 0.030..  Test Loss: 0.252..  Test Accuracy: 0.939</span></pre><h2 id="bccc" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">步骤 5:让我们对聚合教师生成的学生标签执行 PATE 分析</h2><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="c528" class="na la it nn b gy nr ns l nt nu"><strong class="nn iu">from</strong> <strong class="nn iu">syft.frameworks.torch.differential_privacy</strong> <strong class="nn iu">import</strong> pate<br/><br/>data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)<br/>print("Data Independent Epsilon:", data_ind_eps)<br/>print("Data Dependent Epsilon:", data_dep_eps)</span></pre><p id="88b2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">输出:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="db34" class="na la it nn b gy nr ns l nt nu">Data Independent Epsilon: 1451.5129254649705<br/>Data Dependent Epsilon: 4.34002697554237</span></pre><p id="e6b2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">pate.perform_analysis 方法返回两个值——一个与数据无关的ε和一个与数据相关的ε。数据相关的 epsilon 是通过查看教师之间的一致程度而获得的 epsilon 值。在某种程度上，PATE 分析奖励用户构建彼此一致的教师模型，因为泄漏信息和跟踪个人信息变得更加困难。</p><h1 id="2c62" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="8638" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用 PATE 分析方法指导的学生-教师架构是一种将差分隐私引入深度学习模型的好方法。然而，差分隐私仍处于早期阶段，随着该领域研究的深入，将会开发出更复杂的方法来减少隐私-准确性之间的权衡，以及差分隐私仅在大型数据集上表现良好的缺点。</p><h1 id="bbba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="e0e7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] Dwork，c .和 Roth，A. <a class="ae ky" href="http://www.nowpublishers.com/article/Details/TCS-042" rel="noopener ugc nofollow" target="_blank">《差分隐私的算法基础》</a> (2014)，<em class="mw">理论计算机科学的基础和趋势</em>，<em class="mw">9</em>(3–4)，第 211–407 页。</p><p id="1d92" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2] Abadi，Martin 等，<a class="ae ky" href="https://dl.acm.org/citation.cfm?id=2978318" rel="noopener ugc nofollow" target="_blank">具有差分隐私的深度学习</a> (2016)，<em class="mw">2016 年 ACM SIGSAC 计算机与通信安全会议论文集</em>。ACM，2016。</p><p id="bfe4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3]图 1，由<a class="ae ky" href="https://unsplash.com/@euwars?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">法尔扎德·纳兹菲</a>在<a class="ae ky" href="https://unsplash.com/search/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p><p id="c0cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4]图 2，Nicolas Papernot 等人，<a class="ae ky" href="https://arxiv.org/abs/1802.08908" rel="noopener ugc nofollow" target="_blank">PATE</a>(2018)的可扩展私人学习，在 2018 年 ICLR 会议上作为会议论文发表</p></div></div>    
</body>
</html>