<html>
<head>
<title>Fake Face Generator Using DCGAN Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 DCGAN 模型的人脸生成器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fake-face-generator-using-dcgan-model-ae9322ccfd65?source=collection_archive---------9-----------------------#2019-11-11">https://towardsdatascience.com/fake-face-generator-using-dcgan-model-ae9322ccfd65?source=collection_archive---------9-----------------------#2019-11-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f187ff70ddf10488db0e2ebb03faf113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CwMr-TNpuwRON1VB"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@coolmilo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Camilo Jimenez</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="3d58" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">概观</h1><p id="4a2b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在下面的文章中，我们将在人脸数据集上定义和训练一个<strong class="lg iu">深度卷积生成对抗网络<em class="mc"> (DCGAN) </em> </strong>模型。该模型的主要目标是获得一个生成器网络来生成看起来尽可能真实的新的假人脸图像。</p><p id="9300" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">为此，我们将首先尝试理解<strong class="lg iu"> <em class="mc"> GAN </em> </strong> s 和<strong class="lg iu"> <em class="mc"> DCGAN </em> </strong> s 工作背后的直觉，然后结合这些知识来构建一个<strong class="lg iu">假脸生成器模型</strong>。到本文结束时，您将能够使用本文中的概念在任何给定的数据集上生成您的假样本。</p><h1 id="7efa" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="5124" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">以下文章分为两个部分:</p><ul class=""><li id="5091" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated"><strong class="lg iu">理论</strong> —理解<em class="mc">甘</em>和<em class="mc"> DCGAN </em>工作背后的直觉。</li><li id="856a" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated"><strong class="lg iu">实用</strong> —在 Pytorch 中实现假面生成器。</li></ul><p id="a42a" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">本文将涵盖这两个部分。所以让我们开始旅程吧…</p></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="ce6f" class="kg kh it bd ki kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld bi translated">理论</h1><h2 id="b77c" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">生成性对抗网络背后的直觉</h2><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/6c15873ffd174e136511db5ceceb1bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKanAdkjQbg1eDDMF2-4ow.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd nz">Generative Adversarial Network(GAN)</strong> architecture. Image from <a class="ae kf" href="https://sthalles.github.io/intro-to-gans/" rel="noopener ugc nofollow" target="_blank">https://sthalles.github.io/intro-to-gans/</a></figcaption></figure><ul class=""><li id="9487" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated"><strong class="lg iu">定义</strong></li></ul><p id="522a" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">总的来说，GAN 可以被定义为一个生成模型，让我们并行地生成一个完整的图像。与其他几种生成模型一起，<em class="mc"> GAN </em> s 使用由神经网络表示的可微分函数作为<em class="mc">生成器网络</em>。</p><ul class=""><li id="2fbc" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated"><strong class="lg iu">发电机网络</strong></li></ul><p id="1670" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">生成器网络将随机噪声作为输入，然后通过可微分函数<em class="mc">(神经网络)</em>运行噪声，以转换噪声并将其整形为具有与训练数据集中的图像相似的可识别结构。发生器的输出由输入随机噪声的选择决定。在几种不同的随机输入噪声上运行发生器网络会产生不同的真实输出图像。</p><p id="500a" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">生成器的最终目标是学习与训练数据集的分布类似的分布，以采样出真实的图像。要做到这一点，需要对发电机网络进行培训。与其他生成模型<em class="mc">相比，<em class="mc"> GAN </em> s 的训练过程非常不同(大多数生成模型通过调整参数来训练，以最大化生成器生成真实样本的概率。For-eg 变分自动编码器(VAE))。另一方面，GAN </em> s 使用第二个网络来训练发电机，称为<em class="mc">鉴别器网络</em>。</p><ul class=""><li id="d252" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated"><strong class="lg iu">鉴频器网络</strong></li></ul><p id="7cf7" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">鉴别器网络是一个基本的分类器网络，它输出图像真实的概率。因此，在训练过程中，鉴别器网络一半时间显示来自训练集的真实图像，另一半时间显示来自生成器的虚假图像。鉴别器的目标是为真实图像分配接近 1 的概率，为虚假图像分配接近 0 的概率。</p><p id="a016" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">另一方面，生成器尝试相反的方法，其目标是生成假图像，对于这些假图像，鉴别器将产生接近 1 <em class="mc">(认为它们是来自训练集的真实图像)</em>的概率。随着训练的进行，鉴别器将会更好地区分真假图像。所以为了骗过鉴别器，生成器将被迫改进以产生更真实的样本。所以我们可以说:</p><blockquote class="oa"><p id="a1a0" class="ob oc it bd od oe of og oh oi oj mb dk translated">GANs 可以被认为是一个两人(生产者和鉴别者)非合作博弈，每个参与者都希望最小化其成本函数。</p></blockquote><h2 id="0cef" class="ni kh it bd ki nj ok dn km nl ol dp kq lp om no ku lt on nq ky lx oo ns lc nt bi translated">gan 和 DCGANs 的区别</h2><p id="90d1" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc"> DCGAN </em>非常类似于<em class="mc"> GAN </em> s，但特别关注于使用深度卷积网络来代替普通<em class="mc"> GAN </em> s 中使用的全连接网络</p><p id="a954" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">卷积网络有助于发现图像中的深度相关性，也就是说，它们寻找空间相关性。这意味着 DCGAN 将是图像/视频数据的更好选择，而<em class="mc"> GAN </em> s 可以被认为是一个通用概念，在此基础上开发了<em class="mc"> DCGAN </em> s 和许多其他架构<em class="mc"> (CGAN、CycleGAN、StarGAN 和许多其他架构)</em>。</p><blockquote class="op oq or"><p id="10f1" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">在本文中，我们主要研究图像数据，这意味着与普通 GAN 相比，DCGAN 是更好的选择。所以从现在开始，我们将主要关注 DCGANs。</p></blockquote><h2 id="2fe9" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">训练 DCGANs 的一些技巧</h2><blockquote class="op oq or"><p id="b432" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">所有的训练技巧同样适用于<em class="it"> </em>香草甘<em class="it"> s </em>以及<em class="it">。</em></p></blockquote><ul class=""><li id="22f7" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">确保鉴别器和生成器至少有一个隐藏层。这确保了两个模型都有一个<strong class="lg iu"> <em class="mc">通用近似属性</em> </strong>。</li></ul><blockquote class="op oq or"><p id="9f78" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated"><strong class="lg iu">普适逼近性质<em class="it"> </em> </strong>陈述了一个单隐层包含有限个隐单元的前馈网络，在给定足够多隐单元的情况下，可以逼近任意概率分布。</p></blockquote><ul class=""><li id="d3a8" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">对于隐藏单元，许多激活功能可以工作，但泄漏 ReLUs 是最受欢迎的。渗漏的 ReLUs 确保梯度流过整个建筑。这对 DCGAN 非常重要，因为发生器能够学习的唯一方法是从鉴频器接收梯度。</li><li id="48dd" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">对于发电机网络的输出，最流行的激活函数之一是正切双曲线激活函数<em class="mc">(基于</em> <a class="ae kf" href="https://video.udacity-data.com/topher/2018/November/5bea0c6a_improved-training-techniques/improved-training-techniques.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mc">对 GANs </em> </a> <em class="mc">论文的改进训练技术)</em>。</li><li id="6d76" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">由于鉴别器是二元分类器，我们将使用 Sigmoid 激活函数来获得最终概率。</li></ul><p id="d9a0" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">到目前为止，我们已经讨论了工作直觉和一些训练<em class="mc"> GAN </em> s/ <em class="mc"> DCGAN </em> s 的技巧和诀窍，但是仍然有许多问题没有得到解答。其中一些是:</p><blockquote class="op oq or"><p id="1f4c" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">选择哪个优化器？成本函数是如何定义的？一个网络需要训练多久？以及许多其他内容，将在<strong class="lg iu">实用</strong>部分中介绍。</p></blockquote></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="98cd" class="kg kh it bd ki kj nd kl km kn ne kp kq kr nf kt ku kv ng kx ky kz nh lb lc ld bi translated">实际的</h1><p id="43d3" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">实施部分被分解为从<strong class="lg iu">加载数据到定义和训练敌对网络</strong>的一系列任务。在本节结束时，您将能够可视化您训练过的生成器的结果，以了解它是如何执行的；你生成的样本应该看起来像带有少量噪点的真实人脸。</p><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ov"><img src="../Images/1ce21a20a5009662ea2a5dc82463a97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8AIzUpBcNzpi-JN3gLWtg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd nz">DCGAN Architecture</strong>. Image from <a class="ae kf" href="https://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html" rel="noopener ugc nofollow" target="_blank">https://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html</a></figcaption></figure><h2 id="b0b4" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">(1)获取数据</h2><p id="935a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你将使用<a class="ae kf" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank">名人面孔属性数据集<em class="mc"> (CelebA) </em> </a>来训练你的敌对网络。与 MNIST 相比，该数据是一个更复杂的数据集。所以，我们需要定义一个更深的网络<em class="mc"> (DCGAN) </em>来产生好的结果。我建议您使用 GPU 进行培训。</p><h2 id="ae09" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">(2)准备数据</h2><p id="9e31" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于本文的主要目标是构建一个<em class="mc"> DCGAN </em>模型，因此我们将使用一个预处理过的数据集，而不是自己进行预处理。你可以从<a class="ae kf" href="https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip" rel="noopener ugc nofollow" target="_blank">这里</a>下载<strong class="lg iu"> CelebA </strong>数据集的较小子集。如果您有兴趣进行预处理，请执行以下操作:</p><ul class=""><li id="005e" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">裁剪图像以移除不包括面部的部分。</li><li id="4674" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">将它们调整为 64x64x3 的数字图像。</li></ul><p id="507e" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">现在，我们将创建一个<em class="mc">数据加载器</em>来批量访问图像。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="1df6" class="ni kh it ox b gy pb pc l pd pe">def get_dataloader(batch_size, image_size, data_dir='train/'):<br/>    """<br/>    Batch the neural network data using DataLoader<br/>    :param batch_size: The size of each batch; the number of images in a batch<br/>    :param img_size: The square size of the image data (x, y)<br/>    :param data_dir: Directory where image data is located<br/>    :return: DataLoader with batched data<br/>    """<br/>    transform = transforms.Compose([transforms.Resize(image_size),transforms.CenterCrop(image_size),transforms.ToTensor()])<br/>  <br/>    dataset = datasets.ImageFolder(data_dir,transform = transform)<br/>    <br/>    dataloader = torch.utils.data.DataLoader(dataset = dataset,batch_size = batch_size,shuffle = True)<br/>    return dataloader</span><span id="be20" class="ni kh it ox b gy pf pc l pd pe"># Define function hyperparameters<br/>batch_size = 256<br/>img_size = 32</span><span id="b339" class="ni kh it ox b gy pf pc l pd pe"># Call your function and get a dataloader<br/>celeba_train_loader = get_dataloader(batch_size, img_size)</span></pre><p id="8045" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated"><em class="mc">数据加载器</em>超参数:</p><ul class=""><li id="ee93" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">您可以决定任何合理的<em class="mc"> batch_size </em>参数。</li><li id="25fd" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">但是，你的<em class="mc"> image_size </em>必须是 32。将数据调整到较小的大小将有助于更快的训练，同时仍然可以创建令人信服的人脸图像。</li></ul><p id="0620" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">接下来，我们将编写一些代码来获得数据集的可视化表示。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="53e3" class="ni kh it ox b gy pb pc l pd pe">def imshow(img):<br/>    npimg = img.numpy()<br/>    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><span id="b765" class="ni kh it ox b gy pf pc l pd pe"># obtain one batch of training images<br/>dataiter = iter(celeba_train_loader)<br/>images, _ = dataiter.next() # _ for no labels</span><span id="9bd9" class="ni kh it ox b gy pf pc l pd pe"># plot the images in the batch, along with the corresponding labels<br/>fig = plt.figure(figsize=(20, 4))<br/>plot_size=20<br/>for idx in np.arange(plot_size):<br/>    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])<br/>    imshow(images[idx])</span></pre><p id="33c4" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">请记住将<em class="mc">张量</em>图像转换成<em class="mc"> NumPy </em>类型，并转置尺寸以正确显示基于上述代码<em class="mc">的图像(在 Dataloader 中，我们将图像转换成张量)</em>。运行这段代码来获得数据集的可视化。</p><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/c29f4ca43e0d520fe9c587523d049ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRNHPpJharyVGwWrHsC0Kg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Images Centered around faces</figcaption></figure><p id="227e" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">现在，在开始下一节<em class="mc">(定义模型)</em>之前，我们将编写一个函数来将图像数据缩放到-1 到 1 的像素范围，我们将在训练时使用该函数。这样做的原因是 tanh 激活的生成器的输出将包含范围从-1 到 1 的像素值，因此，我们需要将我们的训练图像重新缩放到范围从-1 到 1 <em class="mc">(现在，它们在范围 0–1)</em>。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="e4d0" class="ni kh it ox b gy pb pc l pd pe">def scale(x, feature_range=(-1, 1)):<br/>    ''' Scale takes in an image x and returns that image, scaled<br/>       with a feature_range of pixel values from -1 to 1. <br/>       This function assumes that the input x is already scaled from 0-1.'''<br/>    # assume x is scaled to (0, 1)<br/>    # scale to feature_range and return scaled x<br/>    min, max = feature_range<br/>    x = x*(max-min) + min<br/>    return x</span></pre><h1 id="ec95" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">(3)定义模型</h1><p id="982f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一个<em class="mc"> GAN </em>由两个对抗网络组成，一个鉴别器和一个生成器。因此，在这一节中，我们将为它们定义架构。</p><h2 id="4c9e" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">鉴别器</h2><p id="c35b" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是一个卷积分类器，只是没有任何<em class="mc"> MaxpPooling </em>层。这是鉴别器网络的代码。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="6213" class="ni kh it ox b gy pb pc l pd pe">def conv(input_c,output,kernel_size,stride = 2,padding  = 1, batch_norm = True):<br/>    layers =[]<br/>    con = nn.Conv2d(input_c,output,kernel_size,stride,padding,bias = False)<br/>    layers.append(con)<br/>    <br/>    if batch_norm:<br/>        layers.append(nn.BatchNorm2d(output))<br/>    <br/>    return nn.Sequential(*layers)</span><span id="1e57" class="ni kh it ox b gy pf pc l pd pe">class Discriminator(nn.Module):</span><span id="a3ca" class="ni kh it ox b gy pf pc l pd pe">def __init__(self, conv_dim):<br/>        """<br/>        Initialize the Discriminator Module<br/>        :param conv_dim: The depth of the first convolutional layer<br/>        """<br/>        #complete init function</span><span id="afbd" class="ni kh it ox b gy pf pc l pd pe">super(Discriminator, self).__init__()<br/>        self.conv_dim = conv_dim<br/>        self.layer_1 = conv(3,conv_dim,4,batch_norm = False) #16<br/>        self.layer_2 = conv(conv_dim,conv_dim*2,4) #8<br/>        self.layer_3 = conv(conv_dim*2,conv_dim*4,4) #4<br/>        self.fc = nn.Linear(conv_dim*4*4*4,1)</span><span id="1d87" class="ni kh it ox b gy pf pc l pd pe">def forward(self, x):<br/>        """<br/>        Forward propagation of the neural network<br/>        :param x: The input to the neural network     <br/>        :return: Discriminator logits; the output of the neural network<br/>        """<br/>        # define feedforward behavior<br/>        x = F.leaky_relu(self.layer_1(x))<br/>        x = F.leaky_relu(self.layer_2(x))<br/>        x = F.leaky_relu(self.layer_3(x))<br/>        x = x.view(-1,self.conv_dim*4*4*4)<br/>        x = self.fc(x)<br/>        return x</span></pre><p id="77cc" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated"><strong class="lg iu">解释</strong></p><ul class=""><li id="8b47" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">以下架构由三个卷积层和一个最终全连接层组成，输出单个 logit。这个逻辑定义了图像是否真实。</li><li id="be0c" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">除了第一个卷积层，每个卷积层后面都有一个<em class="mc">批量归一化(在 conv 辅助函数中定义)</em>。</li><li id="8de8" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">对于隐藏单元，我们使用了<em class="mc">泄漏 ReLU </em>激活功能，如<strong class="lg iu">理论</strong>部分所述。</li><li id="ceff" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">在每个卷积层之后，高度和宽度变成一半。例如，在第一次卷积后，32×32 的图像将被调整为 16×16，以此类推。</li></ul><blockquote class="op oq or"><p id="d439" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">可以使用以下公式计算输出尺寸:</p></blockquote><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/cdeb9d77358492f036ce485521319274.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*ZaXyt3s1ux3V_BBGXr5T1Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Formula-1</figcaption></figure><blockquote class="op oq or"><p id="e448" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">其中<em class="it"> O </em>为输出高度/长度，<em class="it"> W </em>为输入高度/长度，<em class="it"> K </em>为滤波器尺寸，<em class="it"> P </em>为填充，<em class="it"> S </em>为步幅。</p></blockquote><ul class=""><li id="fa09" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">每次卷积后特征图的数量基于参数<em class="mc">conv _ 尺寸(在我的实现中 conv _ 尺寸= 64) </em>。</li></ul><p id="7a8d" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">在这个模型定义中，我们没有对最终输出 logit 应用<em class="mc"> Sigmoid </em>激活函数。这是因为我们损失函数的选择。这里我们将使用<em class="mc"> BCEWithLogitLoss，</em>而不是使用普通的<em class="mc"> BCE(二元交叉熵损失)，它被认为是<em class="mc"> BCE </em>的数值稳定版本。<em class="mc"> BCEWithLogitLoss </em>被定义为首先在 logit 上应用 Sigmoid 激活函数，然后计算损失，与<em class="mc"> BCE </em>不同。你可以在这里阅读更多关于这些损失函数<a class="ae kf" href="https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586" rel="noopener ugc nofollow" target="_blank">的内容。</a></em></p><h2 id="5214" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">发电机</h2><p id="4f75" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">生成器应该对输入进行上采样，并生成与我们的训练数据 32X32X3 相同大小的新图像。为此，我们将使用转置卷积层。这是发电机网络的代码。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="e55e" class="ni kh it ox b gy pb pc l pd pe">def deconv(input_c,output,kernel_size,stride = 2, padding =1, batch_norm = True):<br/>    layers = []<br/>    decon = nn.ConvTranspose2d(input_c,output,kernel_size,stride,padding,bias = False)<br/>    layers.append(decon)<br/>    <br/>    if batch_norm:<br/>        layers.append(nn.BatchNorm2d(output))<br/>    return nn.Sequential(*layers)</span><span id="680b" class="ni kh it ox b gy pf pc l pd pe">class Generator(nn.Module):<br/>    <br/>    def __init__(self, z_size, conv_dim):<br/>        """<br/>        Initialize the Generator Module<br/>        :param z_size: The length of the input latent vector, z<br/>        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer<br/>        """<br/>        super(Generator, self).__init__()<br/>        # complete init function<br/>        self.conv_dim = conv_dim<br/>        self.fc = nn.Linear(z_size,conv_dim*8*2*2)<br/>        self.layer_1 = deconv(conv_dim*8,conv_dim*4,4) #4<br/>        self.layer_2 = deconv(conv_dim*4,conv_dim*2,4) #8<br/>        self.layer_3 = deconv(conv_dim*2,conv_dim,4) #16<br/>        self.layer_4 = deconv(conv_dim,3,4,batch_norm = False) #32<br/>        <br/>        <br/>    def forward(self, x):<br/>        """<br/>        Forward propagation of the neural network<br/>        :param x: The input to the neural network     <br/>        :return: A 32x32x3 Tensor image as output<br/>        """<br/>        # define feedforward behavior<br/>        x = self.fc(x)<br/>        x = x.view(-1,self.conv_dim*8,2,2) #(batch_size,depth,width,height)<br/>        x = F.relu(self.layer_1(x))<br/>        x = F.relu(self.layer_2(x))<br/>        x = F.relu(self.layer_3(x))<br/>        x = torch.tanh(self.layer_4(x))<br/>        return x</span></pre><p id="3c01" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated"><strong class="lg iu">解说</strong></p><ul class=""><li id="68ed" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">以下架构由一个全连接层和四个转置卷积层组成。该架构被定义为使得在第四转置卷积层之后的输出产生尺寸为 32×32×3<em class="mc">(来自训练数据集的图像大小)的图像。</em></li><li id="d53d" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">发生器的输入是一定长度的矢量<em class="mc"> z_size(z_size 是噪声矢量)</em>。</li><li id="c982" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">除了最后一层，每个转置卷积层后面都有一个<em class="mc">批量归一化(在 deconv 辅助函数中定义)</em>。</li><li id="d888" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">对于隐藏单元，我们使用了<em class="mc"> ReLU </em>激活功能。</li><li id="5b68" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">在每个转置卷积层之后，高度和宽度变成两倍。例如，在第一次转置卷积后，2X2 图像将被调整大小为 4X4，依此类推。</li></ul><blockquote class="op oq or"><p id="c22c" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">可以使用以下公式计算:</p><p id="2650" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated"><code class="fe pi pj pk ox b"><em class="it"># Padding==Same:<br/>H = H1 * stride</em></code></p><p id="7805" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated"><code class="fe pi pj pk ox b"><em class="it"># Padding==Valid<br/>H = (H1-1) * stride + HF</em></code></p><p id="625f" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">其中 H =输出尺寸，H1 =输入尺寸，HF =滤波器尺寸。</p></blockquote><ul class=""><li id="0ee7" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">在每个转置卷积之后的特征映射的数量基于参数<em class="mc"> conv_dim(在我的实现中 conv_dim = 64) </em>。</li></ul><h1 id="bf7d" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">(4)初始化网络的权重</h1><p id="bfb0" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了帮助模型收敛，我根据<a class="ae kf" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">原文<em class="mc"> DCGAN </em>论文</a>对模型中卷积层和线性层的权重进行了初始化，该论文称:所有权重都是从以零为中心的正态分布初始化的，标准差为 0.02。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="d9ba" class="ni kh it ox b gy pb pc l pd pe">def weights_init_normal(m):<br/>    """<br/>    Applies initial weights to certain layers in a model .<br/>    The weights are taken from a normal distribution <br/>    with mean = 0, std dev = 0.02.<br/>    :param m: A module or layer in a network    <br/>    """<br/>    # classname will be something like:<br/>    # `Conv`, `BatchNorm2d`, `Linear`, etc.<br/>    classname = m.__class__.__name__<br/>    <br/>    if hasattr(m,'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):<br/>        <br/>        m.weight.data.normal_(0.0,0.02)<br/>    <br/>        if hasattr(m,'bias') and m.bias is not None:<br/>            m.bias.data.zero_()</span></pre><ul class=""><li id="3f8a" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">这会将权重初始化为正态分布，以 0 为中心，标准偏差为 0.02。</li><li id="425a" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">如果存在偏差项，可以不考虑或设置为 0。</li></ul><h1 id="f197" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">(5)建立完整的网络</h1><p id="1829" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">定义您的模型的超参数，并从在<strong class="lg iu">定义模型</strong>部分定义的类中实例化鉴别器和生成器。这是代码。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="a569" class="ni kh it ox b gy pb pc l pd pe">def build_network(d_conv_dim, g_conv_dim, z_size):<br/>    # define discriminator and generator<br/>    D = Discriminator(d_conv_dim)<br/>    G = Generator(z_size=z_size, conv_dim=g_conv_dim)</span><span id="7e35" class="ni kh it ox b gy pf pc l pd pe"># initialize model weights<br/>    D.apply(weights_init_normal)<br/>    G.apply(weights_init_normal)</span><span id="497b" class="ni kh it ox b gy pf pc l pd pe">print(D)<br/>    print()<br/>    print(G)<br/>    <br/>    return D, G<br/>   <br/># Define model hyperparams<br/>d_conv_dim = 64<br/>g_conv_dim = 64<br/>z_size = 100</span><span id="37ff" class="ni kh it ox b gy pf pc l pd pe">D, G = build_network(d_conv_dim, g_conv_dim, z_size)</span></pre><p id="6742" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">当您运行上面的代码时，您会得到下面的输出。它还描述了鉴别器和生成器模型的模型架构。</p><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pl"><img src="../Images/654bff5b92a9aaa9ce5b2ab08fda04bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NdHLb4iAMJkjblS1DI2FUw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Instantiated<strong class="bd nz"> Discriminator</strong> and <strong class="bd nz">Generator</strong> models</figcaption></figure><h1 id="d38f" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">(6)培训过程</h1><blockquote class="op oq or"><p id="e085" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">训练过程包括定义损失函数、选择优化器以及最后训练模型。</p></blockquote><h2 id="b323" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">鉴频器和发电机损耗</h2><p id="b7fb" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="lg iu">鉴频器损耗</strong></p><ul class=""><li id="9ee2" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">对于鉴别器，总损失是<em class="mc">(d _ real _ loss+d _ fake _ loss)</em>之和，其中<em class="mc"> d_real_loss </em>是从训练数据中的图像上获得的损失，<em class="mc"> d_fake_loss </em>是从生成器网络生成的图像上获得的损失。对于-例如</li></ul><blockquote class="op oq or"><p id="e70b" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">z —噪声矢量</p><p id="8c37" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">i —来自训练集的图像</p><p id="1f2e" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">g(z)-生成的图像</p><p id="9f2a" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">D(G(z)) —生成图像上的鉴别器输出</p><p id="dab2" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">d(I)-训练数据集图像上的鉴别器输出</p><p id="bb08" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">损失=真实损失(D(i)) +虚假损失(D(G(z)))</p></blockquote><ul class=""><li id="9a3c" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">请记住，我们希望鉴别器为真实图像输出 1，为虚假图像输出 0，因此我们需要设置损耗来反映<em class="mc">(在阅读下面的代码时请记住这一行)</em>。</li></ul><p id="36f3" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated"><strong class="lg iu">发电机损耗</strong></p><ul class=""><li id="80e1" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">只有在标签翻转的情况下，发电机损耗才会看起来相似。生成器的目标是让鉴别器认为它生成的图像是真实的 T4。对于-例如</li></ul><blockquote class="op oq or"><p id="4417" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">z —噪声矢量</p><p id="af02" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">g(z)-生成的图像</p><p id="a03b" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">D(G(z)) —生成图像上的鉴别器输出</p><p id="20bd" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">Loss = real_loss(D(G(z))。</p></blockquote><p id="657d" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">下面是<em class="mc">真实损失</em>和<em class="mc">虚假损失</em>的代码</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="2a27" class="ni kh it ox b gy pb pc l pd pe">def real_loss(D_out):<br/>    '''Calculates how close discriminator outputs are to being real.<br/>       param, D_out: discriminator logits<br/>       return: real loss'''<br/>    batch_size = D_out.size(0)<br/>    labels = torch.ones(batch_size)<br/>    if train_on_gpu:<br/>        labels = labels.cuda()<br/>    criterion = nn.BCEWithLogitsLoss()<br/>    loss = criterion(D_out.squeeze(),labels)<br/>    return loss</span><span id="6139" class="ni kh it ox b gy pf pc l pd pe">def fake_loss(D_out):<br/>    '''Calculates how close discriminator outputs are to being fake.<br/>       param, D_out: discriminator logits<br/>       return: fake loss'''<br/>    batch_size = D_out.size(0)<br/>    labels = torch.zeros(batch_size)<br/>    if train_on_gpu:<br/>        labels = labels.cuda()<br/>    criterion =  nn.BCEWithLogitsLoss()<br/>    loss = criterion(D_out.squeeze(),labels)<br/>    return loss</span></pre><h2 id="c239" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">优化者</h2><p id="e675" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于 GAN，我们定义了两个优化器，一个用于发生器，另一个用于鉴别器。这个想法是同时运行它们以不断改善两个网络。在这个实现中，我在两种情况下都使用了<a class="ae kf" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mc"> Adam </em>优化器</a>。要了解更多关于不同优化器的信息，请参考此<a class="ae kf" href="https://d2l.ai/chapter_optimization/index.html" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="d437" class="ni kh it ox b gy pb pc l pd pe"># Create optimizers for the discriminator D and generator G<br/>d_optimizer = optim.Adam(D.parameters(),lr = .0002, betas = [0.5,0.999])<br/>g_optimizer = optim.Adam(G.parameters(),lr = .0002, betas = [0.5,0.999])</span></pre><p id="914c" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated"><em class="mc">学习率(lr) </em>和<em class="mc">β值</em>基于原始<a class="ae kf" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank"> DCGAN 纸</a>。</p><h2 id="03cc" class="ni kh it bd ki nj nk dn km nl nm dp kq lp nn no ku lt np nq ky lx nr ns lc nt bi translated">培养</h2><p id="5393" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">训练将包括交替训练鉴别器和发生器。我们将使用之前定义的<em class="mc">真实损耗</em>和<em class="mc">虚假损耗</em>函数来帮助我们计算鉴频器和发电机损耗。</p><ul class=""><li id="0a52" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated">你应该通过交替使用真实和虚假的图像来训练鉴别者</li><li id="7a05" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated">然后是发生器，它试图欺骗鉴别器，应该有一个相反的损失函数</li></ul><p id="05e6" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">这是训练的代码。</p><pre class="nv nw nx ny gt ow ox oy oz aw pa bi"><span id="6e3a" class="ni kh it ox b gy pb pc l pd pe">def train(D, G, n_epochs, print_every=50):<br/>    '''Trains adversarial networks for some number of epochs<br/>       param, D: the discriminator network<br/>       param, G: the generator network<br/>       param, n_epochs: number of epochs to train for<br/>       param, print_every: when to print and record the models' losses<br/>       return: D and G losses'''<br/>    <br/>    # move models to GPU<br/>    if train_on_gpu:<br/>        D.cuda()<br/>        G.cuda()</span><span id="3792" class="ni kh it ox b gy pf pc l pd pe"># keep track of loss and generated, "fake" samples<br/>    samples = []<br/>    losses = []</span><span id="5807" class="ni kh it ox b gy pf pc l pd pe"># Get some fixed data for sampling. These are images that are held<br/>    # constant throughout training, and allow us to inspect the model's performance<br/>    sample_size=16<br/>    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))<br/>    fixed_z = torch.from_numpy(fixed_z).float()<br/>    # move z to GPU if available<br/>    if train_on_gpu:<br/>        fixed_z = fixed_z.cuda()</span><span id="bd00" class="ni kh it ox b gy pf pc l pd pe"># epoch training loop<br/>    for epoch in range(n_epochs):</span><span id="229f" class="ni kh it ox b gy pf pc l pd pe"># batch training loop<br/>        for batch_i, (real_images, _) in enumerate(celeba_train_loader):</span><span id="87d9" class="ni kh it ox b gy pf pc l pd pe">batch_size = real_images.size(0)<br/>            real_images = scale(real_images)<br/>            if train_on_gpu:<br/>                real_images = real_images.cuda()<br/>          <br/>            # 1. Train the discriminator on real and fake ima.ges<br/>            d_optimizer.zero_grad()<br/>            d_out_real = D(real_images)<br/>            z = np.random.uniform(-1,1,size = (batch_size,z_size))<br/>            z = torch.from_numpy(z).float()<br/>            if train_on_gpu:<br/>                z = z.cuda()<br/>            d_loss = real_loss(d_out_real) + fake_loss(D(G(z)))<br/>            d_loss.backward()<br/>            d_optimizer.step()<br/>            # 2. Train the generator with an adversarial loss<br/>            G.train()<br/>            g_optimizer.zero_grad()<br/>            z = np.random.uniform(-1,1,size = (batch_size,z_size))<br/>            z = torch.from_numpy(z).float()<br/>            if train_on_gpu:<br/>                z = z.cuda()<br/>            g_loss = real_loss(D(G(z)))<br/>            g_loss.backward()<br/>            g_optimizer.step()<br/>            <br/>            # Print some loss stats<br/>            if batch_i % print_every == 0:<br/>                # append discriminator loss and generator loss<br/>                losses.append((d_loss.item(), g_loss.item()))<br/>                # print discriminator and generator loss<br/>                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(<br/>                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))</span><span id="bb51" class="ni kh it ox b gy pf pc l pd pe">## AFTER EACH EPOCH##    <br/>        # this code assumes your generator is named G, feel free to change the name<br/>        # generate and save sample, fake images<br/>        G.eval() # for generating samples<br/>        samples_z = G(fixed_z)<br/>        samples.append(samples_z)<br/>        G.train() # back to training mode</span><span id="24af" class="ni kh it ox b gy pf pc l pd pe"># Save training generator samples<br/>    with open('train_samples.pkl', 'wb') as f:<br/>        pkl.dump(samples, f)<br/>    <br/>    # finally return losses<br/>    return losses<br/>    <br/>    <br/># set number of epochs <br/>n_epochs = 40</span><span id="c0fe" class="ni kh it ox b gy pf pc l pd pe"># call training function<br/>losses = train(D, G, n_epochs=n_epochs)</span></pre><blockquote class="op oq or"><p id="ee01" class="le lf mc lg b lh md lj lk ll me ln lo os mf lr ls ot mg lv lw ou mh lz ma mb im bi translated">使用 GPU 进行了 40 多次训练，这就是为什么我必须将我的模型和输入从 CPU 转移到 GPU。</p></blockquote><h1 id="1ad4" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">(6)结果</h1><ul class=""><li id="fa20" class="mi mj it lg b lh li ll lm lp pm lt pn lx po mb mn mo mp mq bi translated">以下是在每个时期之后记录的发生器和鉴别器的训练损失的曲线图。</li></ul><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/0226883362b5ba4ef792ba537a0c5deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*PFKb7uOhWTYROQCwYnmpww.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Training Loss for<strong class="bd nz"> Discriminator</strong> and <strong class="bd nz">Generator</strong></figcaption></figure><p id="a526" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">发生器训练损失的高波动是因为发生器网络的输入是一批随机噪声向量<em class="mc">(每个 z_size) </em>，每个随机噪声向量从(-1，1)的均匀分布中采样以生成每个时期的新图像。</p><p id="1eba" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">在鉴别器图中，我们可以观察到训练损失<em class="mc">(x 轴上大约 50)</em>上升，然后逐渐下降，直到结束，这是因为发生器已经开始生成一些真实的图像，欺骗了鉴别器，导致误差增加。但随着训练的进行，鉴别器在区分真假图像方面变得越来越好，导致训练错误逐渐减少。</p><ul class=""><li id="bf87" class="mi mj it lg b lh md ll me lp mk lt ml lx mm mb mn mo mp mq bi translated"><em class="mc"> 40 个历元</em>后生成的样本。</li></ul><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pq"><img src="../Images/6d6a5fe2eee3d2150b5b65a08a46f705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUJzbGDKtouCzLKfup-zUw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Generated fake images</figcaption></figure><p id="4140" class="pw-post-body-paragraph le lf it lg b lh md lj lk ll me ln lo lp mf lr ls lt mg lv lw lx mh lz ma mb im bi translated">我们的模型能够生成看起来尽可能真实的假人脸的新图像。我们还可以观察到，所有图像的阴影都变浅了，甚至棕色的脸也变浅了。这是因为<strong class="lg iu"> CelebA </strong>数据集有偏差；它由大部分是白人的“名人”面孔组成。也就是说，DCGAN 成功地从纯粹的噪声中生成了接近真实的图像。</p></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><figure class="nv nw nx ny gt ju gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/af58343d88a2491d1702141e2b629ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*5ionWHIg4lj0-VrBvAqaKw.jpeg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image from <a class="ae kf" href="https://memeshappen.com/meme/success-kid-original/finally-we-did-it-6891/5" rel="noopener ugc nofollow" target="_blank">https://memeshappen.com/meme/success-kid-original/finally-we-did-it-6891/5</a></figcaption></figure><h1 id="1807" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">参考</h1><ul class=""><li id="e4ae" class="mi mj it lg b lh li ll lm lp pm lt pn lx po mb mn mo mp mq bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank"> DCGAN 原纸</a></li><li id="33ac" class="mi mj it lg b lh mr ll ms lp mt lt mu lx mv mb mn mo mp mq bi translated"><a class="ae kf" href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank"> Udacity 深度学习纳米学位课程</a></li></ul><blockquote class="oa"><p id="bc34" class="ob oc it bd od oe ps pt pu pv pw mb dk translated">查看我关于这篇文章的<a class="ae kf" href="https://github.com/vjrahil/Face-Generator" rel="noopener ugc nofollow" target="_blank"> Github 报告</a>。</p></blockquote></div></div>    
</body>
</html>