<html>
<head>
<title>Predicting future medical diagnoses with RNNs using Fast AI API from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始使用快速 AI API 通过 RNNs 预测未来的医疗诊断</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-future-medical-diagnoses-with-rnns-using-fast-ai-api-from-scratch-ecf78aaf56a2?source=collection_archive---------6-----------------------#2019-04-21">https://towardsdatascience.com/predicting-future-medical-diagnoses-with-rnns-using-fast-ai-api-from-scratch-ecf78aaf56a2?source=collection_archive---------6-----------------------#2019-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ed2d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">全面 pytorch 实现医生 AI 论文使用电子健康档案</em></h2></div><p id="5dfb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在本教程的第一篇<a class="ae lc" href="https://sparalic.github.io/post/gated-recurrent-units-explained-using-matrices-part-1/" rel="noopener ugc nofollow" target="_blank">第一部分</a>中，我们创建了 Edward Choi 等人的<a class="ae lc" href="https://arxiv.org/abs/1511.05942" rel="noopener ugc nofollow" target="_blank">医生 AI:通过递归神经网络预测临床事件论文(2016) </a>的粗略模板。在本教程中，我们使用 Fast.ai 自下而上方法对其进行了进一步处理。该代码功能齐全，有关数据处理的详细信息可在<a class="ae lc" href="https://sparalic.github.io/post/gated-recurrent-units-explained-using-matrices-part-1/" rel="noopener ugc nofollow" target="_blank">第一部分</a>中获取。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/5c3960d7d3a9af296ae9f0fda115df3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*u011VpREMJOMNS0PSCtbpg.png"/></div></figure><p id="e131" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">详细代码:<a class="ae lc" href="https://github.com/sparalic/Predicting-future-medical-diagnoses-with-RNNs-using-Fast-AI-API-from-scratch" rel="noopener ugc nofollow" target="_blank"> Github </a></p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h2 id="0295" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">加载数据</h2><h2 id="8fb9" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">关于数据集:</h2><p id="da50" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">本研究将利用<a class="ae lc" href="https://mimic.physionet.org/" rel="noopener ugc nofollow" target="_blank"> MIMIC III </a>电子健康记录(EHR)数据集，该数据集包含 38，645 名成人和 7，875 名新生儿的 58，000 多份住院记录。该数据集是从 2001 年 6 月至 2012 年 10 月在贝斯以色列女执事医疗中心的去识别重症监护病房住院病人的集合。在<a class="ae lc" href="https://sparalic.github.io/post/using-electronic-health-records-to-predict-future-diagnosis-codes-with-gated-recurrent-units/" rel="noopener ugc nofollow" target="_blank">第一部分</a>中可以找到所使用的数据预处理步骤的详细演练。</p><p id="19d5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">数据预处理数据集将被加载，并按<code class="fe mq mr ms mt b">75%:15%:10%</code>比率分成训练、测试和验证集。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Data loading function</figcaption></figure><h2 id="ddb1" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">填充序列:处理可变长度序列</h2><p id="47db" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">使用在第一部分中创建的人工 EHR 数据，我们将序列填充到每个小批量中最长序列的长度。为了帮助更深入地解释这一点，让我们看一看在第一部分中创建的<code class="fe mq mr ms mt b">Artificial EHR data</code>。</p><h2 id="be28" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">使用人工生成的 EHR 数据进行详细解释</h2><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="4f51" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里，您可以看到我们有一个包含两个列表数组，每个列表代表一个独特的患者。现在，在每个列表中有一系列列表，每个列表代表一次独特的访问。最后，编码的数字代表每次就诊时分配的诊断代码。值得注意的是，鉴于每位患者病情的独特性，指定的就诊和诊断代码都有<code class="fe mq mr ms mt b">variable length</code>序列。因为 EHR 数据本质上是纵向，我们通常对了解患者的风险或随时间的进展感兴趣。当使用表格数据处理时，这些嵌套的依赖于时间的<code class="fe mq mr ms mt b">variable length</code>序列会很快变得复杂。回想第一部分的下图，详细描述了每次就诊日期与就诊期间指定的诊断代码之间的映射关系。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/cd9c0e8e2318d724225b51db292b73ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*poQbXNnKQlEPZq7q-ZWQFg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Patient Sequence Encodings</figcaption></figure><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nf"><img src="../Images/3026a7962dfdb9eb5f28465d16eb0881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fgegl5Xn5ZNBwsSv16hREw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Python Pickled List of List containing patient visits and encoded diagnosis codes</figcaption></figure><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="6538" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">那么我们到底用这个嵌套列表填充什么呢？</h2><p id="2843" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">让我们分解填充函数:</p><ol class=""><li id="e9b8" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb nl nm nn no bi translated"><code class="fe mq mr ms mt b">lenghts = np.array([len(seq) for seq in seqs]) - 1</code>这里神秘地从长度中减去 1，在作者的笔记中，他提到<code class="fe mq mr ms mt b">visit</code>和<code class="fe mq mr ms mt b">label</code>文件必须匹配，因为算法会考虑推理时间的时间延迟。</li></ol><p id="036b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这是什么意思？考虑到数据的结构，每个患者记录中的最后一次就诊将被删除。如此处所示:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi np"><img src="../Images/9dbb283cc0816af42a119eb0bf3bc1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeK9teXhq5ziBDazsI3_Jw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Removing the last visit for inference</figcaption></figure><h2 id="54d4" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">旁白:在字符级 RNN 中处理可变长度序列</h2><p id="5bfb" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">如果这是一个角色级别的问题，我们就说[ <code class="fe mq mr ms mt b">Sparkle</code>、<code class="fe mq mr ms mt b">Dorian</code>、<code class="fe mq mr ms mt b">Deep</code>、<code class="fe mq mr ms mt b">Learning</code>。这些序列首先按长度降序排列，并用零(红色)填充，其中每个字母代表一个令牌。如下所示:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ffb51cb36d5c6ea32c2dcbe6e5535ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*WiREFq9O0xiWpdDlII8uVA.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Variable length sequence padding</figcaption></figure><h2 id="a997" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">EHR 数据:</h2><p id="ac43" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">然而，对于这种形式的 EHR 数据，给出了我们当前的问题，而不是每个编码的诊断代码代表一个唯一的令牌。在这种情况下，每次访问代表一个令牌/序列。因此，使用与字符级 RNNs 相同的方法，我们首先按照患者就诊降序排列每个小批量。在这种情况下，患者 1 具有最长的就诊历史，共有两次就诊，而患者 2 的就诊将被填充到最大长度 2，因为它是最长的序列。如下所示:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nr"><img src="../Images/f4f3d6a11b71c5c2ae6e307de641f721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyaEbwNQjUhETIaFJ1LT_A.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Padding EHR data</figcaption></figure><p id="677d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，我们已经解决了可变长度问题，我们可以继续对我们的序列进行多一热编码。这将产生所需的 S x B x I 尺寸(序列长度、批量大小、输入尺寸/vocab)。</p><p id="9366" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里，我们可以很容易地看到，序列将代表每个小批量中就诊历史最长的患者，而所有其他人将被填充到这个长度(红色)。根据所需的批次大小，批次大小将代表每个时间步输入多少患者序列。最后，内部列表将被编码为词汇表的长度，在本例中是整个数据集中唯一诊断代码的数量。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ns"><img src="../Images/d4ebcaab96c82d3de3641ac3ba19b87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYNkccwIykQ3xCCCn2KFhQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Multi-one hot encoded sequences</figcaption></figure><h2 id="ea5f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">标签</h2><p id="ea6b" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">以确保标签被移位一个序列，以便算法可以准确地预测下一个时间步长。作者通过确保训练数据排除每个患者历史中的最后一次就诊来解决这一问题，使用这种逻辑<code class="fe mq mr ms mt b">for xvec, subseq in zip(x[:, idx, :], seq[:-1]):</code>，其中我们采用每个患者就诊记录<code class="fe mq mr ms mt b">seq[:-1]</code>中除最后一次就诊之外的所有就诊。对于标签，这意味着序列将从患者的第二次就诊开始，或者按照 python 的索引风格，第一个索引<code class="fe mq mr ms mt b">for yvec, subseq in zip(y[:, idx, :], label[1:])</code>，其中标签<code class="fe mq mr ms mt b">label[1:]</code>移动一位。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nt"><img src="../Images/2b105577b31bedeb05b765abf45c65be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eRyRU3XYGvwPe-KAh0SSmw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Label time step lag</figcaption></figure><h2 id="b397" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">什么是掩蔽，它有什么作用？</h2><p id="c4e6" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">屏蔽允许算法知道真正的序列在 one-hot 编码数据中的位置，简单地说就是忽略/过滤掉填充值，在我们的例子中填充值为零。这使我们能够轻松处理 RNNs 中的可变长度序列，它需要固定长度的输入。是怎么做到的？还记得<code class="fe mq mr ms mt b">lengths</code>变量吗？该变量以降序存储每个患者序列的有效长度(<strong class="ki ir">回忆</strong>:在移除每个记录中的最后一个序列以进行推断后，例如，患者 1 有 3 次就诊，但长度将仅反映 2 次)。然后，代码<code class="fe mq mr ms mt b">mask[:lengths[idx], idx] = 1.</code>中的逻辑用 1 沿着行填充我们的归零张量，以匹配从最大到最小的每个患者序列的长度。</p><p id="6a18" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><code class="fe mq mr ms mt b">lenghts_artificial → array([2, 1])</code></p><p id="f752" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><code class="fe mq mr ms mt b">mask_artificial → tensor([[1., 1.], [1., 0.]])</code></p><h2 id="a8cc" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">数据加载器和采样器</h2><p id="b51f" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated"><code class="fe mq mr ms mt b">Dataset</code>类是一个抽象类，表示 x 和 y 对中的数据。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="f394" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><code class="fe mq mr ms mt b">Sampler</code>类随机打乱训练集的顺序(验证集不会被随机化)。此外，它保留创建完整批次所需的准确序列数量。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="7dcf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><code class="fe mq mr ms mt b">DataLoader</code>类结合了数据集和数据采样器，后者遍历数据集并抓取批处理。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="4a69" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">嵌入层</h2><p id="ff00" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">在将输入数据呈现给 GRU 之前，<code class="fe mq mr ms mt b">Custom_Embedding</code>类用于将高维多热点编码向量投影到低维空间。在这一步中，作者使用了两种方法</p><ol class=""><li id="5bbb" class="ng nh iq ki b kj kk km kn kp ni kt nj kx nk lb nl nm nn no bi translated">随机初始化，然后在反向推进期间学习适当的 W(emb)W(emb)权重</li></ol><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/540eaa9f845f85b89411a7e020c139f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*28q4wxkSNNC2NqB6-dPptA.png"/></div></figure><p id="797c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">2.使用 Skip-gram 算法初始化预训练嵌入，然后在 back-prop 期间优化权重</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8a7e16a5cff75c5e22d3e55ffd2216b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*MVrc4Z8R9667Ll7Oyk55sQ.png"/></div></figure><p id="148c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在本文的实现中，我们使用了第一种方法。因此，创建了<code class="fe mq mr ms mt b">Custom Embedding</code>类来在嵌入层上应用 tanh 激活。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Custom embedding Layer</figcaption></figure><h2 id="6dae" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">脱落层</h2><p id="1936" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">在本文中，作者使用了由<a class="ae lc" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Srivastava (2014) </a>首次引入的辍学的简单应用。虽然这种方法效果很好，但它影响了 RNNs 保持长期相关性的能力，因为我们没有在每个时间步长保持相同的掩码。为什么这很重要？很简单，如果我们在每个时间步随机采样一个新的掩码，它会干扰我们的 RNNs 连接，使网络难以确定哪些信息可能是长期相关的。在这种方法中，我测试了 Gal &amp; Ghahramani (2016)提出并由<a class="ae lc" href="https://arxiv.org/pdf/1708.02182.pdf" rel="noopener ugc nofollow" target="_blank"> Merity (2017) </a>进一步开发的 LSTMs 技术。在这里，他们提出通过在 LSTMs 中的多个时间步长上使用相同的漏失掩码来克服上述与随机采样相关的问题。在这里，我将应用相同的方法在每层(两层)之间的 GRU 上。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Dropout Layer</figcaption></figure><h2 id="b193" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">艾医生:通过递归神经网络预测临床事件</h2><p id="22c7" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">尽管 LSTMs 很受欢迎，也很受青睐。本文使用了 GRU 架构，因为它简单并且能够获得与 LSTMs 相似的性能。本文中使用的数据集包含<code class="fe mq mr ms mt b">263, 706 patients</code>，而我们的数据集(MIMIC III)总共包含<code class="fe mq mr ms mt b">7537 patients</code>。然而，作者证明了在一个医院系统缺乏训练像 AI 博士这样的深度学习模型所需的大规模数据集的情况下，迁移学习可能是一个可行的选择。使用以下架构，我的兴趣在于对患者未来诊断代码的预测。然而，人们可以很容易地推断出该算法来预测诊断和就诊间隔时间。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nv"><img src="../Images/bffdbd638187ae52bd3cb0798e498d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAT-F4V9OkG8e6uPpaoM1A.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Model Architecture</figcaption></figure><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="67db" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">GRU 层:</h2><p id="2ee0" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">这个类使用了<code class="fe mq mr ms mt b">EHR_GRU</code>单元格类，并允许在期望的层数上迭代。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="1c63" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">损失函数:</h2><p id="4b93" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">用于评估模型性能的损失函数包含交叉熵的组合。每个小批量的预测损失被标准化为序列长度。最后，L2 范数正则化应用于所有的权重矩阵。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="b47a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">模型参数:</h2><p id="b435" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">这里使用的参数选自 AI 博士论文中使用的参数。这种方法和我在这里介绍的方法之间的主要区别是，我对 RNNs 使用了更新的 drop out 方法。</p><p id="69eb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">num class = 4894<br/>input dimsize = 4894<br/>embSize = 200<br/>hiddenDimSize = 200<br/>batch size = 100 num layers = 2</p><h2 id="6bbe" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">加载数据:</h2><p id="9bba" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">需要注意的是，您希望将序列和标签的同一个文件传递到<code class="fe mq mr ms mt b">load_data</code>函数中，因为模型会在内部负责调整预测的时间步长。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="fd81" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">训练和验证循环</h2><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="771d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">我的实现与论文算法的比较:</h2><p id="fd1a" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">我在论文的算法上运行了相同的序列，这是用 theano 和 python 2.7 编写的，这里可以看到 10 个时期后的最佳交叉熵分数约为 86.79，而 my 为 107。虽然，通过一些超参数调整和优化，我并没有表现得更好，但算法肯定会表现得更好。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nw"><img src="../Images/3b3f6c1c68ed007f2f909b2aa7a93842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVTENwWAKdkN844Z5HFgzQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Dr. Algorithm results for comparison</figcaption></figure><h2 id="590b" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">观察结果:</h2><p id="7514" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">正如你所看到的，我们的训练和验证损失几乎是一样的，实际论文中使用的数据是如此之少。如果不过度拟合，可能很难获得更好的性能。然而，本教程的目的是提供一个如何使用 EHR 数据驱动洞察力的详细演练！</p><h2 id="df12" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">完整脚本</h2><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="fb77" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">后续步骤:</h2><ol class=""><li id="3506" class="ng nh iq ki b kj ml km mm kp nx kt ny kx nz lb nl nm nn no bi translated">使用 Fast 添加回调。人工智能的回调方法来跟踪训练数据</li><li id="0461" class="ng nh iq ki b kj oa km ob kp oc kt od kx oe lb nl nm nn no bi translated">尝试不同的初始化方法</li></ol><h2 id="70b7" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kp mb mc md kt me mf mg kx mh mi mj mk bi translated">致谢:</h2><ol class=""><li id="e239" class="ng nh iq ki b kj ml km mm kp nx kt ny kx nz lb nl nm nn no bi translated">Fast.ai(雷切尔·托马斯、杰瑞米·霍华德和令人惊叹的 fast.ai 社区)</li><li id="58f9" class="ng nh iq ki b kj oa km ob kp oc kt od kx oe lb nl nm nn no bi translated">多里安·普勒里</li></ol></div></div>    
</body>
</html>