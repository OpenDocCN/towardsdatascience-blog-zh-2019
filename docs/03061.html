<html>
<head>
<title>LSTM Autoencoder for Extreme Rare Event Classification in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">喀拉斯极端罕见事件分类的 LSTM 自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb?source=collection_archive---------0-----------------------#2019-05-17">https://towardsdatascience.com/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb?source=collection_archive---------0-----------------------#2019-05-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b6e6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这里，我们将学习 LSTM 模型的数据准备的细节，并为罕见事件分类建立一个 LSTM 自动编码器。</h2></div><p id="4379" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">&lt;<download the="" free="" book="" class="ae lb" href="https://www.understandingdeeplearning.com" rel="noopener ugc nofollow" target="_blank">了解深度学习，了解更多&gt; &gt;</download></p><p id="bfe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章是我上一篇文章<a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">使用自动编码器进行极端罕见事件分类</a>的延续。在上一篇文章中，我们谈到了一个极其罕见的事件数据中的挑战，这些数据中只有不到 1%的数据被正面标记。我们使用异常检测的概念为此类流程构建了一个自动编码器分类器。</p><p id="d7ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，我们掌握的数据是一个时间序列。但是之前我们使用了密集层自动编码器，它不使用数据中的时间特征。因此，在这篇文章中，我们将通过构建一个 LSTM 自动编码器来改进我们的方法。</p><p id="0a59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们将学习:</p><ul class=""><li id="4eea" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">LSTM 模型的数据准备步骤，</li><li id="14cc" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">构建和实现 LSTM 自动编码器，以及</li><li id="5963" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">使用 LSTM 自动编码器进行罕见事件分类。</li></ul><p id="5493" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">快速回顾 LSTM </strong>:</p><ul class=""><li id="159d" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">LSTM 是一种递归神经网络(RNN)。一般来说，RNNs，特别是 LSTM，用于序列或时间序列数据。</li><li id="9f8e" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">这些模型能够自动提取过去事件的影响。</li><li id="47ab" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">众所周知，LSTM 有能力提取过去事件长期和短期影响。</li></ul><p id="c05b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面，我们将直接开发一个 LSTM 自动编码器。建议阅读<a class="ae lb" rel="noopener" target="_blank" href="/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352">逐步理解 LSTM 自动编码器层</a>以更好地理解和进一步改进下面的网络。</p><p id="aeea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，关于数据问题，我们有来自造纸厂的纸张断裂的真实数据。我们的目标是提前预测中断。有关数据、问题和分类方法的详细信息，请参考<a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">使用自动编码器的极端罕见事件分类</a>。</p><h1 id="97cc" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">多元数据的 LSTM 自动编码器</h1><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/1df28a71dec86987bcb50ee001b4800a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tY4F3BPq4ctTMelMEnLZvw.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 1. An LSTM Autoencoder.</figcaption></figure><p id="9b2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的问题中，我们有一个多元时间序列数据。多元时间序列数据包含在一段时间内观察到的多个变量。我们将在这个多元时间序列上建立一个 LSTM 自动编码器来执行稀有事件分类。如[ <a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098"> 1 </a>中所述，这是通过使用异常检测方法实现的:</p><ul class=""><li id="9661" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">我们在<em class="my">正常</em>(负标签)数据上建立一个自动编码器，</li><li id="1deb" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">用它来重建一个新样本，</li><li id="3798" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">如果重建误差很高，我们将其标记为断纸。</li></ul><p id="80f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LSTM 几乎不需要特殊的数据预处理步骤。在下文中，我们将对这些步骤给予足够的重视。</p><p id="ab9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始实现。</p><h1 id="d391" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">图书馆</h1><p id="c392" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">我喜欢先把库和全局常量放在一起。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="5b7c" class="nj lr iq nf b gy nk nl l nm nn">%matplotlib inline<br/><strong class="nf ir">import</strong> <strong class="nf ir">matplotlib.pyplot</strong> <strong class="nf ir">as</strong> <strong class="nf ir">plt</strong><br/><strong class="nf ir">import</strong> <strong class="nf ir">seaborn</strong> <strong class="nf ir">as</strong> <strong class="nf ir">sns</strong><br/><br/><strong class="nf ir">import</strong> <strong class="nf ir">pandas</strong> <strong class="nf ir">as</strong> <strong class="nf ir">pd</strong><br/><strong class="nf ir">import</strong> <strong class="nf ir">numpy</strong> <strong class="nf ir">as</strong> <strong class="nf ir">np</strong><br/><strong class="nf ir">from</strong> <strong class="nf ir">pylab</strong> <strong class="nf ir">import</strong> rcParams<br/><br/><strong class="nf ir">import</strong> <strong class="nf ir">tensorflow</strong> <strong class="nf ir">as</strong> <strong class="nf ir">tf</strong><br/><strong class="nf ir">from</strong> <strong class="nf ir">keras</strong> <strong class="nf ir">import</strong> optimizers, Sequential<br/><strong class="nf ir">from</strong> <strong class="nf ir">keras.models</strong> <strong class="nf ir">import</strong> Model<br/><strong class="nf ir">from</strong> <strong class="nf ir">keras.utils</strong> <strong class="nf ir">import</strong> plot_model<br/><strong class="nf ir">from</strong> <strong class="nf ir">keras.layers</strong> <strong class="nf ir">import</strong> Dense, LSTM, RepeatVector, TimeDistributed<br/><strong class="nf ir">from</strong> <strong class="nf ir">keras.callbacks</strong> <strong class="nf ir">import</strong> ModelCheckpoint, TensorBoard<br/><br/><strong class="nf ir">from</strong> <strong class="nf ir">sklearn.preprocessing</strong> <strong class="nf ir">import</strong> StandardScaler<br/><strong class="nf ir">from</strong> <strong class="nf ir">sklearn.model_selection</strong> <strong class="nf ir">import</strong> train_test_split<br/><strong class="nf ir">from</strong> <strong class="nf ir">sklearn.metrics</strong> <strong class="nf ir">import</strong> confusion_matrix, precision_recall_curve<br/><strong class="nf ir">from</strong> <strong class="nf ir">sklearn.metrics</strong> <strong class="nf ir">import</strong> recall_score, classification_report, auc, roc_curve<br/><strong class="nf ir">from</strong> <strong class="nf ir">sklearn.metrics</strong> <strong class="nf ir">import</strong> precision_recall_fscore_support, f1_score<br/><br/><strong class="nf ir">from</strong> <strong class="nf ir">numpy.random</strong> <strong class="nf ir">import</strong> seed<br/>seed(7)<br/><strong class="nf ir">from</strong> <strong class="nf ir">tensorflow</strong> <strong class="nf ir">import</strong> set_random_seed<br/>set_random_seed(11)</span><span id="64f9" class="nj lr iq nf b gy no nl l nm nn"><strong class="nf ir">from</strong> <strong class="nf ir">sklearn.model_selection</strong> <strong class="nf ir">import</strong> train_test_split<br/><br/>SEED = 123 <em class="my">#used to help randomly select the data points</em><br/>DATA_SPLIT_PCT = 0.2<br/><br/>rcParams['figure.figsize'] = 8, 6<br/>LABELS = ["Normal","Break"]</span></pre><h1 id="7224" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">数据准备</h1><p id="9ce9" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">如前所述，LSTM 要求在数据准备中采取一些具体步骤。LSTMs 的输入是从时间序列数据创建的三维数组。这是一个<strong class="kh ir">容易出错的步骤</strong>，所以我们将查看细节。</p><h2 id="f8c5" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated">读出数据</h2><p id="3a22" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">数据取自<a class="ae lb" href="https://arxiv.org/abs/1809.10717" rel="noopener ugc nofollow" target="_blank"> 2 </a>。到数据的链接在这里是<a class="ae lb" href="https://docs.google.com/forms/d/e/1FAIpQLSdyUk3lfDl7I5KYK_pw285LCApc-_RcoC0Tf9cnDnZ_TWzPAw/viewform" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="2337" class="nj lr iq nf b gy nk nl l nm nn">df = pd.read_csv("data/processminer-rare-event-mts - data.csv") <br/>df.head(n=5)  <em class="my"># visualize the data.</em></span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oa"><img src="../Images/1078b10849e7677f1f624040cbfbca8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvjbSVTUiNQgOh_fxXVvvg.png"/></div></div></figure><h2 id="767e" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated"><strong class="ak">曲线移动</strong></h2><p id="120b" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">正如在[ <a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098"> 1 </a>中提到的，这个罕见事件问题的目标是在板块断裂发生之前预测它。我们将尝试提前 4 分钟预测<strong class="kh ir">到</strong>的休息时间。对于这些数据，这相当于将标签上移两行。直接用<code class="fe ob oc od nf b">df.y=df.y.shift(-2)</code>就可以了。然而，在这里我们需要做以下事情:</p><ul class=""><li id="5357" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">对于标签为 1 的任意一行<em class="my"> n </em>，使(<em class="my"> n </em> -2):( <em class="my"> n </em> -1)为 1。这样，我们就可以教会分类器预测<strong class="kh ir">到</strong> 4 分钟前的情况。而且，</li><li id="8042" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">拆下排<em class="my"> n </em>。行<em class="my"> n </em>被删除，因为我们对训练分类器预测已经发生的中断不感兴趣。</li></ul><p id="eac0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们开发了以下函数来执行这种曲线移动。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="800a" class="nj lr iq nf b gy nk nl l nm nn">sign = <strong class="nf ir">lambda</strong> x: (1, -1)[x &lt; 0]<br/><br/><strong class="nf ir">def</strong> curve_shift(df, shift_by):<br/>    <em class="my">'''</em><br/><em class="my">    This function will shift the binary labels in a dataframe.</em><br/><em class="my">    The curve shift will be with respect to the 1s. </em><br/><em class="my">    For example, if shift is -2, the following process</em><br/><em class="my">    will happen: if row n is labeled as 1, then</em><br/><em class="my">    - Make row (n+shift_by):(n+shift_by-1) = 1.</em><br/><em class="my">    - Remove row n.</em><br/><em class="my">    i.e. the labels will be shifted up to 2 rows up.</em><br/><em class="my">    </em><br/><em class="my">    Inputs:</em><br/><em class="my">    df       A pandas dataframe with a binary labeled column. </em><br/><em class="my">             This labeled column should be named as 'y'.</em><br/><em class="my">    shift_by An integer denoting the number of rows to shift.</em><br/><em class="my">    </em><br/><em class="my">    Output</em><br/><em class="my">    df       A dataframe with the binary labels shifted by shift.</em><br/><em class="my">    '''</em><br/><br/>    vector = df['y'].copy()<br/>    <strong class="nf ir">for</strong> s <strong class="nf ir">in</strong> range(abs(shift_by)):<br/>        tmp = vector.shift(sign(shift_by))<br/>        tmp = tmp.fillna(0)<br/>        vector += tmp<br/>    labelcol = 'y'<br/>    <em class="my"># Add vector to the df</em><br/>    df.insert(loc=0, column=labelcol+'tmp', value=vector)<br/>    <em class="my"># Remove the rows with labelcol == 1.</em><br/>    df = df.drop(df[df[labelcol] == 1].index)<br/>    <em class="my"># Drop labelcol and rename the tmp col as labelcol</em><br/>    df = df.drop(labelcol, axis=1)<br/>    df = df.rename(columns={labelcol+'tmp': labelcol})<br/>    <em class="my"># Make the labelcol binary</em><br/>    df.loc[df[labelcol] &gt; 0, labelcol] = 1<br/><br/>    <strong class="nf ir">return</strong> df</span></pre><p id="720c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将转换我们的数据，并验证转换是否正确。在接下来的部分中，我们还有一些测试步骤。建议使用它们来确保数据准备步骤按预期运行。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="ccd5" class="nj lr iq nf b gy nk nl l nm nn">print('Before shifting')  <em class="my"># Positive labeled rows before shifting.</em><br/>one_indexes = df.index[df['y'] == 1]<br/>display(df.iloc[(np.where(np.array(input_y) == 1)[0][0]-5):(np.where(np.array(input_y) == 1)[0][0]+1), ])<br/><br/><em class="my"># Shift the response column y by 2 rows to do a 4-min ahead prediction.</em><br/>df = curve_shift(df, shift_by = -2)<br/><br/>print('After shifting')  <em class="my"># Validating if the shift happened correctly.</em><br/>display(df.iloc[(one_indexes[0]-4):(one_indexes[0]+1), 0:5].head(n=5))</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oe"><img src="../Images/c71b13d094c854b35e6d5ca88e6bd58d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PG3cLm13IiWI9D6Z6FBs7Q.png"/></div></div></figure><p id="981f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们在这里注意到，我们将 1999 年 5 月 1 日 8:38 的正标签移动到了<em class="my"> n </em> -1 和<em class="my"> n </em> -2 时间戳，并删除了第<em class="my"> n </em>行。此外，在中断行和下一行之间有超过 2 分钟的时间差。这是因为，当发生中断时，机器会在中断状态停留一段时间。在此期间，连续行的 y = 1。在所提供的数据中，这些连续的中断行被删除，以防止分类器学习预测已经发生的之后的中断<strong class="kh ir">。详见[ <a class="ae lb" href="https://arxiv.org/pdf/1809.10717.pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a>。</strong></p><p id="c31c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，我们通过删除时间和另外两个分类列来清理数据。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="5509" class="nj lr iq nf b gy nk nl l nm nn"><em class="my"># Remove time column, and the categorical columns</em><br/>df = df.drop(['time', 'x28', 'x61'], axis=1)</span></pre><h2 id="76a1" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated">为 LSTM 准备输入数据</h2><p id="78f3" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">LSTM 比其他车型要求更高一些。在准备适合 LSTM 的数据时可能会花费大量的时间和精力。然而，这通常是值得努力的。</p><p id="6c2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LSTM 模型的输入数据是一个三维数组。数组的形状是<em class="my">样本</em> x <em class="my">回看</em> x <em class="my">特征</em>。让我们来理解它们，</p><ul class=""><li id="2ea4" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated"><em class="my">样本</em>:这就是观察的数量，或者换句话说，就是数据点的数量。</li><li id="bcd3" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">回顾过去:LSTM 模特是用来回顾过去的。也就是说，在时间<em class="my"> t </em>时，LSTM 将处理数据直到(<em class="my"> t </em> - <em class="my">回看</em>)以做出预测。</li><li id="8b8e" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated"><em class="my">特征</em>:输入数据中出现的特征数量。</li></ul><p id="ba68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将提取特征和响应。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="848c" class="nj lr iq nf b gy nk nl l nm nn">input_X = df.loc[:, df.columns != 'y'].values  <em class="my"># converts the df to a numpy array</em><br/>input_y = df['y'].values<br/><br/>n_features = input_X.shape[1]  <em class="my"># number of features</em></span></pre><p id="1785" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的<code class="fe ob oc od nf b">input_X</code>是一个大小为<em class="my">样本</em> x <em class="my">特征</em>的二维数组。我们希望能够将这样的 2D 数组转换成大小为:<em class="my">样本</em> x <em class="my">回看</em> x <em class="my">特征</em>的 3D 数组。请参考上面的图 1 以获得直观的理解。</p><p id="704d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们开发了一个函数<code class="fe ob oc od nf b">temporalize</code>。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="6b9e" class="nj lr iq nf b gy nk nl l nm nn"><strong class="nf ir">def</strong> temporalize(X, y, lookback):<br/>    <em class="my">'''</em><br/><em class="my">    Inputs</em><br/><em class="my">    X         A 2D numpy array ordered by time of shape: </em><br/><em class="my">              (n_observations x n_features)</em><br/><em class="my">    y         A 1D numpy array with indexes aligned with </em><br/><em class="my">              X, i.e. y[i] should correspond to X[i]. </em><br/><em class="my">              Shape: n_observations.</em><br/><em class="my">    lookback  The window size to look back in the past </em><br/><em class="my">              records. Shape: a scalar.</em><br/><br/><em class="my">    Output</em><br/><em class="my">    output_X  A 3D numpy array of shape: </em><br/><em class="my">              ((n_observations-lookback-1) x lookback x </em><br/><em class="my">              n_features)</em><br/><em class="my">    output_y  A 1D array of shape: </em><br/><em class="my">              (n_observations-lookback-1), aligned with X.</em><br/><em class="my">    '''</em><br/>    output_X = []<br/>    output_y = []<br/>    <strong class="nf ir">for</strong> i <strong class="nf ir">in</strong> range(len(X) - lookback - 1):<br/>        t = []<br/>        <strong class="nf ir">for</strong> j <strong class="nf ir">in</strong> range(1, lookback + 1):<br/>            <em class="my"># Gather the past records upto the lookback period</em><br/>            t.append(X[[(i + j + 1)], :])<br/>        output_X.append(t)<br/>        output_y.append(y[i + lookback + 1])<br/>    <strong class="nf ir">return</strong> np.squeeze(np.array(output_X)), np.array(output_y)</span></pre><p id="ff63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了测试和演示这个功能，我们将在下面用<code class="fe ob oc od nf b">lookback = 5</code>看一个例子。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="db52" class="nj lr iq nf b gy nk nl l nm nn">print('First instance of y = 1 in the original data')<br/>display(df.iloc[(np.where(np.array(input_y) == 1)[0][0]-5):(np.where(np.array(input_y) == 1)[0][0]+1), ])</span><span id="b941" class="nj lr iq nf b gy no nl l nm nn">lookback = 5  # Equivalent to 10 min of past data.<br/># Temporalize the data<br/>X, y = temporalize(X = input_X, y = input_y, lookback = lookback)</span><span id="7db0" class="nj lr iq nf b gy no nl l nm nn">print('For the same instance of y = 1, we are keeping past 5 samples in the 3D predictor array, X.')<br/>display(pd.DataFrame(np.concatenate(X[np.where(np.array(y) == 1)[0][0]], axis=0 )))</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi of"><img src="../Images/1f7a4a3a71dbf97f819c257d1f720f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-o9cv_nzI0m5g9SFCNXgQ.png"/></div></div></figure><p id="866e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在这里寻找的是，</p><ul class=""><li id="b446" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">在原始数据中，第 257 行的 y = 1。</li><li id="437f" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">使用<code class="fe ob oc od nf b">lookback = 5</code>，我们希望 LSTM 查看 257 行之前的 5 行(包括它自己)。</li><li id="7890" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">在 3D 阵列<code class="fe ob oc od nf b">X</code>中，<code class="fe ob oc od nf b">X[i,:,:]</code>处的每个 2D 块表示对应于<code class="fe ob oc od nf b">y[i]</code>的预测数据。打个比方，在回归中<code class="fe ob oc od nf b">y[i]</code>对应一个 1D 向量<code class="fe ob oc od nf b">X[i,:]</code>；在 LSTM <code class="fe ob oc od nf b">y[i]</code>对应一个 2D 阵列<code class="fe ob oc od nf b">X[i,:,:]</code>。</li><li id="677f" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">这个 2D 块<code class="fe ob oc od nf b">X[i,:,:]</code>应该具有在<code class="fe ob oc od nf b">input_X[i,:]</code>和直到给定<code class="fe ob oc od nf b">lookback</code>的先前行的预测器。</li><li id="3b1a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">正如我们在上面的输出中看到的，底部的<code class="fe ob oc od nf b">X[i,:,:]</code>块与顶部显示的 y=1 的过去五行相同。</li><li id="bb13" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">类似地，这适用于所有 y 的全部数据。此处的示例显示了 y=1 的情况，以便于可视化。</li></ul><h1 id="9b51" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">分为训练、有效和测试</h1><p id="659d" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">这对于<code class="fe ob oc od nf b">sklearn</code>功能来说很简单。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="da92" class="nj lr iq nf b gy nk nl l nm nn">X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, random_state=SEED)</span><span id="6778" class="nj lr iq nf b gy no nl l nm nn">X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT, random_state=SEED)</span></pre><p id="4d2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了训练自动编码器，我们将使用仅来自负标签数据的 X。所以我们把 y = 0 对应的 X 分开。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="4ab1" class="nj lr iq nf b gy nk nl l nm nn">X_train_y0 = X_train[y_train==0]<br/>X_train_y1 = X_train[y_train==1]</span><span id="1d1f" class="nj lr iq nf b gy no nl l nm nn">X_valid_y0 = X_valid[y_valid==0]<br/>X_valid_y1 = X_valid[y_valid==1]</span></pre><p id="04c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将把 X 的形状重塑成所需的 3D 尺寸:<em class="my">样本</em> x <em class="my">回看</em> x <em class="my">特征</em>。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="7e6d" class="nj lr iq nf b gy nk nl l nm nn">X_train = X_train.reshape(X_train.shape[0], lookback, n_features)<br/>X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)<br/>X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)</span><span id="36a8" class="nj lr iq nf b gy no nl l nm nn">X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)<br/>X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)<br/>X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)</span><span id="93f8" class="nj lr iq nf b gy no nl l nm nn">X_test = X_test.reshape(X_test.shape[0], lookback, n_features)</span></pre><h2 id="c967" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated">使数据标准化</h2><p id="9f27" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">对于自动编码器，通常最好使用标准化数据(转换为高斯数据，平均值为 0，标准偏差为 1)。</p><blockquote class="og oh oi"><p id="96f4" class="kf kg my kh b ki kj jr kk kl km ju kn oj kp kq kr ok kt ku kv ol kx ky kz la ij bi translated">一个常见的标准化错误是:我们将整个数据标准化，然后分成训练测试。这是不正确的。在建模过程中，测试数据应该是完全不可见的。因此，我们应该规范化训练数据，并使用其汇总统计数据来规范化测试数据(对于规范化，这些统计数据是每个特征的平均值和方差)。</p></blockquote><p id="317c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">标准化这些数据有点棘手。这是因为 X 矩阵是 3D 的，我们希望标准化发生在原始 2D 数据上。</p><p id="0ebe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们需要两个 UDF。</p><ul class=""><li id="c837" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated"><code class="fe ob oc od nf b">flatten</code>:该功能将重新创建原始的 2D 阵列，3D 阵列是从该阵列创建的。这个函数是<code class="fe ob oc od nf b">temporalize</code>的倒数，意思是<code class="fe ob oc od nf b">X = flatten(temporalize(X))</code>。</li><li id="5433" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated"><code class="fe ob oc od nf b">scale</code>:这个函数将缩放我们创建的 3D 数组，作为 LSTM 的输入。</li></ul><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="27a1" class="nj lr iq nf b gy nk nl l nm nn"><strong class="nf ir">def</strong> flatten(X):<br/>    <em class="my">'''</em><br/><em class="my">    Flatten a 3D array.</em><br/><em class="my">    </em><br/><em class="my">    Input</em><br/><em class="my">    X            A 3D array for lstm, where the array is sample x timesteps x features.</em><br/><em class="my">    </em><br/><em class="my">    Output</em><br/><em class="my">    flattened_X  A 2D array, sample x features.</em><br/><em class="my">    '''</em><br/>    flattened_X = np.empty((X.shape[0], X.shape[2]))  <em class="my"># sample x features array.</em><br/>    <strong class="nf ir">for</strong> i <strong class="nf ir">in</strong> range(X.shape[0]):<br/>        flattened_X[i] = X[i, (X.shape[1]-1), :]<br/>    <strong class="nf ir">return</strong>(flattened_X)<br/><br/><strong class="nf ir">def</strong> scale(X, scaler):<br/>    <em class="my">'''</em><br/><em class="my">    Scale 3D array.</em><br/><br/><em class="my">    Inputs</em><br/><em class="my">    X            A 3D array for lstm, where the array is sample x timesteps x features.</em><br/><em class="my">    scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize</em><br/><em class="my">    </em><br/><em class="my">    Output</em><br/><em class="my">    X            Scaled 3D array.</em><br/><em class="my">    '''</em><br/>    <strong class="nf ir">for</strong> i <strong class="nf ir">in</strong> range(X.shape[0]):<br/>        X[i, :, :] = scaler.transform(X[i, :, :])<br/>        <br/>    <strong class="nf ir">return</strong> X</span></pre><blockquote class="og oh oi"><p id="2316" class="kf kg my kh b ki kj jr kk kl km ju kn oj kp kq kr ok kt ku kv ol kx ky kz la ij bi translated">为什么我们不首先归一化原始 2D 数据，然后创建三维阵列？因为，为了做到这一点，我们将:把数据分成训练和测试，然后对它们进行规范化。然而，当我们在测试数据上创建 3D 阵列时，我们丢失了最初的样本行，直到<em class="iq">回看。</em>拆分为训练有效测试将导致验证集和测试集都出现这种情况。</p></blockquote><p id="4a11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将安装一个来自<code class="fe ob oc od nf b">sklearn</code>的标准化对象。该函数将数据标准化为正态(0，1)。注意，我们需要展平<code class="fe ob oc od nf b">X_train_y0</code>数组以传递给<code class="fe ob oc od nf b">fit</code>函数。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="2485" class="nj lr iq nf b gy nk nl l nm nn"><em class="my"># Initialize a scaler using the training data.</em><br/>scaler = StandardScaler().fit(flatten(X_train_y0))</span></pre><p id="96fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用我们的 UDF<code class="fe ob oc od nf b">scale</code>，用合适的转换对象<code class="fe ob oc od nf b">scaler</code>来标准化<code class="fe ob oc od nf b">X_train_y0</code>。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="ff91" class="nj lr iq nf b gy nk nl l nm nn">X_train_y0_scaled = scale(X_train_y0, scaler)</span></pre><p id="ed63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">确保</strong> <code class="fe ob oc od nf b"><strong class="kh ir">scale</strong></code> <strong class="kh ir">正常工作？</strong></p><p id="44ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ob oc od nf b">X_train</code>的正确转换将确保展平的<code class="fe ob oc od nf b">X_train</code>的每一列的均值和方差分别为 0 和 1。我们测试这个。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="d62f" class="nj lr iq nf b gy nk nl l nm nn">a = flatten(X_train_y0_scaled)<br/>print('colwise mean', np.mean(a, axis=0).round(6))<br/>print('colwise variance', np.var(a, axis=0))</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi om"><img src="../Images/cb76d99f51447fc4cfa25688a53b1708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uaefM3LrgtG5l7bow0-hSQ.png"/></div></div></figure><p id="1edf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面输出的所有均值和方差分别为 0 和 1。因此，缩放是正确的。我们现在将扩展验证和测试集。我们将再次在这些器械包中使用<code class="fe ob oc od nf b">scaler</code>对象。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="4c40" class="nj lr iq nf b gy nk nl l nm nn">X_valid_scaled = scale(X_valid, scaler)<br/>X_valid_y0_scaled = scale(X_valid_y0, scaler)</span><span id="0c79" class="nj lr iq nf b gy no nl l nm nn">X_test_scaled = scale(X_test, scaler)</span></pre><h1 id="5263" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">LSTM 自动编码器培训</h1><p id="2bce" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">首先，我们将初始化一些变量。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="274f" class="nj lr iq nf b gy nk nl l nm nn">timesteps =  X_train_y0_scaled.shape[1] <em class="my"># equal to the lookback</em><br/>n_features =  X_train_y0_scaled.shape[2] <em class="my"># 59</em><br/><br/>epochs = 200<br/>batch = 64<br/>lr = 0.0001</span></pre><p id="79a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们开发一个简单的架构。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="d7a1" class="nj lr iq nf b gy nk nl l nm nn">lstm_autoencoder = Sequential()<br/><em class="my"># Encoder</em><br/>lstm_autoencoder.add(LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=<strong class="nf ir">True</strong>))<br/>lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=<strong class="nf ir">False</strong>))<br/>lstm_autoencoder.add(RepeatVector(timesteps))<br/><em class="my"># Decoder</em><br/>lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=<strong class="nf ir">True</strong>))<br/>lstm_autoencoder.add(LSTM(32, activation='relu', return_sequences=<strong class="nf ir">True</strong>))<br/>lstm_autoencoder.add(TimeDistributed(Dense(n_features)))<br/><br/>lstm_autoencoder.summary()</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi on"><img src="../Images/b96be2b91f36545edff5d30878064733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bTHwVO1nPx9UI8T4lARGBw.png"/></div></div></figure><p id="dbdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从 summary()来看，参数的总数是 5，331。这大约是训练规模的一半。因此，这是一个合适的模型。为了有一个更大的架构，我们将需要添加正规化，例如辍学，这将在下一篇文章中讨论。</p><p id="e340" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将训练自动编码器。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="acaf" class="nj lr iq nf b gy nk nl l nm nn">adam = optimizers.Adam(lr)<br/>lstm_autoencoder.compile(loss='mse', optimizer=adam)<br/><br/>cp = ModelCheckpoint(filepath="lstm_autoencoder_classifier.h5",<br/>                               save_best_only=<strong class="nf ir">True</strong>,<br/>                               verbose=0)<br/><br/>tb = TensorBoard(log_dir='./logs',<br/>                histogram_freq=0,<br/>                write_graph=<strong class="nf ir">True</strong>,<br/>                write_images=<strong class="nf ir">True</strong>)<br/><br/>lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled, <br/>                                                epochs=epochs, <br/>                                                batch_size=batch, <br/>                                                validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),<br/>                                                verbose=2).history</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oo"><img src="../Images/247af26a71748e37a52fcbab6bbd4856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sB9yuVxvt2QUV4Kad8VZ0Q.png"/></div></div></figure><p id="69d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">绘制各时期损失的变化。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="dde8" class="nj lr iq nf b gy nk nl l nm nn">plt.plot(lstm_autoencoder_history['loss'], linewidth=2, label='Train')<br/>plt.plot(lstm_autoencoder_history['val_loss'], linewidth=2, label='Valid')<br/>plt.legend(loc='upper right')<br/>plt.title('Model loss')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.show()</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi op"><img src="../Images/47091b562a7f7c4dfb8edf705cc64940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WcEJVC3b8udIGbaMpQCkjA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 2. Loss function over the epochs.</figcaption></figure><h2 id="5adf" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated"><strong class="ak">分类</strong></h2><p id="b509" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">类似于<a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">之前的文章</a> [ <a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098"> 1 </a> ]，这里我们展示了如何使用自动编码器重建误差进行罕见事件分类。我们遵循这个概念:自动编码器被期望重建一个 noi。如果重建误差很高，我们将把它分类为一个断页。</p><p id="9bad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要确定这方面的门槛。另外，请注意，这里我们将使用包含 y = 0 或 1 的整个验证集。</p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="44bd" class="nj lr iq nf b gy nk nl l nm nn">valid_x_predictions = lstm_autoencoder.predict(X_valid_scaled)<br/>mse = np.mean(np.power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)<br/><br/>error_df = pd.DataFrame({'Reconstruction_error': mse,<br/>                        'True_class': y_valid.tolist()})<br/><br/>precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)<br/>plt.plot(threshold_rt, precision_rt[1:], label="Precision",linewidth=5)<br/>plt.plot(threshold_rt, recall_rt[1:], label="Recall",linewidth=5)<br/>plt.title('Precision and recall for different threshold values')<br/>plt.xlabel('Threshold')<br/>plt.ylabel('Precision/Recall')<br/>plt.legend()<br/>plt.show()</span></pre><p id="664f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我们必须对数组进行<code class="fe ob oc od nf b">flatten</code>运算来计算<code class="fe ob oc od nf b">mse</code>。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oq"><img src="../Images/9115cede60e5c9979e0a35430195f531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crj4hR0KnIs6XaWUObVpQA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 3. A threshold of 0.3 should provide a reasonable trade-off between precision and recall, as we want to higher recall.</figcaption></figure><p id="30cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将对测试数据进行分类。</p><blockquote class="og oh oi"><p id="1ba0" class="kf kg my kh b ki kj jr kk kl km ju kn oj kp kq kr ok kt ku kv ol kx ky kz la ij bi translated"><strong class="kh ir">我们不应该从测试数据中估计分类阈值。这将导致过度拟合。</strong></p></blockquote><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="b791" class="nj lr iq nf b gy nk nl l nm nn">test_x_predictions = lstm_autoencoder.predict(X_test_scaled)<br/>mse = np.mean(np.power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)<br/><br/>error_df = pd.DataFrame({'Reconstruction_error': mse,<br/>                        'True_class': y_test.tolist()})<br/><br/>threshold_fixed = 0.3<br/>groups = error_df.groupby('True_class')<br/>fig, ax = plt.subplots()<br/><br/><strong class="nf ir">for</strong> name, group <strong class="nf ir">in</strong> groups:<br/>    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',<br/>            label= "Break" <strong class="nf ir">if</strong> name == 1 <strong class="nf ir">else</strong> "Normal")<br/>ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')<br/>ax.legend()<br/>plt.title("Reconstruction error for different classes")<br/>plt.ylabel("Reconstruction error")<br/>plt.xlabel("Data point index")<br/>plt.show();</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi or"><img src="../Images/23f113e715298db09d0a721e10fc6182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAKM6saNgsqOlTClmqjDEA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 4. Using threshold = 0.8 for classification. The orange and blue dots above the threshold line represents the True Positive and False Positive, respectively.</figcaption></figure><p id="b213" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图 4 中，阈值线上方的橙色和蓝色圆点分别代表真阳性和假阳性。如我们所见，我们有很多误报。</p><p id="28d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看准确度的结果。</p><h2 id="261c" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated"><strong class="ak">测试精度</strong></h2><p id="a141" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated"><strong class="kh ir">混淆矩阵</strong></p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="f890" class="nj lr iq nf b gy nk nl l nm nn">pred_y = [1 if e &gt; threshold_fixed else 0 for e in error_df.Reconstruction_error.values]</span><span id="65f7" class="nj lr iq nf b gy no nl l nm nn">conf_matrix = confusion_matrix(error_df.True_class, pred_y)<br/><br/>plt.figure(figsize=(6, 6))<br/>sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=<strong class="nf ir">True</strong>, fmt="d");<br/>plt.title("Confusion matrix")<br/>plt.ylabel('True class')<br/>plt.xlabel('Predicted class')<br/>plt.show()</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi os"><img src="../Images/c05e88ca01d03d5576f79832d55a1d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-6WuQFHNb86mJGKuX-hXg.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 5. Confusion matrix showing the True Positives and False Positives.</figcaption></figure><p id="cb05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> ROC 曲线和 AUC </strong></p><pre class="mj mk ml mm gt ne nf ng nh aw ni bi"><span id="3b69" class="nj lr iq nf b gy nk nl l nm nn">false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)<br/>roc_auc = auc(false_pos_rate, true_pos_rate,)<br/><br/>plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = <strong class="nf ir">%0.3f</strong>'% roc_auc)<br/>plt.plot([0,1],[0,1], linewidth=5)<br/><br/>plt.xlim([-0.01, 1])<br/>plt.ylim([0, 1.01])<br/>plt.legend(loc='lower right')<br/>plt.title('Receiver operating characteristic curve (ROC)')<br/>plt.ylabel('True Positive Rate')<br/>plt.xlabel('False Positive Rate')<br/>plt.show()</span></pre><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi op"><img src="../Images/21250bd35673819ca784c31712b2b213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bjSwlMXiyHKEzu-5wRNPFA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Figure 6. The ROC curve.</figcaption></figure><p id="2ab7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与[ <a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098"> 1 </a>中的密集层自动编码器相比，我们看到 AUC 提高了大约 10%。根据图 5 中的混淆矩阵，我们可以预测 39 个中断实例中的 10 个。正如[ <a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098"> 1 </a>中所讨论的，这对造纸厂来说意义重大。然而，与密集层自动编码器相比，我们实现的改进是微小的。</p><p id="41c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主要原因是 LSTM 模型有更多的参数需要估计。对 LSTMs 使用正则化变得很重要。正则化和其他模型改进将在下一篇文章中讨论。</p><h2 id="7608" class="nj lr iq bd ls np nq dn lw nr ns dp ma ko nt nu mc ks nv nw me kw nx ny mg nz bi translated"><a class="ae lb" href="https://github.com/cran2367/lstm_autoencoder_classifier/blob/master/lstm_autoencoder_classifier.ipynb" rel="noopener ugc nofollow" target="_blank"> Github 知识库</a></h2><div class="ot ou gp gr ov ow"><a href="https://github.com/cran2367/lstm_autoencoder_classifier/blob/master/lstm_autoencoder_classifier.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">cran 2367/lstm _ 自动编码器 _ 分类器</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">用于罕见事件分类的 LSTM 自动编码器。通过…为 cran 2367/lstm _ auto encoder _ classifier 开发做出贡献</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">github.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk ms ow"/></div></div></a></div><h1 id="0dae" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">什么可以做得更好？</h1><p id="e4c3" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">在下一篇文章中，我们将学习调优自动编码器。我们会过去，</p><ul class=""><li id="de22" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">CNN LSTM 自动编码器，</li><li id="d17d" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">脱落层，</li><li id="65ec" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">LSTM 辍学(辍学和辍学)</li><li id="e891" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">高斯漏失层</li><li id="84d1" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">SELU 激活，以及</li><li id="1bdd" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">阿尔法辍学与 SELU 激活。</li></ul><h1 id="dbcc" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">结论</h1><p id="3a1b" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">这篇文章继续了[ <a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098"> 1 </a> ]中关于极端罕见事件二进制标记数据的工作。为了利用时间模式，LSTM 自动编码器被用来建立一个多元时间序列过程的罕见事件分类器。详细讨论了 LSTM 模型的数据预处理步骤。一个简单的 LSTM 自动编码器模型被训练并用于分类。发现密集自动编码器的精度有所提高。为了进一步改进，我们将在下一篇文章中探讨如何改进具有 Dropout 和其他技术的自动编码器。</p></div><div class="ab cl pl pm hu pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="ij ik il im in"><h1 id="755a" class="lq lr iq bd ls lt ps lv lw lx pt lz ma jw pu jx mc jz pv ka me kc pw kd mg mh bi translated">后续阅读</h1><p id="33c2" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">建议阅读<a class="ae lb" rel="noopener" target="_blank" href="/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352">逐步理解 LSTM 自动编码器层</a>以明确 LSTM 网络概念。</p><h1 id="d4e1" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">参考</h1><ol class=""><li id="5a88" class="lc ld iq kh b ki mz kl na ko px ks py kw pz la qa li lj lk bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">使用 Keras 中的自动编码器进行极端罕见事件分类</a></li><li id="72b0" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la qa li lj lk bi translated">Ranjan、m . Mustonen、k . pay nabar 和 k . pour AK(2018 年)。数据集:多元时间序列中的稀有事件分类。<a class="ae lb" href="https://arxiv.org/pdf/1809.10717.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="my"> arXiv 预印本 arXiv:1809.10717 </em> </a></li><li id="99b0" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la qa li lj lk bi translated"><a class="ae lb" href="https://www.kaggle.com/dimitreoliveira/time-series-forecasting-with-lstm-autoencoders" rel="noopener ugc nofollow" target="_blank">深度学习时间序列预测&amp; LSTM 自动编码器</a></li><li id="c41d" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la qa li lj lk bi translated">完整代码:<a class="ae lb" href="https://github.com/cran2367/lstm_autoencoder_classifier/blob/master/lstm_autoencoder_classifier.ipynb" rel="noopener ugc nofollow" target="_blank"> LSTM 自动编码器</a></li></ol><p id="4784" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="my">免责声明:这篇文章的范围仅限于构建 LSTM 自动编码器并将其用作罕见事件分类器的教程。通过网络调优，从业者有望获得更好的结果。这篇文章的目的是帮助数据科学家实现 LSTM 自动编码器。</em></p></div></div>    
</body>
</html>