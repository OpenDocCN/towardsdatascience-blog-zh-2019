<html>
<head>
<title>Reinforcement Learning — Cliff Walking Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习—悬崖行走实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-cliff-walking-implementation-e40ce98418d4?source=collection_archive---------4-----------------------#2019-06-22">https://towardsdatascience.com/reinforcement-learning-cliff-walking-implementation-e40ce98418d4?source=collection_archive---------4-----------------------#2019-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9b5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">开和关策略比较</h2></div><p id="0afc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习的本质是代理通过试验迭代更新其状态估计、动作对的方式(如果您不熟悉值迭代，请查看我之前的<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff">示例</a>)。在以前的帖子中，我一直在重复谈论 Q-learning 以及代理如何基于这种方法更新其 Q 值。事实上，除了在 Q-learning 中定义的更新方法之外，还有更多其他方法来更新状态、动作对的估计。在这篇文章中，我们将一起探索另一种叫做 SARSA 的方法，将这种方法与 Q-learning 进行比较，看看更新方法的不同如何影响代理的行为。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/2f6333e32dc123d22669dadfec99616d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wpEl8eANQiOht3v28csQg.jpeg"/></div></div></figure><p id="7e9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">先说<strong class="kh ir">时差</strong>，这是一个更新方法的核心。我们知道，在每次迭代或事件中，代理通过遵循策略(比如ϵ-greedy)采取行动来探索环境，并基于其最新观察(概括为状态-行动的值)，它通过将当前估计向最新观察调整一点来更新其当前估计，最新观察值和上次观察值之间的差异称为时间差异。正是从这个时间差中，我们的代理学习和更新自己。</p><p id="cac6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">时间差异的定义将各种方法区分开来。为了给你一个更具体的感觉，让我们直接进入算法定义，并检查不同之处。</p><h2 id="92c5" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">SARSA &amp; Q-学习</h2><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mh"><img src="../Images/5da0c0445baf862879f39295de7a31a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOL0d-fufdWdE8mC7Dk21Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">SARSA(from Sutton’s book)</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mm"><img src="../Images/f48cee9dbb7b0048676ea3d2c43dcf7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3R7eX6N6P1yFOVQwJ7jJGw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Q-learning(from Sutton’s book)</figcaption></figure><p id="79df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很明显，唯一的区别在于更新 Q 函数。在 SARSA 中(顺便说一下，SARSA 这个名字明确来自于代理的过程，即状态、动作、奖励、状态、动作…)，时间差异被定义为:</p><pre class="ld le lf lg gt mn mo mp mq aw mr bi"><span id="a5d8" class="lo lp iq mo b gy ms mt l mu mv">[R + Q(S', A') - Q(S, A)]</span></pre><p id="9631" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中下一状态、动作对的观察 Q 值直接有助于当前状态的更新。<strong class="kh ir"> SARSA 也称为 on-policy，因为更新过程与当前策略</strong>一致。</p><p id="4c72" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，Q-learning 中定义的时间差异是:</p><pre class="ld le lf lg gt mn mo mp mq aw mr bi"><span id="0840" class="lo lp iq mo b gy ms mt l mu mv">[R + max_a(Q(S', a)) - Q(S, A)]</span></pre><p id="3d50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中观察到的下一状态、动作对的 Q 值可能不会直接有助于当前状态的更新。<strong class="kh ir">Q-learning 总是使用下一个状态的最大值，在这种情况下，更新 Q 值所采取的状态、动作可能与当前策略不一致，因此被称为非策略方法</strong>。</p><p id="c1f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mw">事实上，这种差异会导致代理的不同行为。直觉认为，Q-learning(非策略)在价值评估方面更乐观，它总是假设在过程中采取最佳行动，这可能导致代理人更大胆的行动。而 SARSA(非策略)在值估计上更保守，这导致代理的储蓄器动作。</em></p><h1 id="1e19" class="mx lp iq bd lq my mz na lt nb nc nd lw jw ne jx lz jz nf ka mc kc ng kd mf nh bi translated">悬崖漫步</h1><p id="2962" class="pw-post-body-paragraph kf kg iq kh b ki ni jr kk kl nj ju kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">为了清楚地证明这一点，让我们来看一个例子，悬崖行走，它摘自<a class="ae lb" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mw">强化学习入门</em> </a>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nn"><img src="../Images/2117eff86bdaebdbe9d19b7ae4f1e3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52MwrYKyzQXuKZ88rqu70A.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Cliff Walking</figcaption></figure><p id="756e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个标准的不打折扣的、偶发的任务，有开始和目标状态，以及引起向上、向下、向右和向左运动的常见动作。除了进入标有悬崖的区域外，所有转换的奖励都是-1。进入该区域将获得最佳路径 100 的奖励，并使代理立即返回起点。</p><h1 id="8ad1" class="mx lp iq bd lq my mz na lt nb nc nd lw jw ne jx lz jz nf ka mc kc ng kd mf nh bi translated">履行</h1><p id="a4a3" class="pw-post-body-paragraph kf kg iq kh b ki ni jr kk kl nj ju kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">这是一个典型的二维棋盘游戏，所以棋盘设置与我在这里描述的例子<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff"/>基本相同。在接下来的章节中，我将主要强调 SARSA 和 Q-learning 的实现，以及这两种方法所产生的 agent 行为的比较。</p><h2 id="5df8" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">悬崖背景</h2><p id="ae5d" class="pw-post-body-paragraph kf kg iq kh b ki ni jr kk kl nj ju kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">在坚果壳中，我们将有一个 Cliff 类，它代表能够:</p><ol class=""><li id="0931" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">跟踪代理的当前位置</li><li id="08c4" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">给定一个动作，决定代理的下一个位置，并判断是否游戏结束</li><li id="1166" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">给予反馈作为奖励</li></ol><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="1a62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是课件中的主要功能。<code class="fe oe of og mo b">nxtPosition</code>函数接受一个动作，并返回代理在棋盘上的下一个位置，如果代理将其头部撞到墙上(到达边界)，它将保持在同一位置。<code class="fe oe of og mo b">giveReward</code>函数将奖励-1 赋予除悬崖区域之外的所有状态，悬崖区域的结果是奖励-100。</p><p id="bb33" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看我们实现的板。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oh"><img src="../Images/727e598ce0ac8bac71709161c6957c19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zpwb_n5V5BNNfbfCk5i8Tw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Cliff Walk Board</figcaption></figure><p id="634c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代理人从棋盘左端的符号<strong class="kh ir"> S </strong>开始，结束游戏的唯一方法是到达棋盘右端的符号<strong class="kh ir"> G </strong>。而<strong class="kh ir"> * </strong>代表悬崖区域。</p><h2 id="5755" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">博弈</h2><p id="bba2" class="pw-post-body-paragraph kf kg iq kh b ki ni jr kk kl nj ju kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">就玩游戏而言，我们将有一个代理类来代表我们的代理，在代理类中，有一个函数决定代理的动作，这也是ϵ-greedy 策略。(点击查看完整实施<a class="ae lb" href="https://github.com/MJeremy2017/RL/blob/master/CliffWalking/cliffWalking.py" rel="noopener ugc nofollow" target="_blank"/></p><p id="9883" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关键区别在于<code class="fe oe of og mo b">play</code>功能:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="b647" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每一集(游戏的每一轮)中，我们在列表<code class="fe oe of og mo b">self.states</code>中记录我们的代理的行动、状态和奖励。在游戏结束时，我们以相反的方式更新 Q 函数(T1)，如果方法是 SARSA(on-policy)，新更新的 <code class="fe oe of og mo b">reward</code> <strong class="kh ir">(本质上是</strong> <code class="fe oe of og mo b">Q(S', A')</code> <strong class="kh ir">)将直接应用于下一次更新，如果方法是 Q-learning，还有一个步骤</strong></p><pre class="ld le lf lg gt mn mo mp mq aw mr bi"><span id="6f87" class="lo lp iq mo b gy ms mt l mu mv">reward = np.max(list(self.state_actions[pos].values()))</span></pre><p id="5d93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">取该位置所有动作的最大值到下一轮更新</strong>。最大化操作塑造了代理的行为，并使其能够采取更冒险的行动。</p><h2 id="6de5" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">开/关策略比较</h2><p id="afe0" class="pw-post-body-paragraph kf kg iq kh b ki ni jr kk kl nj ju kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">我用探索率为 0.1 的两种方法运行了 500 轮，并采用了他们学习的最后一个<code class="fe oe of og mo b">(state, action)</code>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oi"><img src="../Images/219beb943d7cf71abe0ca239bbc4cdf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4iZRkk_XTzd0EfiCz5RhA.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Result of SARSA</figcaption></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oj"><img src="../Images/d28c1225a157e68609ddda095d370d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYY6Az5dXRS3_-yl86nX8Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Result of Q-learning</figcaption></figure><p id="9a56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我学到的结果与书中的最优结果略有不同，但这足以清楚地看出两者之间的差异。</p><h1 id="4dc3" class="mx lp iq bd lq my mz na lt nb nc nd lw jw ne jx lz jz nf ka mc kc ng kd mf nh bi translated">结论</h1><p id="6c01" class="pw-post-body-paragraph kf kg iq kh b ki ni jr kk kl nj ju kn ko nk kq kr ks nl ku kv kw nm ky kz la ij bi translated">结论第一部分将引用 Sutton 的书，该书完美地总结了两种方法之间的差异:</p><p id="e83a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mw"> Q-learning 学习沿着悬崖边缘行进的最优策略的值。不幸的是，这导致它偶尔会因为“ε-贪婪”的行动选择而掉下悬崖。另一方面，SARSA 将动作选择考虑在内，并通过网格的上部学习更长但更安全的路径。虽然 Q-learning 实际上是学习最优策略的值，但是它的在线性能比学习迂回策略的 SARSA 差。当然，如果ϵ逐渐减小，那么这两种方法都将渐近收敛于最优策略。</em></p><p id="8fe9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，请点击查看完整代码<a class="ae lb" href="https://github.com/MJeremy2017/RL/blob/master/CliffWalking/cliffWalking.py" rel="noopener ugc nofollow" target="_blank">。欢迎您投稿，如果您有任何问题或建议，请在下面发表评论！</a></p><p id="3fa8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考</strong></p><p id="65ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]<a class="ae lb" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></p></div></div>    
</body>
</html>