<html>
<head>
<title>Optimized Space Invaders using Deep Q-learning: An Implementation in Tensorflow 2.0.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度 Q 学习优化空间入侵者:Tensorflow 2.0 中的实现。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc?source=collection_archive---------13-----------------------#2019-12-27">https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc?source=collection_archive---------13-----------------------#2019-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e531" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索数据预处理的效果</h2></div><h1 id="2df7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="53f5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在过去几篇关于 GradientCrescent 的文章中，我们花了大量时间探索<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-illustrating-online-learning-through-temporal-differences-ec4833b6b06a" rel="noopener">在线学习</a>领域，这是一个高度反应性的强化学习算法家族，背后隐藏着许多通用人工智能的最新成就。在线学习属于<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff" rel="noopener">基于样本的学习</a>类方法，reliant 允许简单地通过重复观察来确定状态值，消除了对转换动态的需要。与它们的<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed" rel="noopener">离线对应方式</a>，<strong class="lc iu">不同，在线学习方法允许在环境事件期间对状态和动作的值进行增量更新，允许观察到持续的、增量的性能改进。</strong></p><p id="3d05" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">除了时间差异学习(TD ),我们还讨论了 Q-learning 的<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830" rel="noopener">理论</a>和<a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">实际实现</a>,这是 TD 的一种发展，旨在允许增量估计和状态-动作值的改进。Q-learning 因成为模拟游戏环境的强化学习方法的支柱而闻名，如在 OpenAI 的健身房中观察到的那些。因为我们已经在<a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">过去的文章</a>中涉及了 Q-learning 的理论方面，所以这里不再重复。</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mh mi l"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Q-learning powered Miss Pacman, a implemented in our <a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">previous article</a>.</figcaption></figure><p id="9140" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">在我们之前对 OpenAI 的 Pacman 小姐健身房环境的<a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">实现中，我们依赖于一组单个游戏帧的观察实例(状态)作为我们训练过程的输入。然而，这种方法存在部分缺陷，因为它没有考虑 Atari 游戏环境的许多特性，包括:</a></p><ul class=""><li id="9d2d" class="mn mo it lc b ld lx lg ly lj mp ln mq lr mr lv ms mt mu mv bi translated">经典 Atari 游戏中观察到的游戏环境的跳帧渲染。</li><li id="ba32" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">环境中存在多个快速移动的参与者。</li><li id="5c6d" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">在代理和环境中观察到的特定于帧的闪烁。</li></ul><p id="0320" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">总的来说，这些问题可能会大大降低代理的性能，因为一些数据实例实际上已经超出了领域，或者与实际的游戏环境完全无关。此外，这些问题只会随着更复杂的游戏环境和现实世界的应用(如自动驾驶)而变得更加复杂。我们在之前的实施中观察到，在培训过程中，这种情况表现为高水平的变化和绩效持平。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0b4ea2e8b7e39eafabbe13b76cd7382d.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*vOG3cyyzlcPX6gXx.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Reward versus training episodes for our Q-learning trained Miss Pacman agent, trained over 600+800 cycles.</figcaption></figure><p id="29af" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">为了克服这些问题，我们可以利用由<a class="ae lw" href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/" rel="noopener ugc nofollow" target="_blank"> Deepmind 团队</a>在 2015 年首次引入的几项技术。</p><ul class=""><li id="5c3f" class="mn mo it lc b ld lx lg ly lj mp ln mq lr mr lv ms mt mu mv bi translated"><strong class="lc iu">帧堆叠</strong>:将几个游戏帧连接在一起，为我们的游戏环境提供一个时间参考。</li><li id="f2e1" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated"><strong class="lc iu">帧合成:</strong>两个游戏帧的元素最大化，以提供一个运动参考，也克服了部分渲染的问题。</li></ul><p id="5d4d" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">让我们在更复杂的 Atari Space Invader 环境中实现检查这些技术的效果。</p><h1 id="0f7f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">实施</strong></h1><p id="2ca2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们的 Google 协作实现是利用 Tensorflow Core 用 Python 编写的，可以在<a class="ae lw" href="https://github.com/EXJUSTICE/GradientCrescent" rel="noopener ugc nofollow" target="_blank"> GradientCrescent Github 上找到。</a>我们已经使用新的<em class="ne"> compat </em>包将我们的代码转换为 TF2 兼容的。首先，让我们简要回顾一下 Q-learning 实现所需的操作。</p><ol class=""><li id="7306" class="mn mo it lc b ld lx lg ly lj mp ln mq lr mr lv nf mt mu mv bi translated"><strong class="lc iu">我们定义我们的深度 Q 学习神经网络</strong>。这是一个 CNN，它拍摄游戏中的屏幕图像，并输出 Ms-Pacman gamespace 中每个动作的概率，或 Q 值。为了获得概率张量，我们在最后一层不包括任何激活函数。</li><li id="b87e" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">由于 Q-learning 要求我们了解当前和下一个状态，我们需要从数据生成开始。我们将表示初始状态<em class="ne"> s </em>的游戏空间的预处理输入图像输入到网络中，并获取动作的初始概率分布，或 Q 值。在训练之前，这些值将是随机的和次优的。请注意，我们的预处理现在还包括堆叠和合成。</li><li id="0c38" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">利用我们的概率张量，我们然后<strong class="lc iu">使用 argmax()函数选择具有当前最高概率</strong>的动作，并使用它来构建ε贪婪策略。</li><li id="57d6" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">使用我们的策略，我们将选择动作<em class="ne"> a </em>，并评估我们在健身房环境中的决定，以让<strong class="lc iu">接收关于新状态<em class="ne">s’</em>的信息、奖励<em class="ne"> r </em> </strong>，以及该集是否已经结束。</li><li id="b4ff" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">我们以列表形式<s>将该信息组合存储在一个缓冲区中，并重复步骤 2-4 预设次数，以建立一个足够大的缓冲区数据集。</s></li><li id="1eae" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">一旦步骤 5 完成，我们转到<strong class="lc iu">生成损失计算所需的目标<em class="ne"> y </em>值<em class="ne">R’</em>和<em class="ne">A’</em></strong>。虽然前者只是从<em class="ne"> R </em>中减去，但我们通过将<em class="ne">S’</em>输入到我们的网络中来获得 A’。</li><li id="2b36" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">所有的组件都准备好了，我们就可以<strong class="lc iu">计算训练网络的损耗了。</strong></li><li id="6d66" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv nf mt mu mv bi translated">培训结束后，我们将通过图形和演示来评估代理的表现。</li></ol><p id="7a94" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">作为参考，让我们首先演示使用普通数据输入方法的结果，这与我们之前为 Pacman 小姐实施的<a class="ae lw" rel="noopener" target="_blank" href="/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c">中观察到的结果基本相同。</a>在对我们的代理人进行了 800 集的训练后，我们观察到以下的报酬分布。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a2161e972e6cd8163cbbb9c93c9d1952.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*HvOhAUGVjVshV36r3jEjhg.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Reward distribution for the vanilla data input approach for the Space Invaders environment.</figcaption></figure><p id="7314" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">请注意性能的变化如何表现出高度的变化，在 650 次发作后观察到非常有限的改善。</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="dd8b" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">同样，我们代理的性能也不太好，几乎没有检测到任何逃避行为。如果你仔细观察，你会注意到出射和入射激光轨迹的闪烁——这是游戏环境中有意的一部分，导致某些帧中根本没有投射物，或者只有一组投射物可见。<strong class="lc iu">这意味着我们输入数据的元素具有高度误导性，并对代理绩效产生负面影响。</strong></p><p id="72f7" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">让我们检查一下改进后的实现。</p><p id="6bf3" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们首先导入所有必要的包，包括 OpenAI gym 环境和 Tensorflow 核心。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="2a13" class="nm kj it ni b gy nn no l np nq">import numpy as np</span><span id="7380" class="nm kj it ni b gy nr no l np nq">import gym</span><span id="063d" class="nm kj it ni b gy nr no l np nq">import tensorflow as tf</span><span id="cfec" class="nm kj it ni b gy nr no l np nq">from tensorflow.contrib.layers import flatten, conv2d, fully_connected</span><span id="54ce" class="nm kj it ni b gy nr no l np nq">from collections import deque, Counter</span><span id="ee60" class="nm kj it ni b gy nr no l np nq">import random</span><span id="f029" class="nm kj it ni b gy nr no l np nq">from datetime import datetime</span></pre><p id="4462" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">接下来，我们定义一个预处理函数，从我们的健身房环境中裁剪图像，并将它们转换成一维张量。在我们的<a class="ae lw" href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff" rel="noopener"> Pong 自动化实现</a>中，我们已经看到了这一点。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="71a1" class="nm kj it ni b gy nn no l np nq">def preprocess_observation(obs):</span><span id="0894" class="nm kj it ni b gy nr no l np nq">  # Crop and resize the image</span><span id="9010" class="nm kj it ni b gy nr no l np nq">  img = obs[25:201:2, ::2]</span><span id="9842" class="nm kj it ni b gy nr no l np nq">  # Convert the image to greyscale</span><span id="d682" class="nm kj it ni b gy nr no l np nq">  img = img.mean(axis=2)</span><span id="7b66" class="nm kj it ni b gy nr no l np nq">  # Improve image contrast</span><span id="e16c" class="nm kj it ni b gy nr no l np nq">  img[img==color] = 0</span><span id="9de4" class="nm kj it ni b gy nr no l np nq">  # Next we normalize the image from -1 to +1</span><span id="2443" class="nm kj it ni b gy nr no l np nq">  img = (img - 128) / 128 - 1</span><span id="2fe2" class="nm kj it ni b gy nr no l np nq">  return img.reshape(88,80)</span></pre><p id="47de" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">接下来，让我们初始化健身房环境，检查几个游戏画面，并了解 gamespace 中可用的 9 个动作。当然，我们的代理人无法获得这些信息。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="91f5" class="nm kj it ni b gy nn no l np nq">env = gym.make(“SpaceInvaders-v0”)</span><span id="cef9" class="nm kj it ni b gy nr no l np nq">n_outputs = env.action_space.n</span><span id="fb5c" class="nm kj it ni b gy nr no l np nq">print(n_outputs)</span><span id="b5dc" class="nm kj it ni b gy nr no l np nq">print(env.env.get_action_meanings())</span><span id="f8fa" class="nm kj it ni b gy nr no l np nq">observation = env.reset()</span><span id="ecc1" class="nm kj it ni b gy nr no l np nq">import tensorflow as tf</span><span id="567a" class="nm kj it ni b gy nr no l np nq">import matplotlib.pyplot as plt</span><span id="8dcb" class="nm kj it ni b gy nr no l np nq">for i in range(22):</span><span id="61a6" class="nm kj it ni b gy nr no l np nq">if i &gt; 20:</span><span id="0a3d" class="nm kj it ni b gy nr no l np nq">plt.imshow(observation)</span><span id="79d3" class="nm kj it ni b gy nr no l np nq">plt.show()</span><span id="f023" class="nm kj it ni b gy nr no l np nq">observation, _, _, _ = env.step(1)</span></pre><p id="d231" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">您应该遵守以下几点:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi ns"><img src="../Images/7349752c219998c2eb96790cb13f9e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xfuHhAjRhcQFGke7QkY-EQ.png"/></div></div></figure><p id="8962" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们可以借此机会比较我们的原始和预处理输入图像:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi nx"><img src="../Images/f1670382de51e6d84656650500a3978b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CGFsqaFbj-cJzVTBJmamYQ.png"/></div></div></figure><p id="3ef3" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">接下来，我们将输入堆叠和输入组合引入预处理管道。在新的一集里，我们从获取两个输入帧开始，并返回这两个帧的元素式最大总和<em class="ne"> maxframe </em>(注意，从技术上讲这是不必要的，因为这两个帧是相同的，但这是一种很好的实践)。堆叠的帧存储在队列中，当引入新的条目时，队列会自动删除旧的条目。最初，我们复制预处理的 maxframe 来填充我们的 deque。随着剧集的进展，我们通过获取新的帧来创建新的<em class="ne"> maxframes </em>，将它与我们的 dequee 中最近的条目进行元素式最大求和，然后将新的 maxframe 附加到我们的 dequee。然后，我们在流程的最后堆叠这些帧。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="9a2b" class="nm kj it ni b gy nn no l np nq">stack_size = 4 # We stack 4 composite frames in total</span><span id="1dbd" class="nm kj it ni b gy nr no l np nq"># Initialize deque with zero-images one array for each image. Deque is a special kind of queue that deletes last entry when new entry comes in</span><span id="15cc" class="nm kj it ni b gy nr no l np nq">stacked_frames = deque([np.zeros((88,80), dtype=np.int) for i in range(stack_size)], maxlen=4)</span><span id="1a2a" class="nm kj it ni b gy nr no l np nq">def stack_frames(stacked_frames, state, is_new_episode):</span><span id="07eb" class="nm kj it ni b gy nr no l np nq"># Preprocess frame</span><span id="e969" class="nm kj it ni b gy nr no l np nq">frame = preprocess_observation(state)</span><span id="53eb" class="nm kj it ni b gy nr no l np nq">  if is_new_episode:</span><span id="7a49" class="nm kj it ni b gy nr no l np nq">    # Clear our stacked_frames</span><span id="53a2" class="nm kj it ni b gy nr no l np nq">    stacked_frames = deque([np.zeros((88,80), dtype=np.int) for i in range(stack_size)], maxlen=4)</span><span id="c1ea" class="nm kj it ni b gy nr no l np nq">    # Because we’re in a new episode, copy the same frame 4x, apply elementwise maxima</span><span id="8ad0" class="nm kj it ni b gy nr no l np nq">    maxframe = np.maximum(frame,frame)</span><span id="017a" class="nm kj it ni b gy nr no l np nq">    stacked_frames.append(maxframe)</span><span id="7762" class="nm kj it ni b gy nr no l np nq">    stacked_frames.append(maxframe)</span><span id="62ad" class="nm kj it ni b gy nr no l np nq">    stacked_frames.append(maxframe)</span><span id="adc2" class="nm kj it ni b gy nr no l np nq">    stacked_frames.append(maxframe)</span><span id="9c04" class="nm kj it ni b gy nr no l np nq">  # Stack the frames</span><span id="2254" class="nm kj it ni b gy nr no l np nq">     stacked_state = np.stack(stacked_frames, axis=2)</span><span id="3aa4" class="nm kj it ni b gy nr no l np nq">  else:</span><span id="21cb" class="nm kj it ni b gy nr no l np nq">  #Since deque append adds t right, we can fetch rightmost element</span><span id="9eb7" class="nm kj it ni b gy nr no l np nq">     maxframe=np.maximum(stacked_frames[-1],frame)</span><span id="448e" class="nm kj it ni b gy nr no l np nq">  # Append frame to deque, automatically removes the oldest frame</span><span id="2701" class="nm kj it ni b gy nr no l np nq">     stacked_frames.append(maxframe)</span><span id="a7b4" class="nm kj it ni b gy nr no l np nq">  # Build the stacked state (first dimension specifies different frames)</span><span id="2543" class="nm kj it ni b gy nr no l np nq">     stacked_state = np.stack(stacked_frames, axis=2)</span><span id="dc36" class="nm kj it ni b gy nr no l np nq">  return stacked_state, stacked_frames</span></pre><p id="f217" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">接下来，让我们定义我们的模型，一个深度 Q 网络。这本质上是一个三层卷积网络，它获取预处理的输入图像，展平并将其馈送到一个全连接层，并输出在游戏空间中采取每个行动的概率。如前所述，这里没有激活层，因为激活层的存在会导致二进制输出分布。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="6980" class="nm kj it ni b gy nn no l np nq"><strong class="ni iu">def q_network(X, name_scope):</strong></span><span id="35e9" class="nm kj it ni b gy nr no l np nq"># Initialize layers</span><span id="43b5" class="nm kj it ni b gy nr no l np nq">initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)</span><span id="5ed5" class="nm kj it ni b gy nr no l np nq">with tf.compat.v1.variable_scope(name_scope) as scope:</span><span id="aec9" class="nm kj it ni b gy nr no l np nq"># initialize the convolutional layers</span><span id="63d2" class="nm kj it ni b gy nr no l np nq">layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding=’SAME’, weights_initializer=initializer)</span><span id="4e15" class="nm kj it ni b gy nr no l np nq">tf.compat.v1.summary.histogram(‘layer_1’,layer_1)</span><span id="0d60" class="nm kj it ni b gy nr no l np nq">layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4),    stride=2, padding=’SAME’, weights_initializer=initializer)</span><span id="95f6" class="nm kj it ni b gy nr no l np nq">tf.compat.v1.summary.histogram(‘layer_2’,layer_2)</span><span id="42e4" class="nm kj it ni b gy nr no l np nq">layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding=’SAME’, weights_initializer=initializer)</span><span id="00d5" class="nm kj it ni b gy nr no l np nq">tf.compat.v1.summary.histogram(‘layer_3’,layer_3)</span><span id="5693" class="nm kj it ni b gy nr no l np nq">flat = flatten(layer_3)</span><span id="6f10" class="nm kj it ni b gy nr no l np nq">fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)</span><span id="181a" class="nm kj it ni b gy nr no l np nq">tf.compat.v1.summary.histogram(‘fc’,fc)</span><span id="a758" class="nm kj it ni b gy nr no l np nq">#Add final output layer</span><span id="f4fb" class="nm kj it ni b gy nr no l np nq">output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)</span><span id="95f2" class="nm kj it ni b gy nr no l np nq">tf.compat.v1.summary.histogram(‘output’,output)</span><span id="dc3d" class="nm kj it ni b gy nr no l np nq">vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}</span><span id="a380" class="nm kj it ni b gy nr no l np nq">#Return both variables and outputs together</span><span id="9bd4" class="nm kj it ni b gy nr no l np nq">return vars, output</span></pre><p id="c9aa" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">让我们也借此机会为我们的模型和训练过程定义超参数。注意，由于我们的堆叠框架，X_shape 现在是<em class="ne">(无，88，80，4) </em>。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="502d" class="nm kj it ni b gy nn no l np nq">num_episodes = 800</span><span id="4118" class="nm kj it ni b gy nr no l np nq">batch_size = 48</span><span id="0987" class="nm kj it ni b gy nr no l np nq">input_shape = (None, 88, 80, 1)</span><span id="ddc6" class="nm kj it ni b gy nr no l np nq">learning_rate = 0.001</span><span id="8243" class="nm kj it ni b gy nr no l np nq">X_shape = (None, 88, 80, 4)</span><span id="f822" class="nm kj it ni b gy nr no l np nq">discount_factor = 0.97</span><span id="c180" class="nm kj it ni b gy nr no l np nq">global_step = 0</span><span id="0ccb" class="nm kj it ni b gy nr no l np nq">copy_steps = 100</span><span id="2186" class="nm kj it ni b gy nr no l np nq">steps_train = 4</span><span id="7150" class="nm kj it ni b gy nr no l np nq">start_steps = 2000</span></pre><p id="bf4e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">回想一下，Q-learning 要求我们选择具有最高行动值的行动。为了确保我们仍然访问每一个可能的状态-行为组合，我们将让我们的代理遵循一个ε贪婪策略，探索率为 5%。我们将这个探索率设置为随时间衰减，因为我们最终假设所有的组合都已经被探索过了——在那个点之后的任何探索只会导致次优行动的强制选择。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="ed63" class="nm kj it ni b gy nn no l np nq">epsilon = 0.5</span><span id="42a8" class="nm kj it ni b gy nr no l np nq">eps_min = 0.05</span><span id="c4d5" class="nm kj it ni b gy nr no l np nq">eps_max = 1.0</span><span id="a6e3" class="nm kj it ni b gy nr no l np nq">eps_decay_steps = 500000</span><span id="dca2" class="nm kj it ni b gy nr no l np nq">#</span><span id="1cfc" class="nm kj it ni b gy nr no l np nq"><strong class="ni iu">def epsilon_greedy(action, step):</strong></span><span id="10ac" class="nm kj it ni b gy nr no l np nq">  p = np.random.random(1).squeeze() #1D entries returned using squeeze</span><span id="5321" class="nm kj it ni b gy nr no l np nq">  epsilon = max(eps_min, eps_max — (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps</span><span id="bfa7" class="nm kj it ni b gy nr no l np nq">  if np.random.rand() &lt; epsilon:</span><span id="deac" class="nm kj it ni b gy nr no l np nq">    return np.random.randint(n_outputs)</span><span id="7037" class="nm kj it ni b gy nr no l np nq">  else:</span><span id="ebfc" class="nm kj it ni b gy nr no l np nq">    return action</span></pre><p id="ced5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">回想上面的等式，Q-learning 的更新函数要求如下:</p><ul class=""><li id="f046" class="mn mo it lc b ld lx lg ly lj mp ln mq lr mr lv ms mt mu mv bi translated">当前状态<em class="ne"> s </em></li><li id="6abf" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">当前动作<em class="ne">一个</em></li><li id="0f3c" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">当前动作后的奖励<em class="ne"> r </em></li><li id="f19e" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">下一个状态<em class="ne">s’</em></li><li id="eaa6" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">下一个动作<em class="ne">a’</em></li></ul><p id="dc0e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">为了以有意义的数量提供这些参数，我们需要按照一组参数评估我们当前的策略，并将所有变量存储在一个缓冲区中，我们将在训练期间从该缓冲区中提取迷你批次中的数据。让我们继续创建我们的缓冲区和一个简单的采样函数:</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="c2ce" class="nm kj it ni b gy nn no l np nq">buffer_len = 20000</span><span id="2a1b" class="nm kj it ni b gy nr no l np nq">#Buffer is made from a deque — double ended queue</span><span id="2752" class="nm kj it ni b gy nr no l np nq">exp_buffer = deque(maxlen=buffer_len)</span><span id="ab47" class="nm kj it ni b gy nr no l np nq"><strong class="ni iu">def sample_memories(batch_size):</strong></span><span id="96f1" class="nm kj it ni b gy nr no l np nq">  perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]</span><span id="b2b0" class="nm kj it ni b gy nr no l np nq">  mem = np.array(exp_buffer)[perm_batch]</span><span id="551c" class="nm kj it ni b gy nr no l np nq">  return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]</span></pre><p id="f605" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">接下来，让我们将原始网络的权重参数复制到目标网络中。这种双网络方法允许我们在使用现有策略的训练过程中生成数据，同时仍然为下一次策略迭代优化我们的参数。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="31b6" class="nm kj it ni b gy nn no l np nq"># we build our Q network, which takes the input X and generates Q values for all the actions in the state</span><span id="ab5a" class="nm kj it ni b gy nr no l np nq">mainQ, mainQ_outputs = q_network(X, ‘mainQ’)</span><span id="dc4c" class="nm kj it ni b gy nr no l np nq"># similarly we build our target Q network, for policy evaluation</span><span id="8484" class="nm kj it ni b gy nr no l np nq">targetQ, targetQ_outputs = q_network(X, ‘targetQ’)</span><span id="5f08" class="nm kj it ni b gy nr no l np nq">copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]</span><span id="4bb9" class="nm kj it ni b gy nr no l np nq">copy_target_to_main = tf.group(*copy_op)</span></pre><p id="cfda" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">最后，我们还将定义我们的损失。这就是我们的目标动作(具有最高动作值)和我们的预测动作的平方差。我们将使用 ADAM 优化器来最大限度地减少我们在训练中的损失。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="0d18" class="nm kj it ni b gy nn no l np nq"># define a placeholder for our output i.e action</span><span id="9fef" class="nm kj it ni b gy nr no l np nq">y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))</span><span id="86ad" class="nm kj it ni b gy nr no l np nq"># now we calculate the loss which is the difference between actual value and predicted value</span><span id="ce32" class="nm kj it ni b gy nr no l np nq">loss = tf.reduce_mean(input_tensor=tf.square(y — Q_action))</span><span id="85bc" class="nm kj it ni b gy nr no l np nq"># we use adam optimizer for minimizing the loss</span><span id="cb6d" class="nm kj it ni b gy nr no l np nq">optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)</span><span id="d887" class="nm kj it ni b gy nr no l np nq">training_op = optimizer.minimize(loss)</span><span id="b88d" class="nm kj it ni b gy nr no l np nq">init = tf.compat.v1.global_variables_initializer()</span><span id="170c" class="nm kj it ni b gy nr no l np nq">loss_summary = tf.compat.v1.summary.scalar(‘LOSS’, loss)</span><span id="4e8f" class="nm kj it ni b gy nr no l np nq">merge_summary = tf.compat.v1.summary.merge_all()</span><span id="100a" class="nm kj it ni b gy nr no l np nq">file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())</span></pre><p id="9fea" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">定义好所有代码后，让我们运行我们的网络并检查培训过程。我们已经在最初的总结中定义了大部分，但是让我们为后代回忆一下。</p><ul class=""><li id="3635" class="mn mo it lc b ld lx lg ly lj mp ln mq lr mr lv ms mt mu mv bi translated">对于每个时期，在使用ε-贪婪策略选择下一个动作之前，我们将输入图像堆栈输入到我们的网络中，以生成可用动作的概率分布</li><li id="48ff" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">然后，我们将它输入到网络中，获取下一个状态和相应奖励的信息，并将其存储到我们的缓冲区中。我们更新我们的堆栈，并通过一些预定义的步骤重复这一过程。</li><li id="17c7" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">在我们的缓冲区足够大之后，我们将下一个状态输入到我们的网络中，以便获得下一个动作。我们还通过贴现当前的奖励来计算下一个奖励</li><li id="dc2a" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">我们通过 Q 学习更新函数生成我们的目标 y 值，并训练我们的网络。</li><li id="7c79" class="mn mo it lc b ld mw lg mx lj my ln mz lr na lv ms mt mu mv bi translated">通过最小化训练损失，我们更新网络权重参数，以便为下一个策略输出改进的状态-动作值。</li></ul><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="22ad" class="nm kj it ni b gy nn no l np nq">with tf.compat.v1.Session() as sess:</span><span id="19ec" class="nm kj it ni b gy nr no l np nq">  init.run()</span><span id="3c8e" class="nm kj it ni b gy nr no l np nq">  # for each episode<br/>  history = []</span><span id="e28f" class="nm kj it ni b gy nr no l np nq">  for i in range(num_episodes):</span><span id="2b85" class="nm kj it ni b gy nr no l np nq">    done = False</span><span id="5012" class="nm kj it ni b gy nr no l np nq">    obs = env.reset()</span><span id="2aa1" class="nm kj it ni b gy nr no l np nq">    epoch = 0</span><span id="2759" class="nm kj it ni b gy nr no l np nq">    episodic_reward = 0</span><span id="c9e0" class="nm kj it ni b gy nr no l np nq">     actions_counter = Counter()</span><span id="da8d" class="nm kj it ni b gy nr no l np nq">    episodic_loss = []</span><span id="ff4c" class="nm kj it ni b gy nr no l np nq">    #First step, preprocess + initialize stack</span><span id="a90d" class="nm kj it ni b gy nr no l np nq">    obs,stacked_frames= stack_frames(stacked_frames,obs,True)</span><span id="4e0e" class="nm kj it ni b gy nr no l np nq">    # while the state is not the terminal state<br/>    while not done:</span><span id="4cff" class="nm kj it ni b gy nr no l np nq">    #Data generation using the untrained network</span><span id="7b98" class="nm kj it ni b gy nr no l np nq">    # feed the game screen and get the Q values for each action</span><span id="4c44" class="nm kj it ni b gy nr no l np nq">    actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})</span><span id="f35a" class="nm kj it ni b gy nr no l np nq">    # get the action<br/>    action = np.argmax(actions, axis=-1)</span><span id="0f9d" class="nm kj it ni b gy nr no l np nq">    actions_counter[str(action)] += 1</span><span id="9b43" class="nm kj it ni b gy nr no l np nq">    # select the action using epsilon greedy policy<br/>    <br/>    action = epsilon_greedy(action, global_step)</span><span id="2575" class="nm kj it ni b gy nr no l np nq">    # now perform the action and move to the next state, next_obs, receive reward</span><span id="adb0" class="nm kj it ni b gy nr no l np nq">    next_obs, reward, done, _ = env.step(action)</span><span id="d46a" class="nm kj it ni b gy nr no l np nq">    #Updated stacked frames with new episode</span><span id="5ddd" class="nm kj it ni b gy nr no l np nq">    next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)</span><span id="7c4a" class="nm kj it ni b gy nr no l np nq">    # Store this transition as an experience in the replay buffer! Quite important</span><span id="8cf6" class="nm kj it ni b gy nr no l np nq">    exp_buffer.append([obs, action, next_obs, reward, done])</span><span id="abfa" class="nm kj it ni b gy nr no l np nq">    # After certain steps, we train our Q network with samples from the experience replay buffer</span><span id="559a" class="nm kj it ni b gy nr no l np nq">    if global_step % steps_train == 0 and global_step &gt; start_steps:</span><span id="2216" class="nm kj it ni b gy nr no l np nq">      #Our buffer should already contain everything preprocessed and stacked</span><span id="848c" class="nm kj it ni b gy nr no l np nq">      o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)</span><span id="df9b" class="nm kj it ni b gy nr no l np nq">      # states</span><span id="39d4" class="nm kj it ni b gy nr no l np nq">      o_obs = [x for x in o_obs]</span><span id="9822" class="nm kj it ni b gy nr no l np nq">      # next states</span><span id="bffc" class="nm kj it ni b gy nr no l np nq">      o_next_obs = [x for x in o_next_obs]</span><span id="0f8a" class="nm kj it ni b gy nr no l np nq">      # next actions</span><span id="993a" class="nm kj it ni b gy nr no l np nq">      next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})</span><span id="961f" class="nm kj it ni b gy nr no l np nq">      # discounted reward: these are our Y-values</span><span id="de59" class="nm kj it ni b gy nr no l np nq">      y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)</span><span id="abfb" class="nm kj it ni b gy nr no l np nq">      # merge all summaries and write to the file</span><span id="4358" class="nm kj it ni b gy nr no l np nq">      mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})</span><span id="1abe" class="nm kj it ni b gy nr no l np nq">      file_writer.add_summary(mrg_summary, global_step)</span><span id="2129" class="nm kj it ni b gy nr no l np nq">      # To calculate the loss, we run the previously defined functions mentioned while feeding inputs</span><span id="a9f3" class="nm kj it ni b gy nr no l np nq">      train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})</span><span id="6e76" class="nm kj it ni b gy nr no l np nq">      episodic_loss.append(train_loss)</span><span id="9091" class="nm kj it ni b gy nr no l np nq">      # after some interval we copy our main Q network weights to target Q network</span><span id="ed8d" class="nm kj it ni b gy nr no l np nq">   if (global_step+1) % copy_steps == 0 and global_step &gt; start_steps:</span><span id="6476" class="nm kj it ni b gy nr no l np nq">      copy_target_to_main.run()</span><span id="8bfc" class="nm kj it ni b gy nr no l np nq">   obs = next_obs</span><span id="1edf" class="nm kj it ni b gy nr no l np nq">   epoch += 1</span><span id="25a7" class="nm kj it ni b gy nr no l np nq">   global_step += 1</span><span id="c02f" class="nm kj it ni b gy nr no l np nq">   episodic_reward += reward</span><span id="d199" class="nm kj it ni b gy nr no l np nq">next_obs=np.zeros(obs.shape)</span><span id="62fb" class="nm kj it ni b gy nr no l np nq">exp_buffer.append([obs, action, next_obs, reward, done])</span><span id="8187" class="nm kj it ni b gy nr no l np nq">obs= env.reset()</span><span id="b8ac" class="nm kj it ni b gy nr no l np nq">obs,stacked_frames= stack_frames(stacked_frames,obs,True)</span><span id="1464" class="nm kj it ni b gy nr no l np nq">history.append(episodic_reward)</span><span id="2ef8" class="nm kj it ni b gy nr no l np nq">print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,"Episode number:", len(history))</span></pre><p id="b7d5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">一旦训练完成，我们就可以根据增量情节绘制奖励分布图。前 800 集如下所示:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6fe46132a7aa75467491a7ed4749ab11.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*2rF9Mp2-a6cSgVJg6-D4vg.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Reward distribution for the stacked and composited approach in the Space Invaders environment.</figcaption></figure><p id="1cea" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">请注意奖励分布的核心变化是如何显著减少的，从而可以观察到更加一致的剧集间分布，并且表现的增加在统计上变得更加显著。从 550 集开始可以观察到性能的明显提高，比普通数据方法早了整整 100 集，验证了我们的假设。</p><p id="dba6" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">为了在实验室环境的限制下评估我们的结果，我们可以录制整个情节，并使用基于 IPython 库的包装在虚拟显示器中显示:</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="4983" class="nm kj it ni b gy nn no l np nq">“””Utility functions to enable video recording of gym environment and displaying it. To enable video, just do “env = wrap_env(env)””“”</span><span id="b1a1" class="nm kj it ni b gy nr no l np nq"><strong class="ni iu">def show_video():</strong></span><span id="c5b1" class="nm kj it ni b gy nr no l np nq">mp4list = glob.glob(‘video/*.mp4’)</span><span id="9277" class="nm kj it ni b gy nr no l np nq">if len(mp4list) &gt; 0:</span><span id="ec74" class="nm kj it ni b gy nr no l np nq">mp4 = mp4list[0]</span><span id="6c0e" class="nm kj it ni b gy nr no l np nq">video = io.open(mp4, ‘r+b’).read()</span><span id="3229" class="nm kj it ni b gy nr no l np nq">encoded = base64.b64encode(video)</span><span id="480d" class="nm kj it ni b gy nr no l np nq">ipythondisplay.display(HTML(data=’’’&lt;video alt=”test” autoplay</span><span id="9765" class="nm kj it ni b gy nr no l np nq">loop controls style=”height: 400px;”&gt;</span><span id="f798" class="nm kj it ni b gy nr no l np nq">&lt;source src=”data:video/mp4;base64,{0}” type=”video/mp4" /&gt;</span><span id="c1a4" class="nm kj it ni b gy nr no l np nq">&lt;/video&gt;’’’.format(encoded.decode(‘ascii’))))</span><span id="1ede" class="nm kj it ni b gy nr no l np nq">else:</span><span id="c719" class="nm kj it ni b gy nr no l np nq">print(“Could not find video”)    <br/><strong class="ni iu">def wrap_env(env):</strong></span><span id="6cef" class="nm kj it ni b gy nr no l np nq">env = Monitor(env, ‘./video’, force=True)</span><span id="bcda" class="nm kj it ni b gy nr no l np nq">return env</span></pre><p id="93b9" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">然后，我们使用我们的模型运行一个新的环境会话，并记录它。</p><pre class="mc md me mf gt nh ni nj nk aw nl bi"><span id="39eb" class="nm kj it ni b gy nn no l np nq">Evaluate model on openAi GYM</span><span id="fe69" class="nm kj it ni b gy nr no l np nq">environment = wrap_env(gym.make('SpaceInvaders-v0'))</span><span id="7127" class="nm kj it ni b gy nr no l np nq">done = False</span><span id="e3f7" class="nm kj it ni b gy nr no l np nq">observation = environment.reset()</span><span id="f878" class="nm kj it ni b gy nr no l np nq">new_observation = observation</span><span id="d5c1" class="nm kj it ni b gy nr no l np nq">prev_input = None</span><span id="66da" class="nm kj it ni b gy nr no l np nq">with tf.compat.v1.Session() as sess:</span><span id="7c5c" class="nm kj it ni b gy nr no l np nq">  init.run()</span><span id="69bf" class="nm kj it ni b gy nr no l np nq">  observation, stacked_frames = stack_frames(stacked_frames, observation, True)</span><span id="7872" class="nm kj it ni b gy nr no l np nq">  while True:</span><span id="6331" class="nm kj it ni b gy nr no l np nq">    #set input to network to be difference image</span><span id="cbf5" class="nm kj it ni b gy nr no l np nq">    # feed the game screen and get the Q values for each action</span><span id="7e22" class="nm kj it ni b gy nr no l np nq">    actions = mainQ_outputs.eval(feed_dict={X:[observation], in_training_mode:False})</span><span id="8ff0" class="nm kj it ni b gy nr no l np nq">    # get the action</span><span id="7047" class="nm kj it ni b gy nr no l np nq">    action = np.argmax(actions, axis=-1)</span><span id="10cc" class="nm kj it ni b gy nr no l np nq">    actions_counter[str(action)] += 1</span><span id="fcb6" class="nm kj it ni b gy nr no l np nq">    # select the action using epsilon greedy policy</span><span id="8cae" class="nm kj it ni b gy nr no l np nq">    action = epsilon_greedy(action, global_step)</span><span id="52e6" class="nm kj it ni b gy nr no l np nq">    environment.render()</span><span id="1dca" class="nm kj it ni b gy nr no l np nq">    new_observation, stacked_frames = stack_frames(stacked_frames, new_observation, False)</span><span id="4fb7" class="nm kj it ni b gy nr no l np nq">    observation = new_observation</span><span id="6e4b" class="nm kj it ni b gy nr no l np nq">    # now perform the action and move to the next state, next_obs, receive reward</span><span id="afdd" class="nm kj it ni b gy nr no l np nq">    new_observation, reward, done, _ = environment.step(action)</span><span id="35de" class="nm kj it ni b gy nr no l np nq">    if done:</span><span id="18f4" class="nm kj it ni b gy nr no l np nq">      break</span><span id="e20c" class="nm kj it ni b gy nr no l np nq">environment.close()</span><span id="b0f2" class="nm kj it ni b gy nr no l np nq">show_video()</span></pre><p id="c700" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们来考察几轮玩法。</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mh mi l"/></div></figure><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="cbd7" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们的特工已经学会了防守和进攻，有效地利用掩护和躲避。这两个事件之间行为的巨大差异可以归因于 Q 学习的工作方式——早先选择的行为获得了 Q 值，这往往有利于ε贪婪策略。随着进一步的训练，我们希望这两种行为会趋于一致。</p><p id="7797" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">这就结束了优化 Q-learning 的介绍。在我们的下一篇文章中，我们将带着我们所学到的一切，从 Atari 的世界继续前进，去解决世界上最著名的 FPS 游戏之一。</p><p id="8e8f" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们希望你喜欢这篇文章，并希望你查看 GradientCrescent 上的许多其他文章，涵盖人工智能的应用和理论方面。为了保持对<a class="ae lw" href="https://medium.com/gradientcrescent" rel="noopener"> GradientCrescent </a>的最新更新，请考虑关注该出版物并关注我们的 Github 资源库。</p><p id="6fa1" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">参考文献</strong></p><p id="947e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">萨顿等人。强化学习</p><p id="7d59" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">怀特等人。阿尔伯塔大学强化学习基础</p><p id="867b" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">席尔瓦等人。阿尔，强化学习，UCL</p><p id="7022" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">Ravichandiran 等人。al，用 Python 实践强化学习</p><p id="ecf5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">Takeshi 等人。艾尔，<a class="ae lw" href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/" rel="noopener ugc nofollow" target="_blank"> Github </a></p></div></div>    
</body>
</html>