<html>
<head>
<title>Evaluating a Machine Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估机器学习算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluating-a-machine-learning-algorithm-81746c947ad3?source=collection_archive---------24-----------------------#2019-10-19">https://towardsdatascience.com/evaluating-a-machine-learning-algorithm-81746c947ad3?source=collection_archive---------24-----------------------#2019-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eaf5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“下一步做什么？”的工具箱</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3deb731d5a6260c968a00ef88abc9b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5h84jPy1zYiSi5DVIWtPYA.jpeg"/></div></div></figure><p id="a48b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated">随着易于使用的机器学习库的丰富，应用它们并在大多数情况下实现 80%以上的预测准确率通常是很有吸引力的。但是，<strong class="kt ir">‘下一步该怎么办？’</strong>这个问题困扰着我，也可能困扰着其他有抱负的数据科学家。</p><p id="2b0c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我在 Coursera 上<strong class="kt ir">“机器学习——斯坦福在线”</strong>的课程期间，<strong class="kt ir">吴恩达教授</strong>帮我顺利通过。我希望这篇简要介绍了他在一次讲座中的解释的文章能够帮助我们许多人理解“调试或诊断学习算法”的重要性。</p><p id="6325" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，让我们说出所有的可能性或<strong class="kt ir">‘下一步尝试什么？’</strong>当假设在其预测中出现不可接受的大误差，或者需要改进我们的假设时:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lw"><img src="../Images/c3bbd769ccd15375fe478d265fb3d0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OcTx0PKQz53RoI6pAVbD6A.jpeg"/></div></div></figure><p id="db8d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="lx">我们将再次访问该表，做出明智的选择，并创建我们的</em> <strong class="kt ir"> <em class="lx">工具箱</em> </strong> <em class="lx">。</em></p><p id="bffd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上述诊断将基本上帮助我们找到<strong class="kt ir">偏差方差权衡</strong>。<br/>让我们用一个简单的图来简单地形象化这个概念，来说明<em class="lx">过拟合</em>(高方差)和<em class="lx">欠拟合</em>(高偏差)。</p><h1 id="7443" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">偏差方差权衡</h1><p id="9d7f" class="pw-post-body-paragraph kr ks iq kt b ku mq jr kw kx mr ju kz la ms lc ld le mt lg lh li mu lk ll lm ij bi translated"><em class="lx">从根本上说,“最佳模型”的问题是在偏差和方差之间找到一个最佳平衡点。这里有一个链接，链接到杰克·范德普拉斯在《Python 数据科学手册》中解释的<a class="ae mv" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off" rel="noopener ugc nofollow" target="_blank">偏差方差权衡</a>。</em></p><p id="97cf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以下代码可视化了我们的假设在多项式的不同<em class="lx">次上的拟合。<br/>请注意，<em class="lx">过拟合</em>和<em class="lx">欠拟合</em>也可以在不同的<em class="lx">正则化参数</em>和<em class="lx">训练集大小</em>上可视化。</em></p><blockquote class="mw mx my"><p id="9209" class="kr ks lx kt b ku kv jr kw kx ky ju kz mz lb lc ld na lf lg lh nb lj lk ll lm ij bi translated"><em class="iq">源代码:</em> <a class="ae mv" href="https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_bias_variance.html#bias-and-variance-of-polynomial-fit" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> Scipy 讲义</em> </a></p></blockquote><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="7697" class="nh lz iq nd b gy ni nj l nk nl">%matplotlib inline<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="2158" class="nh lz iq nd b gy nm nj l nk nl">def generating_func(x, err=0.5):<br/>    return np.random.normal(10 - 1. / (x + 0.1), err)</span><span id="7846" class="nh lz iq nd b gy nm nj l nk nl">from sklearn.pipeline import make_pipeline<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.preprocessing import PolynomialFeatures</span><span id="d1b7" class="nh lz iq nd b gy nm nj l nk nl">n_samples = 8</span><span id="7a21" class="nh lz iq nd b gy nm nj l nk nl">np.random.seed(0)<br/>x = 10 ** np.linspace(-2, 0, n_samples)<br/>y = generating_func(x)</span><span id="4c5a" class="nh lz iq nd b gy nm nj l nk nl">x_test = np.linspace(-0.2, 1.2, 1000)</span><span id="4e29" class="nh lz iq nd b gy nm nj l nk nl">titles = ['d = 1 (under-fit; high bias)',<br/>          'd = 2',<br/>          'd = 6 (over-fit; high variance)']<br/>degrees = [1, 2, 6]</span><span id="aeb4" class="nh lz iq nd b gy nm nj l nk nl">fig = plt.figure(figsize=(9, 3.5))<br/>fig.subplots_adjust(left=0.06, right=0.98, bottom=0.15, top=0.85, wspace=0.05)</span><span id="67bc" class="nh lz iq nd b gy nm nj l nk nl">for i, d in enumerate(degrees):<br/>    ax = fig.add_subplot(131 + i, xticks=[], yticks=[])<br/>    ax.scatter(x, y, marker='x', c='k', s=50)</span><span id="7370" class="nh lz iq nd b gy nm nj l nk nl">model = make_pipeline(PolynomialFeatures(d), LinearRegression())<br/>    model.fit(x[:, np.newaxis], y)<br/>    ax.plot(x_test, model.predict(x_test[:, np.newaxis]), '-b')</span><span id="bbdd" class="nh lz iq nd b gy nm nj l nk nl">ax.set_xlim(-0.2, 1.2)<br/>    ax.set_ylim(0, 12)<br/>    ax.set_xlabel('house size')<br/>    if i == 0:<br/>        ax.set_ylabel('price')</span><span id="207d" class="nh lz iq nd b gy nm nj l nk nl">ax.set_title(titles[i])<br/>    <br/>fig.savefig('graph-Images/bias-variance.png')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/92b9953212250669bb49804a0d9de3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*mlK6fJ6BWM5DBEdrnMAZSQ.jpeg"/></div></figure></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="7764" class="ly lz iq bd ma mb nv md me mf nw mh mi jw nx jx mk jz ny ka mm kc nz kd mo mp bi translated">验证曲线</h1><h2 id="38ab" class="nh lz iq bd ma oa ob dn me oc od dp mi la oe of mk le og oh mm li oi oj mo ok bi translated">1.变化的模型复杂性</h2><blockquote class="mw mx my"><p id="0184" class="kr ks lx kt b ku kv jr kw kx ky ju kz mz lb lc ld na lf lg lh nb lj lk ll lm ij bi translated"><em class="iq">源代码:</em> <a class="ae mv" href="https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_bias_variance.html#bias-and-variance-of-polynomial-fit" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> Scipy 讲义</em> </a></p></blockquote><p id="d939" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">生成更大的数据集</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="267f" class="nh lz iq nd b gy ni nj l nk nl">from sklearn.model_selection import train_test_split<br/><br/>n_samples = 200<br/>test_size = 0.4<br/>error = 1.0<br/><br/># randomly sample the data<br/>np.random.seed(1)<br/>x = np.random.random(n_samples)<br/>y = generating_func(x, error)<br/><br/># split into training, validation, and testing sets.<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)<br/><br/># show the training and validation sets<br/>plt.figure(figsize=(6, 4))<br/>plt.scatter(x_train, y_train, color='red', label='Training set')<br/>plt.scatter(x_test, y_test, color='blue', label='Test set')<br/>plt.title('The data')<br/>plt.legend(loc='best');</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/366354678d69a21106d0d173bf9a151b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*Ocp1q93kEjdHbZQB.png"/></div></figure><p id="2d12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">绘制不同模型复杂性的验证曲线</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="2e94" class="nh lz iq nd b gy ni nj l nk nl">from sklearn.model_selection import validation_curve<br/><br/>degrees = np.arange(1, 21)<br/><br/>model = make_pipeline(PolynomialFeatures(), LinearRegression())<br/><br/># The parameter to vary is the "degrees" on the pipeline step<br/># "polynomialfeatures"<br/>train_scores, validation_scores = validation_curve(<br/>                 model, x[:, np.newaxis], y,<br/>                 param_name='polynomialfeatures__degree',<br/>                 param_range=degrees,<br/>                 cv = 5)<br/><br/># Plot the mean train error and validation error across folds<br/>plt.figure(figsize=(6, 4))<br/>plt.plot(degrees, validation_scores.mean(axis=1), lw=2,<br/>         label='cross-validation')<br/>plt.plot(degrees, train_scores.mean(axis=1), lw=2, label='training')<br/><br/>plt.legend(loc='best')<br/>plt.xlabel('degree of fit')<br/>plt.ylabel('explained variance')<br/>plt.title('Validation curve')<br/>plt.tight_layout()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/e44baaf2e8e5dda0272ab7229349a517.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*UXxgXqp39sbupFCG.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/e8dc02346819f7058207649e18462dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PBkSlhbqOfnP6CaS.png"/></div></div><figcaption class="on oo gj gh gi op oq bd b be z dk">Image Source: <a class="ae mv" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html" rel="noopener ugc nofollow" target="_blank">Python Data Science Handbook by Jake VanderPlas</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/bd3d8976bcef86221adfa860d125673f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCDUjvmPdzAqc70qbg2aXg.jpeg"/></div></div></figure><h2 id="c0f9" class="nh lz iq bd ma oa ob dn me oc od dp mi la oe of mk le og oh mm li oi oj mo ok bi translated">2.变化正则化参数</h2><blockquote class="mw mx my"><p id="2b78" class="kr ks lx kt b ku kv jr kw kx ky ju kz mz lb lc ld na lf lg lh nb lj lk ll lm ij bi translated"><em class="iq">生成数据集和绘制不同正则化参数的验证曲线不在本讨论范围内。还感兴趣吗？请参考杰克·范德普拉斯的</em> <a class="ae mv" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Regularization" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> Python 数据科学手册</em> </a></p></blockquote><p id="d666" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们知道，基函数(多项式特征、高斯特征等)的引入。)到我们的线性回归中使得模型更加灵活。</p><p id="156d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是，它会带来风险吗？<br/>是的，答案是 <strong class="kt ir"> <em class="lx">过拟合</em> </strong></p><p id="58c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">原因</strong>:我们的模型过于灵活，无法捕捉训练数据集中的异常值或极值。<br/> <strong class="kt ir">解决方案</strong>:正规化</p><p id="7525" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">正则化</strong>是一种回归形式，它将模型参数惩罚或约束或正则化为零。</p><ul class=""><li id="4da5" class="os ot iq kt b ku kv kx ky la ou le ov li ow lm ox oy oz pa bi translated">当我们有很多特征时，这种方法很有效，每个特征都对我们的预测有所贡献。</li><li id="512b" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated">因此，我们保留了所有的特征，但是减少了参数的大小或值。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/4bf4f8eb9e715f973a1cfdda68ecd51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rQc3QOGy9Bpws_1B.png"/></div></div><figcaption class="on oo gj gh gi op oq bd b be z dk">Image Source Code: <a class="ae mv" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html" rel="noopener ugc nofollow" target="_blank">Python Data Science Handbook by Jake VanderPlas</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/0868e0819509b356d4a54b3339967e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qp-BsF7MJEL5ROhD8PT18g.jpeg"/></div></div></figure></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="85f3" class="ly lz iq bd ma mb nv md me mf nw mh mi jw nx jx mk jz ny ka mm kc nz kd mo mp bi translated">学习曲线</h1><blockquote class="mw mx my"><p id="69e3" class="kr ks lx kt b ku kv jr kw kx ky ju kz mz lb lc ld na lf lg lh nb lj lk ll lm ij bi translated"><em class="iq">源代码:</em> <a class="ae mv" href="https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_bias_variance.html#bias-and-variance-of-polynomial-fit" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> Scipy 讲义</em> </a></p></blockquote><p id="73ca" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">针对<em class="lx">固定模型复杂性</em>(d = 1 和 d = 15)，随着样本数量的增加，绘制训练和测试误差图</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="2c0a" class="nh lz iq nd b gy ni nj l nk nl"># A learning curve for d = 1, 15<br/>for d in [1, 15]:<br/>    model = make_pipeline(PolynomialFeatures(degree=d), LinearRegression())</span><span id="89f7" class="nh lz iq nd b gy nm nj l nk nl">from sklearn.model_selection import learning_curve<br/>    train_sizes, train_scores, validation_scores = learning_curve(<br/>        model, x[:, np.newaxis], y,<br/>        train_sizes=np.logspace(-1, 0, 20),<br/>        cv = 5)</span><span id="61c8" class="nh lz iq nd b gy nm nj l nk nl"># Plot the mean train error and validation error across folds<br/>    plt.figure(figsize=(6, 4))<br/>    plt.plot(train_sizes, validation_scores.mean(axis=1),<br/>            lw=2, label='cross-validation')<br/>    plt.plot(train_sizes, train_scores.mean(axis=1),<br/>                lw=2, label='training')<br/>    plt.ylim(bottom=-.1, top=1)</span><span id="8cf1" class="nh lz iq nd b gy nm nj l nk nl">plt.legend(loc='best')<br/>    plt.xlabel('number of train samples')<br/>    plt.ylabel('explained variance')<br/>    plt.title('Learning curve (degree=%i)' % d)<br/>    plt.tight_layout()</span><span id="7b7e" class="nh lz iq nd b gy nm nj l nk nl">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a71325aa00b1c7aff926e481928f6afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*91nLkWV2CGbvXcoA.png"/></div></figure><p id="bd1d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">固定模型复杂度(d = 1)的可变样本量—高偏差估计量:</strong></p><ul class=""><li id="4977" class="os ot iq kt b ku kv kx ky la ou le ov li ow lm ox oy oz pa bi translated">它<em class="lx">欠拟合</em>数据，因为训练和验证分数都很低，或者学习曲线已经收敛到一个较低的分数。</li><li id="dfea" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated">我们可以预期，添加更多的训练数据不会有所帮助</li><li id="3188" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated">重温<em class="lx">‘下一步尝试什么？’</em>工作台修复<em class="lx">偏高</em></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d1c370e2f1f773bb1376d1d32bfaa9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*9yITLOsd9KuYD_Fk.png"/></div></figure><p id="3144" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">固定模型复杂性(d = 15)的可变样本量—高方差估计量:</strong></p><ul class=""><li id="fb0e" class="os ot iq kt b ku kv kx ky la ou le ov li ow lm ox oy oz pa bi translated">由于训练分数远高于验证分数，因此<em class="lx">过度拟合</em>数据。</li><li id="8d4f" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated">随着我们向这个训练集添加更多的样本，训练分数将继续降低，而交叉验证分数将继续增加，直到它们在中间相遇。</li><li id="8eb4" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated">我们可以预期，添加更多的训练数据将会有所帮助。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/88713775cda2ce2dd59e44e492834628.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*1of7faPDySjWq2ddfGARJA.jpeg"/></div><figcaption class="on oo gj gh gi op oq bd b be z dk">Image Source Code: <a class="ae mv" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html" rel="noopener ugc nofollow" target="_blank">Python Data Science Handbook by Jake VanderPlas</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/0d354809fd23046727d3a639d63722c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dfk-3EbJx_E8qvGElyv5ag.jpeg"/></div></div></figure><p id="05a0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我所承诺的，这里有我们的 <strong class="kt ir"> <em class="lx">工具箱——“下一步尝试什么？”</em> </strong> <em class="lx"> <br/>它与验证和学习曲线一起，将帮助我们做出明智的决策。</em></p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="d251" class="ly lz iq bd ma mb nv md me mf nw mh mi jw nx jx mk jz ny ka mm kc nz kd mo mp bi translated">参考</h1><ul class=""><li id="c13b" class="os ot iq kt b ku mq kx mr la pj le pk li pl lm ox oy oz pa bi translated">Coursera 的“<strong class="kt ir">机器学习—斯坦福在线</strong>”讲座，主讲人<strong class="kt ir">吴恩达教授</strong></li><li id="1a47" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated">杰克·范德普拉斯的《Python 数据科学手册》</li><li id="58f3" class="os ot iq kt b ku pb kx pc la pd le pe li pf lm ox oy oz pa bi translated"><a class="ae mv" href="https://scipy-lectures.org/packages/scikit-learn/index.html" rel="noopener ugc nofollow" target="_blank">犀利的讲稿</a></li></ul></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><p id="13e9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">感谢您的阅读！如果你觉得这有帮助或者没有帮助，请在评论中告诉我。<br/>如果此文对您有帮助，<a class="ae mv" href="https://medium.com/@eklavyasaxena/evaluating-a-machine-learning-algorithm-81746c947ad3?source=friends_link&amp;sk=7dd263e10bfbff45638f219c3ddaad80" rel="noopener"> <em class="lx">分享一下</em> </a>。</p><p id="6c91" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">访问 Jupyter 笔记本— <a class="ae mv" href="https://nbviewer.jupyter.org/github/eklavyasaxena/Evaluating-a-Machine-Learning-Algorithm/blob/master/Evaluating-a-Machine-Learning-Algorithm.ipynb" rel="noopener ugc nofollow" target="_blank">点击此处</a></p><p id="1d2c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">领英</strong></p><div class="pm pn gp gr po pp"><a href="https://linkedin.com/in/EklavyaSaxena" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd ir gy z fp pu fr fs pv fu fw ip bi translated">Eklavya Saxena -东北大学-马萨诸塞州波士顿| LinkedIn</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">东北大学精通数据科学的研究生，在以下领域有 2 年以上的工作经验</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">linkedin.com</p></div></div><div class="py l"><div class="pz l qa qb qc py qd kp pp"/></div></div></a></div><p id="5791" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> GitHub </strong></p><div class="pm pn gp gr po pp"><a href="https://github.com/eklavyasaxena/Evaluating-a-Machine-Learning-Algorithm" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd ir gy z fp pu fr fs pv fu fw ip bi translated">eklavyasaxena/评估机器学习算法</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">工具箱——“下一步要做什么？”有了大量易于使用的机器学习库，它经常吸引…</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">github.com</p></div></div><div class="py l"><div class="qe l qa qb qc py qd kp pp"/></div></div></a></div></div></div>    
</body>
</html>