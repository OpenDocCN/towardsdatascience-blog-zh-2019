<html>
<head>
<title>The BERT-based text classification models of DeepPavlov</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 BERT 的 DeepPavlov 文本分类模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-bert-based-text-classification-models-of-deeppavlov-a85892f14d61?source=collection_archive---------11-----------------------#2019-05-29">https://towardsdatascience.com/the-bert-based-text-classification-models-of-deeppavlov-a85892f14d61?source=collection_archive---------11-----------------------#2019-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b373" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">DeepPavlov 是一个对话式人工智能框架，包含了构建聊天机器人所需的所有组件。DeepPavlov 是在开源机器学习框架<a class="ae ko" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和<a class="ae ko" href="https://keras.io" rel="noopener ugc nofollow" target="_blank"> Keras </a>之上开发的。它免费且易于使用。本文描述了如何使用 DeepPavlov 的基于 BERT 的模型。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/b3a84791c6677c30bbca01797699b778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fEk5wvn2dEhIUy1HltZ2yQ.png"/></div></div></figure><h1 id="21f5" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">关于伯特</h1><p id="0870" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">BERT(Transformers 的双向编码器表示)[1]的发布是自然语言处理(NLP)社区去年最激动人心的事件。BERT 是一种基于转换器的技术，用于预训练语言表示，它可以在各种 NLP 任务中产生最先进的结果。伯特论文被计算语言学协会北美分会评为年度最佳长篇论文。</p><p id="bc55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Google Research 发布了几个预训练的 BERT 模型，包括多语言、中文和英文的 BERT。关于这些预训练模型的更多细节可以在<a class="ae ko" href="https://github.com/google-research/bert#pre-trained-models" rel="noopener ugc nofollow" target="_blank">这里</a>找到。除了已发布的模型，在 DeepPavlov，我们还为俄语培训了基于 BERT 的模型(RuBERT)。</p><p id="51ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">RuBERT 基于多语言 BERT，并在俄语维基百科和新闻数据上进行训练。我们将 BERT 集成到三个下游任务中:文本分类、标注、问题回答。因此，我们在所有这些任务上都取得了重大进展。基于 DeepPavlov BERT 的模型可以在这里找到<a class="ae ko" href="http://docs.deeppavlov.ai/en/master/components/bert.html" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="2cce" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">BERT 预处理器</h1><p id="c970" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">DeepPavlov NLP 管道在 config/faq 文件夹下的单独配置文件中定义。配置文件由四个主要部分组成:<strong class="js iu">数据集 _ 读取器</strong>、<strong class="js iu">数据集 _ 迭代器</strong>、<strong class="js iu">链接器</strong>和<strong class="js iu">训练器</strong>。你可以在这里阅读更多关于配置文件<a class="ae ko" href="https://medium.com/deeppavlov/simple-intent-recognition-and-question-answering-with-deeppavlov-c54ccf5339a9" rel="noopener">的结构。</a></p><p id="445a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有基于 BERT 的模型的公共元素是配置文件的<strong class="js iu">链接器</strong>部分中的<strong class="js iu">BERT _ 预处理器</strong>块。未处理的文本应该被传递给<strong class="js iu"> bert_preprocessor </strong>用于标记化成子标记，用它们的索引编码子标记，并创建标记和段掩码。如果将类处理为流水线中的独热标签，则应将<strong class="js iu"> one_hot_labels </strong>设置为<strong class="js iu"> true </strong>。<strong class="js iu"> vocab_file </strong>参数定义了 BERT 词汇文件。</p><pre class="kq kr ks kt gt me mf mg mh aw mi bi"><span id="92a1" class="mj lc it mf b gy mk ml l mm mn">{<br/>    "in": [ "x" ],    <br/>    "class_name": "bert_preprocessor",<br/>    "do_lower_case": false,<br/>    "vocab_file": "{DOWNLOADS_PATH}/bert_models/multi_cased_L-12_H-768_A-12/vocab.txt",<br/>    "out": [ "bert_features" ]<br/>}</span></pre><h1 id="bcbf" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">文本分类的 BERT</h1><p id="6892" class="pw-post-body-paragraph jq jr it js b jt lz jv jw jx ma jz ka kb mb kd ke kf mc kh ki kj md kl km kn im bi translated">DeepPavlov 通过使用预训练的 BERT，为 NLP 中最受欢迎的任务之一——文本分类问题——提供了一个易于使用的解决方案。人们可以使用几个经过预先训练的英语、多语言和 RuBERT 模型。对于文本分类情况，对应于[CLS]令牌的最终隐藏状态(即，变换器输出)被用作分类任务的聚合序列表示。</p><p id="7262" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们在情感分析问题上演示基于 DeepPavlov BERT 的文本分类模型。它包括确定作者对某一特定主题的态度。这可以通过应用文本分类来实现。在这种情况下，类别可以是阴性、中性和阳性。</p><p id="5c45" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，安装模型的所有需求。</p><pre class="kq kr ks kt gt me mf mg mh aw mi bi"><span id="2a1e" class="mj lc it mf b gy mk ml l mm mn">python -m deeppavlov install rusentiment_bert</span></pre><p id="fe8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">rusentiment _ bert 模型基于多语言环境的 bert，其最大序列长度(<strong class="js iu"> max_seq_length </strong>)等于 64。</p><p id="4317" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以使用命令行与模型进行交互。</p><pre class="kq kr ks kt gt me mf mg mh aw mi bi"><span id="2308" class="mj lc it mf b gy mk ml l mm mn">python -m deeppavlov interact rusentiment_bert -d</span><span id="df4b" class="mj lc it mf b gy mo ml l mm mn">x::I like this game</span><span id="3282" class="mj lc it mf b gy mo ml l mm mn">&gt;&gt; [‘positive’]</span></pre><p id="dc1a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者，您可以通过 Python 代码与模型进行交互。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="2c09" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以在自己的数据上训练基于 BERT 的文本分类模型。为此，更改配置文件<strong class="js iu"> </strong>的<strong class="js iu"> dataset_reader </strong>中的<strong class="js iu"> data_path </strong>参数，以及分别定义训练集和测试集文件的<strong class="js iu"/><strong class="js iu">train</strong>和<strong class="js iu"> test </strong>参数。数据文件应该是 csv 格式，由<strong class="js iu"> class_sep </strong>分隔(默认为 class_sep= "，)。然后以同样的方式训练模型:</p><pre class="kq kr ks kt gt me mf mg mh aw mi bi"><span id="5cf1" class="mj lc it mf b gy mk ml l mm mn">python -m deeppavlov train &lt;config_file&gt;</span></pre><p id="1420" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这里阅读更多关于基于 BERT 的文本分类模型。</p><p id="1e9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献</strong></p><ol class=""><li id="6d6d" class="mr ms it js b jt ju jx jy kb mt kf mu kj mv kn mw mx my mz bi translated"><a class="ae ko" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.04805</a></li></ol></div></div>    
</body>
</html>