<html>
<head>
<title>Intuition: Exploration vs Exploitation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直觉:探索与开发</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuition-exploration-vs-exploitation-c645a1d37c7a?source=collection_archive---------6-----------------------#2019-11-02">https://towardsdatascience.com/intuition-exploration-vs-exploitation-c645a1d37c7a?source=collection_archive---------6-----------------------#2019-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4fd9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">权衡和常见解决方案背后的直觉。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d1e0656325566966a6ce74e7c2914e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_zBlzu9udiuYqsTRcS0oZw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://unsplash.com/photos/fqkrXYMosT4" rel="noopener ugc nofollow" target="_blank">Berries</a> by <a class="ae ky" href="https://unsplash.com/@gndclouds" rel="noopener ugc nofollow" target="_blank">William Felker</a> on <a class="ae ky" href="https://unsplash.com/license" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="91c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">探索-开发权衡是一个众所周知的问题，它发生在学习系统不得不重复做出不确定回报的选择的场景中。从本质上来说，对于一个对世界只有不完整认识的决策系统来说，困境在于是重复迄今为止行之有效的决策(exploit)还是做出新颖的决策，希望获得更大的回报(explore)。</p><p id="818b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S0893608002000564" rel="noopener ugc nofollow" target="_blank">强化学习</a>高度相关，但对于许多其他应用也是如此，例如<a class="ae ky" href="https://www.datacouncil.ai/talks/a-multi-armed-bandit-framework-for-recommendations-at-netflix" rel="noopener ugc nofollow" target="_blank">推荐系统</a>和<a class="ae ky" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.8373&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">在线广告</a>。</p><p id="d744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我给出了三个简单且行之有效的策略的概述，以解决多武装匪徒的探索-开发权衡。</p><p id="25d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">术语<em class="lv">一个</em>武装强盗来自老虎机，它有一只“胳膊”可以拉，可以有效地“抢劫”玩家的钱。</p><p id="3e80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果问题包括在多个离散选项(武器)中反复选择，每个选项产生一个概率奖励(金钱)，我们称之为“<a class="ae ky" href="https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf" rel="noopener ugc nofollow" target="_blank">多武器强盗问题</a>”。在这种情况下，就像吃角子老虎机一样，每个选择的奖励与之前选择的选项无关。随着时间的推移，决策者对每个选项的回报分配建立了理解。目标是最大化期望回报，这需要找出哪个选项给出最高的平均回报，并尽可能地利用这个选项。</p><p id="21c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，想象你正遭受下午疲劳的折磨，你想知道如何仍然是最有效率的。每天，你可以尝试几种策略中的一种:喝杯咖啡，小睡一会儿，试着放松一下，等等。对于每一种策略，回报看起来都是概率性的，因为许多其他因素也在影响你的生产力。你开始每天选择一个选项，有时尝试新的东西，有时重复过去行之有效的东西。当你观察你每天的工作效率时，你会对你的策略的有效性有更好的理解，使你最终能够始终如一地选择最可靠的策略。</p><h1 id="7d5e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">解决方案策略</h1><p id="e0ec" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">正如你所想象的，有许多方法可以解决多臂土匪问题。我将试着给你一个直观的概述，介绍一些简单但有效的(也就是它们实际上正在被使用的)方法。</p><h2 id="adde" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">ϵ-greedy</h2><p id="2ed8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这个非常简单，如果你学过强化学习 101 课程，我保证你听说过。纯粹贪婪的方法背后的想法是每一个决定都尝试一次，然后一直选择能带来最高回报的那个。</p><p id="6255" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了增加一些探索的成分，<em class="lv"> ϵ-greedy </em>让你决定你的决定中有多少部分要花在探索(ϵ)上，有多少部分要花在探索(1-ϵ)到目前为止的最佳选择上。如果你将ϵ设为 0.4，那么你会在过去 60%的时间里选择给你最好平均回报的选项，而在 40%的时间里选择任何其他选项。通常，你希望ϵ很小，这样你可以充分利用你的经验，但也可以不时地去探索。虽然有时候，从一个更大的ϵ开始是一个好主意，以鼓励最初的探索，并在你收集关于回报的知识时减少它。</p><p id="0414" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于许多应用程序，ϵ-greedy 工作得非常好。在其他场景中，由于<em class="lv">不加选择地</em>探索，它失去了一些效率。这意味着当你探索时，选择两个看似次优的选项的概率是相等的。即使有些选项总是比其他选项差，它们有时还是会被选中，浪费宝贵的资源。</p><h2 id="e1ee" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">置信上限(UCB)</h2><p id="826b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在我们在贝叶斯水域中行走。想象一下，对于每一个选项，我们都维护着一个我们在选择这个选项时收集到的奖励的分布。如果我们还没有收集到任何奖励，我们可以从一个假设开始:每个选项的先前奖励分布通常非常广泛(例如<a class="ae ky" href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)" rel="noopener ugc nofollow" target="_blank">均匀分布</a>)。当我们收集奖励时，我们会不断更新这个分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/45229bb356b40f50f355d76e822a4170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hy9Wh3V_lYKLUNHrKpttGA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><strong class="bd ng">UCB:</strong> Three options with different densities. The orange lines are one standard deviation higher than the mean. As we explore the option with the highest μ + βσ, we would go for option 3 this time (if we chose β=1).</figcaption></figure><p id="f767" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每一步，我们不会像贪婪方法那样，只选择估计平均回报μ最高的选项，而是选择μ <em class="lv">加</em>标准差σ最高的选项。这样做的效果是，在开始时，当我们不确定任何选项的回报并且方差很大时，我们倾向于探索具有高不确定性的选项，因为σ占优势。随着我们对每个选项的平均回报越来越确定，标准差会缩小，我们倾向于选择估计平均回报μ最高的选项，而忽略回报很少的选项。</p><p id="7aff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们称<strong class="lb iu"> μ + βσ </strong>为<a class="ae ky" href="http://jmlr.org/papers/volume3/auer02a/auer02a.pdf" rel="noopener ugc nofollow" target="_blank">置信上限</a>，其中β是一个权衡参数，用于引导更多或更少的探索。随着β趋于零，我们更接近于纯粹利用迄今为止提供最高平均回报的期权。另一方面，高β值有利于探索，直到我们几乎没有不确定性。</p><p id="a357" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，与ϵ-greedy 方法不同，我们的经验会影响我们要探索的选项。如果我们很早就发现其中一个选项几乎肯定比其他选项差，那么我们就不会投资去进一步探索它。</p><h2 id="8f48" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">汤普森取样</h2><p id="7461" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">请记住，我们不断地重新估计我们的奖励分布。当遵循 1933 年引入的<a class="ae ky" href="https://arxiv.org/pdf/1707.02038.pdf" rel="noopener ugc nofollow" target="_blank">汤普森采样</a>方案<a class="ae ky" href="https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf" rel="noopener ugc nofollow" target="_blank">时，我们利用了那些分布所提供的不确定性。在每个时间步，我们从每个分布中抽取一个样本。然后，我们根据这些样本的回报值对选项进行排序，就像我们之前根据平均值加上β乘以标准差对分布进行排序一样。最后，我们选择排名最高的选项。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/10c74a89dd2e673bb79d06d574c1e28f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r31wPO4FwcwS1YQ81_YwAg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><strong class="bd ng">Thompson sampling: </strong>Three options with different densities. The black dots represent one sample drawn from each distribution. For this set of samples, we would explore option 3, although it has a lower mean reward than option 2.</figcaption></figure><p id="f2c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，估计平均回报高的选项就有可能被选中。尽管如此，我们不确定的期权也有机会被选中，因为它们的样本分布在一个很大的可能回报值区域。</p><p id="f0ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">采用这种策略，我们通常不会完全放弃任何选择，不像我们对 UCB 那样。相反，它越来越不可能选择一个看似糟糕的选项。</p><p id="e10b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">凭直觉，我们可以说 UCB 和汤姆森抽样都是“乐观的”，因为他们给了期权一个机会，即使这些期权到目前为止还没有带来最好的平均回报。在理论层面上，可以证明 UCB 和汤普逊抽样之间有着<a class="ae ky" href="https://www.semanticscholar.org/paper/Learning-to-Optimize-via-Posterior-Sampling-Russo-Roy/28cf1bd6110e734e20fc63f727d0d5bba612b921" rel="noopener ugc nofollow" target="_blank">密切的联系</a>。</p><p id="c156" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。我希望你喜欢这篇文章，也学到了一些有用的东西！</p></div></div>    
</body>
</html>