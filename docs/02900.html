<html>
<head>
<title>Building a Multi-label Text Classifier using BERT and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BERT 和 TensorFlow 构建多标签文本分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d?source=collection_archive---------1-----------------------#2019-05-11">https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d?source=collection_archive---------1-----------------------#2019-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5686" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir">多标签分类</strong>问题中，训练集由实例组成，每个实例可以被分配多个类别，这些类别被表示为一组目标标签，</p><ul class=""><li id="b3be" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">一条短信可以同时涉及宗教、政治、金融或教育中的任何一个，也可以什么都不涉及。</li><li id="b5da" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">一部电影可以根据其概要内容分为动作片、喜剧片和爱情片。一部电影有可能分成多种类型，比如浪漫喜剧。</li></ul><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/7786b46a69428cbd5e8a0c539b392ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X9Ol5xgbP7FYBEeQ"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><a class="ae lp" href="https://unsplash.com/photos/Ubhjpv7q0Pk" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="4291" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与<strong class="jp ir">多类</strong>分类问题有何不同？</p><p id="031f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir">多类分类</strong>中，每个样本被分配到一个且只有一个标签:水果可以是苹果或梨，但不能同时是两者。让我们考虑一个三类 C= ["太阳，"月亮，云"]的例子。在多类中，每个样本只能属于一个 C 类。在多标签情况下，每个样本可以属于一个或多个类别。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/3b63956b3849fe6081ddc07487192d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*3urmp9yuK1Ys39UO5j41kg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><a class="ae lp" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="4158" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">资料组</h2><p id="e74a" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">对于我们的讨论，我们将使用 Kaggle 的<a class="ae lp" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="mp">有毒评论分类挑战</em> </strong> </a>数据集，该数据集由大量维基百科评论组成，这些评论已被人类评级者标记为有毒行为。毒性的类型有:</p><p id="a6a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">toxic, severe_toxic, obscene, threat, insult, identity_hate</strong></code></p><p id="039a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">示例:</p><p id="9b04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b">“<strong class="jp ir">Hi! I am back again! Last warning! Stop undoing my edits or die!</strong>”</code></p><p id="5c68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">被标记为[1，0，0，1，0，0]。意思是它既是<code class="fe mq mr ms mt b">toxic </code>又是<code class="fe mq mr ms mt b">threat</code>。</p><p id="6f2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于数据集的详细 EDA，请参见此处的<a class="ae lp" rel="noopener" target="_blank" href="/journey-to-the-center-of-multi-label-classification-384c40229bff"/>。</p><h2 id="3032" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">让我们简单地讨论一下伯特</h2><p id="b26a" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">2018 年 10 月，谷歌发布了一个新的语言表示模型，名为<strong class="jp ir"> BERT </strong>，代表来自变形金刚的<strong class="jp ir">双向编码器表示。</strong> BERT 基于最近在预训练上下文表示方面的工作，包括<a class="ae lp" href="https://arxiv.org/abs/1511.01432" rel="noopener ugc nofollow" target="_blank">半监督序列学习</a>、<a class="ae lp" href="https://blog.openai.com/language-unsupervised/" rel="noopener ugc nofollow" target="_blank">生成性预训练</a>、<a class="ae lp" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>和<a class="ae lp" href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="noopener ugc nofollow" target="_blank"> ULMFit </a>。然而，与这些之前的模型不同，BERT 是第一个<em class="mp">深度双向</em>、<em class="mp">无监督</em>语言表示，仅使用纯文本语料库(<a class="ae lp" href="https://www.wikipedia.org/" rel="noopener ugc nofollow" target="_blank">维基百科</a>)进行预训练。</p><p id="fef7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">预先训练的表示可以是<strong class="jp ir"> <em class="mp">上下文无关的</em> </strong>或<strong class="jp ir"> <em class="mp">上下文相关的</em> </strong></p><ol class=""><li id="9d4b" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mu kr ks kt bi translated"><strong class="jp ir">上下文无关</strong>模型，如<a class="ae lp" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>或<a class="ae lp" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>为词汇表中的每个单词生成单个<a class="ae lp" href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>表示。例如，单词“<em class="mp"> bank </em>”在“<em class="mp"> bank account </em>”和“<em class="mp"> bank of the river”中具有相同的上下文无关的表示。</em></li><li id="eb21" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mu kr ks kt bi translated"><strong class="jp ir">上下文</strong>模型代之以生成基于句子中其他单词的每个单词的表示。上下文表示还可以是单向的<em class="mp">或双向的</em>。例如，在句子“<em class="mp">我访问了银行账户</em>”中，基于“<em class="mp">我访问了</em>”而不是“<em class="mp">账户</em>”，单向上下文模型将表示“<em class="mp">银行</em>”然而，BERT 使用它的上一个和下一个上下文来表示“<em class="mp">银行</em>”——“我访问了 … <em class="mp">账户</em>”——从一个深度神经网络的最底层开始，使它成为深度双向的。</li></ol><p id="34d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于双向 LSTM 的语言模型训练标准的从左到右的语言模型，并且还训练从右到左(反向)的语言模型，该语言模型从随后的单词预测先前的单词，如在 ELMO。在 ELMo 中，前向语言模型和后向语言模型各有一个 LSTM。关键的区别在于，LSTM 没有同时考虑前一个和后一个记号。</p><h2 id="1796" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">为什么 BERT 优于其他双向模型？</h2><p id="8f2f" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">直观地说，深度双向模型比从左到右模型或者从左到右和从右到左模型的结合更强大。不幸的是，标准的条件语言模型只能从左到右或从右到左进行训练，因为双向条件反射将允许每个单词在多层上下文中间接“看到自己”。</p><p id="2be7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决这个问题，BERT 使用“屏蔽”技术屏蔽掉输入中的一些单词，然后双向调节每个单词以预测被屏蔽的单词。例如:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/13b1b0deac57dab3e36fc65281c4ba3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*10VHfC8yNYYDiLNn.png"/></div></figure><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi mw"><img src="../Images/300a31476d2b10a929e888c8c75b20be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SeNcx8e4YZ0q3U1t"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><em class="mx">Forward, Backward, and Masked Language Modeling</em></figcaption></figure><p id="0ab8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">BERT 还通过对一个非常简单的任务进行预训练来学习建立句子之间的关系模型，这个任务可以从任何文本语料库中生成:给定两个句子 A 和 B，B 是语料库中 A 后面的下一个句子，还是只是一个随机的句子？例如:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c208f86e57ff568a3a4b41f671153119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*FHza6DIZ8J_VcTiX.png"/></div></figure><p id="5f67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是对伯特的一个非常基本的概述。详细内容请参考最初的<a class="ae lp" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>和一些参考文献【1】、【2】。</p><p id="21e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">好消息:</strong>谷歌已经将 BERT 上传到<a class="ae lp" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a>这意味着我们可以直接使用预先训练好的模型来解决我们的自然语言处理问题，无论是文本分类还是句子相似度等。</p><p id="6df4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lp" href="https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="noopener ugc nofollow" target="_blank">预测电影评论</a>的例子，二进制分类问题作为示例代码提供在存储库中。在本文中，我们将重点讨论 BERT 在多标签文本分类问题中的应用。因此，我们将基本上修改示例代码，并应用必要的更改，使其适用于多标签场景。</p><h2 id="c205" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">设置</h2><p id="49d7" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">使用<code class="fe mq mr ms mt b">!pip install bert-tensorflow</code>安装 BERT</p><p id="423f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下载预训练的 BERT 模型:这些是权重和其他必要的文件，用于表示 BERT 在预训练中学习到的信息。你需要选择你想要的 BERT 预训练重量。有两种方法可以下载和使用预训练的 BERT 模型:</p><ol class=""><li id="14e1" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mu kr ks kt bi translated">直接使用 tensorflow-hub:</li></ol><p id="1af0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下预先训练的模型可供选择。</p><ol class=""><li id="16b1" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mu kr ks kt bi translated"><code class="fe mq mr ms mt b">BERT-Base, Uncased</code> : 12 层，768 隐，12 头，110M 参数</li><li id="99e0" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mu kr ks kt bi translated"><code class="fe mq mr ms mt b">BERT-Large, Uncased</code> : 24 层，1024 隐，16 头，340 米参数</li><li id="ce11" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mu kr ks kt bi translated"><code class="fe mq mr ms mt b">BERT-Base, Cased</code> : 12 层，768 隐，12 头，110M 参数</li><li id="1493" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mu kr ks kt bi translated"><code class="fe mq mr ms mt b">BERT-Large, Cased</code> : 24 层，1024 隐，16 头，340 米参数</li><li id="3281" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mu kr ks kt bi translated"><code class="fe mq mr ms mt b">BERT-Base, Multilingual Case</code> : 104 种语言，12 层，768 隐，12 头，110M 参数</li><li id="6a40" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk mu kr ks kt bi translated"><code class="fe mq mr ms mt b">BERT-Base, Chinese</code>:简体中文和繁体中文，12 层，768 隐藏，12 个头，110M 参数</li></ol><p id="cfe0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用基本型号:<strong class="jp ir">‘未装箱 _ L-12 _ H-768 _ A-12’</strong><br/><code class="fe mq mr ms mt b"><strong class="jp ir">BERT_MODEL_HUB </strong>= “<a class="ae lp" href="https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1</a>"</code></p><p id="10f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.手动下载 BERT 模型文件:下载并保存到一个目录中，然后解压缩。以下是英文文件的链接:</p><ul class=""><li id="8587" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><code class="fe mq mr ms mt b"><a class="ae lp" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">BERT-Base, Uncased</a>, <a class="ae lp" href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">BERT-Base, Cased</a>,</code></li><li id="0b32" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><code class="fe mq mr ms mt b"><a class="ae lp" href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="noopener ugc nofollow" target="_blank">BERT-Large, Cased</a>, <a class="ae lp" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="noopener ugc nofollow" target="_blank">BERT-Large, Uncased</a></code></li></ul><p id="6573" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以使用任何一种方式，但是让我们看看在预先训练的模型中实际上有哪些文件。当我下载<code class="fe mq mr ms mt b"><a class="ae lp" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">BERT-Base, Uncased</a>,</code>时，有如下 3 个重要文件:</p><p id="1ca0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">BERT_VOCAB</strong>= ‘uncased-l12-h768-a12/vocab.txt'<br/><strong class="jp ir">BERT_INIT_CHKPNT </strong>= ‘uncased-l12-h768-a12/bert_model.ckpt’<br/><strong class="jp ir">BERT_CONFIG </strong>= ‘uncased-l12-h768-a12/bert_config.json’</code></p><p id="011b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">BERT_VOCAB</strong></code> <strong class="jp ir"> : </strong>包含模型词汇【单词到索引的映射】</p><p id="4b7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">BERT_INIT_CHKPNT</strong></code> <strong class="jp ir"> </strong> : <strong class="jp ir"> </strong>包含预训练模型的权重</p><p id="d751" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">BERT_CONFIG</strong></code> <strong class="jp ir"> </strong> : <strong class="jp ir"> </strong>包含 BERT 模型架构。</p><h2 id="1319" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">标记化</strong></h2><p id="2178" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">标记化包括将输入文本分解成单个单词。为此，第一步是创建 tokenizer 对象。有两种方法可以做到这一点:</p><ol class=""><li id="7c55" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk mu kr ks kt bi translated">直接来自张量流中心</li></ol><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi my"><img src="../Images/6f7e9679af93c7d8fccf8ad07d619d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BeTMMr9CFzMqTdyDr45CRg.png"/></div></div></figure><p id="2748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.从手动下载的文件:</p><p id="c1f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<code class="fe mq mr ms mt b"><strong class="jp ir">BERT_INIT_CHKPNT &amp; BERT_VOCAB files</strong></code></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi mz"><img src="../Images/b9283736e4596835b27bd481a629b757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i05nezrYhKQImzdjgTGuRw.png"/></div></div></figure><p id="1c54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建了记号赋予器之后，就该使用它了。让我们来修饰句子:<code class="fe mq mr ms mt b">“This here’s an example of using the BERT tokenizer”</code></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi na"><img src="../Images/a476b3104718ac79571745ce7c6cba22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCAbiE_H42DwTb3xJMi7DQ.png"/></div></div></figure><p id="5cc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">词汇表的大小:~30K</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/fa92e75e6b180235aa9c31acd59fc142.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*haWwbjxR_e3I9zJwEbZqkQ.png"/></div></figure><h2 id="e126" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">数据预处理:</strong></h2><p id="3f0e" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">让我们首先阅读提供的数据集:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nc"><img src="../Images/d878eee10e84480441f0dd119d19f380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7to_qqlx9PGBSd0GGOvivw.png"/></div></div></figure><p id="3628" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">train.head()</strong></code></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nd"><img src="../Images/cd608e248df70f7e19a3f1df013687d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raniyloufSaUD5j2ahqMYw.png"/></div></div></figure><p id="c996" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要将数据转换成伯特能理解的格式。为此提供了一些实用函数。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi ne"><img src="../Images/58d244d32b7cf1ef056aaab48a9b5cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EAdyGCT8iu-jNS7p_7CQog.png"/></div></div></figure><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi na"><img src="../Images/fc1ed0e5fb71bf702e39ec1f48679498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tXRHUCp4LvklxQX7OBUmA.png"/></div></div></figure><p id="8ead" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">create_examples()</strong></code>，读取数据帧并将输入文本和相应的目标标签加载到<code class="fe mq mr ms mt b"><strong class="jp ir">InputExample</strong></code> <strong class="jp ir"> </strong>对象中。</p><p id="6d65" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用 tokenizer，我们将在示例中调用<code class="fe mq mr ms mt b"><strong class="jp ir">convert_examples_to_features</strong></code> <strong class="jp ir"> </strong>方法，将它们转换成 BERT 理解的特性。这个方法添加了特殊的“CLS”和“SEP”标记，BERT 使用它们来标识句子的开始和结束。它还将“索引”和“段”标记附加到每个输入中。因此，根据 BERT 格式化输入的所有工作都由该函数完成。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nf"><img src="../Images/2d4324999c06c031c2e11c81ad8f8f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IA45-w25Ach4LcgsBABYKA.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">BERT input representation. The input embeddings is the sum of the token embeddings, the segmentation embeddings and the position embeddings.</figcaption></figure><h2 id="e3a8" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated"><strong class="ak">创建模型</strong></h2><p id="df6a" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">这里，我们使用预训练的 BERT 模型，并针对我们的分类任务对其进行微调。基本上，我们加载预训练的模型，然后训练最后一层用于分类任务。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi ng"><img src="../Images/c66e8d32fc5402d6ceb3ef7027e1f7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBcJBHQXzr-2gQ-kNEOlTg.png"/></div></div></figure><p id="59da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在多标签分类中，我们使用<code class="fe mq mr ms mt b"><strong class="jp ir">sigmoid()</strong></code>而不是<code class="fe mq mr ms mt b"><strong class="jp ir">softmax()</strong></code>来获得概率。</p><ul class=""><li id="b8fc" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">在简单的二进制分类中，两者之间没有太大的区别，但是在多国分类的情况下，sigmoid 允许处理非排他性标签(又名<em class="mp">多标签</em>)，而 softmax 处理排他性类别。</li><li id="2f3c" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">在计算概率之前，<em class="mp"> logit </em>(也称为分数)是与类别相关联的<a class="ae lp" href="https://stats.stackexchange.com/q/52825/130598" rel="noopener ugc nofollow" target="_blank">原始未缩放值。就神经网络架构而言，这意味着 logit 是密集(全连接)层的输出[3]。</a></li></ul><p id="da95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，为了计算概率，我们做了如下改变:</p><p id="8502" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><strong class="jp ir">### multi-class case: probabilities = tf.nn.softmax(logits)</strong> <br/> <strong class="jp ir">### multi-label case: probabilities = tf.nn.sigmoid(logits)</strong></code></p><p id="c7ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了计算每个示例的损失，tensorflow 提供了另一种方法:</p><p id="381e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mq mr ms mt b"><a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">tf.nn.sigmoid_cross_entropy_with_logits</strong></a></code>测量离散分类任务中的概率误差，其中每个类别都是独立的且不互斥。这适用于多标签分类问题[4]。</p><p id="e5f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其余的代码大部分来自 BERT 参考文献[5]。完整的代码可以在<a class="ae lp" href="https://github.com/javaidnabi31/Multi-Label-Text-classification-Using-BERT/blob/master/multi-label-classification-bert.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>获得。</p><h2 id="d331" class="lr ls iq bd lt lu lv dn lw lx ly dp lz jy ma mb mc kc md me mf kg mg mh mi mj bi translated">Kaggle 提交分数:</h2><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nh"><img src="../Images/aaac6b27355e54dd811b2a4e06e82f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQFxYXfvj-mcKc0TUGCI-g.png"/></div></div></figure><p id="2965" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仅仅通过运行 2 个纪元，就获得了非常好的结果。这就是<strong class="jp ir">迁移学习</strong>的强大之处:使用在庞大数据集上训练过的预训练模型，然后针对特定任务对其进行微调。Kaggle 代码<a class="ae lp" href="https://www.kaggle.com/javaidnabi/toxic-comment-classification-using-bert/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="ce92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在其他一些数据集上进行试验，运行几个时期[3–4]并查看结果。</p><p id="4eb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读。</p><h1 id="4bfb" class="ni ls iq bd lt nj nk nl lw nm nn no lz np nq nr mc ns nt nu mf nv nw nx mi ny bi translated">参考</h1><p id="c270" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">[1]<a class="ae lp" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2018/11/open-sourcing-Bert-state-of-art-pre . html</a></p><p id="f2d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]<a class="ae lp" href="https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/" rel="noopener ugc nofollow" target="_blank">https://ml explained . com/2019/01/07/paper-parsed-Bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/</a></p><p id="9314" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]<a class="ae lp" href="https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensor flow</a></p><p id="096f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]<a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/nn/sigmoid _ cross _ entropy _ with _ logits</a></p><p id="4dc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5]<a class="ae lp" href="https://github.com/google-research/bert/blob/master/run_classifier.py" rel="noopener ugc nofollow" target="_blank">https://github . com/Google-research/Bert/blob/master/run _ classifier . py</a></p><p id="7a56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[6]<a class="ae lp" href="https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://www . depends-on-the-definition . com/guide-to-multi-label-class ification-with-neural-networks/</a></p><p id="6024" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[7]<a class="ae lp" rel="noopener" target="_blank" href="/journey-to-the-center-of-multi-label-classification-384c40229bff">https://towards data science . com/journey-to-the-centre-of-multi-label-class ification-384 c 40229 BFF</a></p><p id="98ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[8]https://gombru.github.io/2018/05/23/cross_entropy_loss/<a class="ae lp" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>