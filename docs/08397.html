<html>
<head>
<title>A Beginners Guide to Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q-Learning 初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c?source=collection_archive---------0-----------------------#2019-11-15">https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c?source=collection_archive---------0-----------------------#2019-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1407" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">决定性的反思</h2><div class=""/><div class=""><h2 id="0318" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">无模型强化学习</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3b6dd37694738cc7cff1fb0f9786d64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPKBncSYi2pDS9dl_ebwAA.jpeg"/></div></div></figure><p id="fdbc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi lz translated">一旦你的狗犯了错误，你是否曾经责备或惩罚它？或者你有没有训练过一只宠物，你要求的每一个正确的命令都会奖励它？如果你是宠物主人，可能你的答案会是“是”。你可能已经注意到，一旦你从它年轻的时候就经常这样做，它的错误行为就会一天天地减少。同样，它会从错误中学习，训练自己。</p><p id="c2aa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">作为人类，我们也经历了同样的事情。你还记得吗，在我们上小学的时候，一旦我们把功课做好了，老师就会奖励我们星星。:D</p><p id="d68d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这正是在<strong class="lf jd">强化学习(RL) </strong>中发生的事情。</p><blockquote class="mi"><p id="2dd9" class="mj mk it bd ml mm mn mo mp mq mr ly dk translated">强化学习是人工智能中最美的分支之一</p></blockquote><p id="afcc" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">RL 的目标是<strong class="lf jd"> <em class="mx">通过响应动态环境</em> </strong>采取一系列行动来最大化代理人的报酬。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi my"><img src="../Images/79c8429f43247c0236a5dad9e69f6919.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*qgTt-GvbDtEugkmTk5z0zg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">There are 4 basic components in Reinforcement Learning; agent, environment, reward and action.</figcaption></figure><p id="4e8f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">强化学习是利用经验做出最佳决策的科学。分解一下，强化学习的过程包括以下简单的步骤:</p><ol class=""><li id="bf06" class="nd ne it lf b lg lh lj lk lm nf lq ng lu nh ly ni nj nk nl bi translated">观察环境</li><li id="4ec3" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">使用某种策略决定如何行动</li><li id="e7e4" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">相应地行动</li><li id="01e6" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">接受奖励或惩罚</li><li id="3b63" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">吸取经验教训，完善我们的战略</li><li id="5918" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly ni nj nk nl bi translated">迭代直到找到最优策略</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c108c46eb5743251c7e15c5b764eeb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*W-xIKHBSp8LrKwZRAQE-0Q.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Source: <a class="ae ns" href="http://H. Nguyen and H. La, &quot;Review of Deep Reinforcement Learning for Robot Manipulation,&quot; in 2019 Third IEEE International Conference on Robotic Computing (IRC), Naples, Italy, 2019, pp. 590–595" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><p id="44b3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有两种主要类型的 RL 算法。分别是<strong class="lf jd"><em class="mx"/></strong>和<strong class="lf jd"> <em class="mx">无模型</em> </strong>。</p><p id="fae4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> <em class="mx">无模型</em> </strong>算法是一种不使用或估计环境的动态性(转移和奖励函数)而估计最优策略的算法。然而，基于<strong class="lf jd"> <em class="mx">模型的</em> </strong>算法是使用转移函数(和回报函数)来估计最优策略的算法。</p><h1 id="9bc9" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">进入 Q-Learning</h1><p id="358c" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated"><strong class="lf jd"><em class="mx">Q</em>-学习</strong>是一种<strong class="lf jd"> <em class="mx">无模型</em> </strong>强化学习算法。</p><p id="7fad" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Q-learning 是一种基于<strong class="lf jd"> <em class="mx">值的</em> </strong>学习算法。基于值的算法基于等式(尤其是贝尔曼等式)更新值函数。而另一种类型，<strong class="lf jd"> <em class="mx">基于策略的</em> </strong>利用从上次策略改进中获得的贪婪策略来估计值函数。</p><p id="bdc4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Q-learning 是一个<strong class="lf jd"> <em class="mx">非策略学习者</em> </strong>。意味着它独立于代理的行为学习最优策略的值。另一方面，<strong class="lf jd"> <em class="mx">基于策略的学习器</em> </strong>学习代理正在执行的策略的值，包括探索步骤，并且考虑到策略中固有的探索，它将找到最优的策略。</p><h2 id="eae4" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated">这个‘Q’是什么？</h2><p id="153f" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">Q-learning 中的“Q”代表质量。质量在这里代表了一个给定的行为在获得未来回报中的有用程度。</p><h2 id="c201" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated"><strong class="ak">Q-学习定义</strong></h2><ul class=""><li id="bd0e" class="nd ne it lf b lg ol lj om lm pb lq pc lu pd ly pe nj nk nl bi translated"><strong class="lf jd"> Q*(s，a) </strong>是在状态 s 下做 a，然后遵循最优策略的期望值(累计贴现报酬)。</li><li id="90a2" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly pe nj nk nl bi translated">Q-learning 使用<strong class="lf jd">时间差异(TD) </strong>来估计 Q*(s，a)的值。时间差异是一个主体在事先没有环境知识的情况下通过片段从环境中学习。</li><li id="6e0c" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly pe nj nk nl bi translated">代理维护一个<strong class="lf jd"> Q[S，A] </strong>表，其中<strong class="lf jd"> S </strong>是<strong class="lf jd">状态</strong>的集合，<strong class="lf jd"> A </strong>是<strong class="lf jd">动作</strong>的集合。</li><li id="fae1" class="nd ne it lf b lg nm lj nn lm no lq np lu nq ly pe nj nk nl bi translated">Q[s，a]表示其对 Q*(s，a)的当前估计。</li></ul><h2 id="5784" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated">q-学习简单示例</h2><p id="53f4" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">在本节中，Q-learning 已经通过演示进行了解释。</p><p id="6734" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">假设一个智能体必须沿着一条有障碍的路径从起点移动到终点。智能体需要以最短的路径到达目标，而不碰到障碍物，并且他需要沿着障碍物覆盖的边界前进。为了方便起见，我在一个定制的网格环境中做了如下介绍。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/cca930ba8e0ffc4c194ee226b8d554ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*3CsV0MUbcAI-6Xc-29txow.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Agent and its Environment</figcaption></figure><h2 id="28a4" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated"><strong class="ak">介绍 Q 表</strong></h2><p id="fe73" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">Q-Table 是用于计算每个状态下行动的最大预期未来回报的数据结构。基本上，这张表将指导我们在每个状态下的最佳行动。为了学习 Q 表的每个值，使用 Q 学习算法。</p><h2 id="b9be" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated"><strong class="ak"> Q 功能</strong></h2><p id="c1fc" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">Q 函数使用贝尔曼方程并接受两个输入:状态(s)和动作(a)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/1b3d20e48bb55418ddbacd08b5d1fd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*raYvY38yWm4E4J3iYqv6dA.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Bellman Equation. Source: <a class="ae ns" href="https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><h2 id="3990" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated">q-学习算法过程</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/e00d0349a785ddb38aefb86cab3f2df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*XSvi_PFFazxEe1TZbsnn7Q.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Q-learning Algorithm</figcaption></figure><h2 id="100e" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated"><strong class="ak">步骤 1:初始化 Q 工作台</strong></h2><p id="6bf0" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">首先要建立 Q 表。有 n 列，其中 n=动作的数量。有 m 行，其中 m=状态数。</p><p id="d9be" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们的例子中，n =向左、向右、向上和向下，m=开始、空闲、正确路径、错误路径和结束。首先，让我们将值初始化为 0。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c7d3e98ebb69663d0d435826326c9cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*3hKL8xjGfLN03ySnLl3Tag.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Initial Q-table</figcaption></figure><h2 id="f10c" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated"><strong class="ak">第二步:选择一个动作</strong></h2><h2 id="d53e" class="oq nu it bd nv or os dn nz ot ou dp od lm ov ow of lq ox oy oh lu oz pa oj iz bi translated"><strong class="ak">步骤 3:执行一个动作</strong></h2><p id="c332" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">步骤 2 和 3 的组合执行不确定的时间量。这些步骤会一直运行，直到训练停止，或者训练循环按照代码中的定义停止。</p><p id="413a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，基于 Q 表选择状态中的动作(a)。注意，如前所述，当剧集开始时，每个 Q 值都应该是 0。</p><p id="eea5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，使用上面陈述的贝尔曼方程更新在起点和向右移动的 Q 值。</p><p id="7366" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"><em class="mx">ε贪婪策略</em> </strong>的概念在这里发挥了出来。开始时，ε比率会更高。代理将探索环境并随机选择行动。这在逻辑上是这样发生的，因为代理对环境一无所知。当代理探索环境时，ε速率降低，代理开始利用环境。</p><p id="0310" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在探索的过程中，代理在估计 Q 值时逐渐变得更有信心。</p><p id="738a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们的代理示例中，当代理的训练开始时，代理完全不知道环境。因此，让我们说，它采取了一个随机行动，其'正确'的方向。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/1273edc0f987f28e7a6fe79bc465884e.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*cuMXeIZILS1lYu7NwgiB2w.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Action : Agent follows ‘right’</figcaption></figure><p id="13ac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们可以使用贝尔曼方程更新位于起点并向右移动的 Q 值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/3bd3d79b55211b354dc1eb94b0123e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*MMwJarCPAg_RIETjl5GSqA.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Updated Q-table</figcaption></figure><p id="5030" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">第四步:衡量奖励</strong></p><p id="3c5c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们已经采取了一项行动，并观察到了一个结果和奖励。</p><p id="9277" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">第五步:评估</strong></p><p id="fcc4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们需要更新函数 Q(s，a)。</p><p id="9b59" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个过程反复重复，直到学习停止。这样，Q 表被更新，并且值函数 Q 被最大化。这里 Q(state，action)返回该状态下该行为的<strong class="lf jd">预期未来回报</strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/f046993a1ff390d28acf368c734116fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ-tDj-iMdsHlGKUR81Xgw.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Bellman Equation Explanation for the episodes</figcaption></figure><p id="71d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在示例中，我输入了如下奖励方案。</p><blockquote class="pm pn po"><p id="7c84" class="ld le mx lf b lg lh kd li lj lk kg ll pp ln lo lp pq lr ls lt pr lv lw lx ly im bi translated">当离目标更近一步时奖励= +1</p><p id="552a" class="ld le mx lf b lg lh kd li lj lk kg ll pp ln lo lp pq lr ls lt pr lv lw lx ly im bi translated">击中障碍物时的奖励=-1</p><p id="4213" class="ld le mx lf b lg lh kd li lj lk kg ll pp ln lo lp pq lr ls lt pr lv lw lx ly im bi translated">空闲时奖励=0</p></blockquote><div class="ks kt ku kv gt ab cb"><figure class="ps kw pt pu pv pw px paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/6ac804012db4da3af1194759f011e7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*bKFEGQK9ODER-KY0iKfzGw.png"/></div></figure><figure class="ps kw py pu pv pw px paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/07ac368a48b3a1428d12219f82d15a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*tGTRg0CxDGLt-FyAWS6efw.png"/></div></figure><figure class="ps kw pz pu pv pw px paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/03e729b73449b0ad5e4aeec629552e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*0U98tVbheXhyNMXLOs-WrA.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk qa di qb qc">Figure (a) Positive Reward, (b) &amp; (c) Negative Rewards</figcaption></figure></div><p id="311b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最初，我们探索代理的环境并更新 Q 表。当 Q 表准备好时，代理开始利用环境并开始采取更好的行动。最终 Q 表可能如下所示(例如)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/a9dbc143d5a7e81dcfde7f91b595070a.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*BGSmzoESJarhZ3ok4Dheyg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Example final Q-table</figcaption></figure><p id="ee9e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下是培训后代理实现目标的最短路径的结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/b884a3cd6bc95480f52b5828a9df3ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*HPwrR1NgxOcqYwjBpxwQQg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Agent’s navigation towards the goal</figcaption></figure><div class="ks kt ku kv gt ab cb"><figure class="ps kw qf pu pv pw px paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/7486befc22249ba9cc8066f9065735a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*i03kmZKZZ_Rr964roo-g0g.png"/></div></figure><figure class="ps kw qf pu pv pw px paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/68691a53d076c1d3c76e35e30fa45bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dammzqP0N7D0MNFlhZiNHg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk qg di qh qc"><em class="qi">Plotting the results for the number of steps (a) Episode via steps, (b) Episode via cost</em></figcaption></figure></div><h1 id="dd5a" class="nt nu it bd nv nw nx ny nz oa ob oc od ki oe kj of kl og km oh ko oi kp oj ok bi translated">履行</h1><p id="da6d" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">请投邮件掌握 python 实现代码的概念解释。</p></div><div class="ab cl qj qk hx ql" role="separator"><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo"/></div><div class="im in io ip iq"><h1 id="3eec" class="nt nu it bd nv nw qq ny nz oa qr oc od ki qs kj of kl qt km oh ko qu kp oj ok bi translated">参考</h1><p id="0cf1" class="pw-post-body-paragraph ld le it lf b lg ol kd li lj om kg ll lm on lo lp lq oo ls lt lu op lw lx ly im bi translated">[1] H. Nguyen 和 H. La，“机器人操纵的深度强化学习综述”，载于<em class="mx"> 2019 年第三届 IEEE 机器人计算国际会议(IRC) </em>，意大利那不勒斯，2019 年，第 590–595 页。</p><p id="a6a5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2]<a class="ae ns" href="https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/" rel="noopener ugc nofollow" target="_blank">https://www . freecodecamp . org/news/an-introduction-to-q-learning-reinforcement-learning-14 ac0 b 4493 cc/</a></p><p id="88aa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[3]<a class="ae ns" href="https://courses.cs.ut.ee/MTAT.03.292/2014_spring/uploads/Main/Q-learning.pdf" rel="noopener ugc nofollow" target="_blank">https://courses . cs . ut . ee/mtat . 03.292/2014 _ spring/uploads/Main/Q-learning . pdf</a></p><p id="c6fd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[4]https://towards data science . com/introduction-to-variable-reinforcement-learning-algorithms-I-q-learning-sarsa-dqn-ddpg-72 a5 E0 CB 6287</p><p id="c13f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[5]<a class="ae ns" href="https://blog.dominodatalab.com/deep-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">https://blog.dominodatalab.com/deep-reinforcement-learning/</a></p></div><div class="ab cl qj qk hx ql" role="separator"><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo qp"/><span class="qm bw bk qn qo"/></div><div class="im in io ip iq"><p id="5a15" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">希望你通过这篇博文对 Q-learning 有一个清晰的认识。如果你对这篇博文有任何问题或评论，请在下面留下你的评论。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qv"><img src="../Images/d9866a2716ce6a5e069f00ea267d86b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfskKP2icFgqx8GA3X4WNg.jpeg"/></div></div></figure><p id="1249" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">干杯，学习愉快！</p></div></div>    
</body>
</html>