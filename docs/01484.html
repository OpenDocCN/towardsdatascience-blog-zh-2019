<html>
<head>
<title>Attention-based Neural Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于注意力的神经机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c?source=collection_archive---------4-----------------------#2019-03-09">https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c?source=collection_archive---------4-----------------------#2019-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/2f1fcd54d00c4880c3759e480365e76f.png" data-original-src="https://miro.medium.com/v2/0*Km2olUwvMv6Plq1c."/></div></figure><p id="688a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过在翻译过程中选择性地关注句子的子部分，注意力机制正越来越多地用于提高神经机器翻译(NMT)的性能。在这篇文章中，我们将介绍两种简单的注意力机制:<strong class="jw ir"> <em class="ks">全局方法</em> </strong>(关注所有源词)和<strong class="jw ir"> <em class="ks">局部方法</em> </strong>(只关注源词的子集)。请记住，源指的是编码器，目标指的是解码器。</p><p id="7658" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本博客将涵盖<a class="ae kt" href="https://arxiv.org/pdf/1508.04025.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir"> <em class="ks">这篇</em> </strong> </a>的论文，这篇论文证明了增加注意力可以导致比基于非注意力的网络显著的性能提升。在上述论文中提出的系综模型为 WMT 的 15 英德翻译产生了一种新的艺术状态模型。</p><p id="2fbf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">除了提高机器翻译练习的性能，基于注意力的网络还允许模型学习不同模态(不同数据类型)之间的对齐，例如语音帧和文本之间或者图片的视觉特征和其文本描述之间的对齐。</p><h1 id="5715" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">神经机器翻译(NMT)</h1><p id="779f" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">NMT 是一个巨大的神经网络，它被训练成端到端的方式，用于将一种语言翻译成另一种语言。下图是基于 RNN 的编码器-解码器架构的 NMT 的示意图。</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/ab77e0909f58869525fc00d904889b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*y0fDASwnhK1buork4sGBcg.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 1 : Neural machine translation as a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here &lt;eos&gt; marks the end of a sentence.</figcaption></figure><p id="da39" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">NMT 直接模拟翻译源的条件概率<strong class="jw ir"><em class="ks">【p(y/x)】</em></strong><em class="ks">(x1，x2…xn) </em>句转化为目标句<em class="ks"> (y1，y2…yn) </em>。</p><p id="739e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">NMT 由两部分组成:</p><ol class=""><li id="b18b" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated">编码器，为每个源语句计算表示<strong class="jw ir"> <em class="ks"> S </em> </strong></li><li id="c2d7" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">一种解码器，一次生成一个单词的翻译，因此将条件概率分解为:</li></ol><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/028af96c9671300baab7d9515f1021c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*nCy3SohohdWUqt2cwN5SuA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">A probability of translation y given the source sentence x</figcaption></figure><p id="f582" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">可以将解码每个字 y(j)的概率参数化为</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/648efeca3b28fa2b2cfc0609110950b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*WA3GNeytF_ROxeo759U9sw.png"/></div></figure><p id="07a9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中<strong class="jw ir"> <em class="ks"> h(j) </em> </strong>可以建模为</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/b0d91f655ab29e0614222c939af3cc83.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*jSnChT1Imr_94kHct5TZmw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">RNN hidden unit definition (h)</figcaption></figure><p id="d6d3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks">其中<br/>g:</em></strong>a<strong class="jw ir"><em class="ks"/></strong>变换函数，输出词汇大小向量<br/> <strong class="jw ir"> <em class="ks"> h </em> </strong> : RNN 隐藏单元<br/> <strong class="jw ir"> <em class="ks"> f </em> </strong>:给定先前隐藏状态，计算当前隐藏状态。</p><p id="9ef0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">翻译流程的<strong class="jw ir"> <em class="ks">培训目标</em> </strong>可以被框定为</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/7bce377fa5afea14178e32c7178af8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*Tk7NDHzt1oqXgKGXJiwKRA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Loss Function</figcaption></figure><h2 id="dbc8" class="my kv iq bd kw mz na dn la nb nc dp le kf nd ne li kj nf ng lm kn nh ni lq nj bi translated">是什么让 NMT 如此受欢迎？</h2><ol class=""><li id="e76e" class="mg mh iq jw b jx ls kb lt kf nk kj nl kn nm kr ml mm mn mo bi translated">NMT 在大规模翻译任务中，如英语到法语/德语的翻译，已经达到了艺术水平。</li><li id="e8b7" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">NMT 需要最少的领域知识，概念上非常简单。</li><li id="f8ff" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">NMT 的内存占用很小，因为它不存储庞大的相位表和语言模型。</li><li id="cf07" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">NMT 能够很好地概括很长的单词句子。</li></ol><h2 id="0d9e" class="my kv iq bd kw mz na dn la nb nc dp le kf nd ne li kj nf ng lm kn nh ni lq nj bi translated">注意网络和非注意网络的区别</h2><p id="18af" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">在大多数基于非注意力的 RNN 架构信源表示中，<strong class="jw ir"> <em class="ks"> S </em> </strong>仅被使用一次来初始化解码器隐藏状态。[在图 1 中，解码器只能访问编码器的最后一层]</p><p id="7bfd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">另一方面，基于注意力的网络在整个翻译过程中指的是一组<strong class="jw ir">源隐藏状态。[在图 2 中，解码器可以访问编码器的所有隐藏状态]</strong></p><h1 id="fb79" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">注意机制的类型</h1><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f01d6f6b27b640d7d5c8e95a3a4b5c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*lhq_KkyyLw0ttugrDhEi4A.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 2: NMT with attention and input-feeding approach</figcaption></figure><p id="321f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">上图重点描绘了基于 RNN 的编码器-解码器架构。正如我们之前解释的，注意力可以大致分为两种类型:</p><ol class=""><li id="e7dd" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated"><strong class="jw ir">全局关注</strong>:关注所有源位置。</li><li id="69cd" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated"><strong class="jw ir">局部注意</strong>:只注意几个源位置。</li></ol><p id="029f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这两种基于注意力的模型仅在解码阶段不同于普通的编码器-解码器架构。这些基于注意力的方法的不同之处在于它们计算上下文向量的方式(<strong class="jw ir"><em class="ks">【c(t)】</em></strong>)。</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ca13d6f3d6f8d99e28024f27bf13c125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*KBKsDHiUBM__dxRV31wZpw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 3: Hidden state of NMT architecture with global attention</figcaption></figure><p id="c7e4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">图 3 的术语表如下</p><p id="32a5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"><em class="ks">【t】</em></strong>:隐藏目标状态<br/> <strong class="jw ir"> <em class="ks"> c(t) </em> </strong>:源端上下文向量<br/> <strong class="jw ir"> <em class="ks"> y(t) </em> </strong>:当前目标词<br/><strong class="jw ir"><em class="ks">h _ bar(t)</em></strong>:注意隐藏状态<br/> <strong class="jw ir"> <em class="ks"> a(t) </em> </strong>:对齐向量</p><h2 id="98b5" class="my kv iq bd kw mz na dn la nb nc dp le kf nd ne li kj nf ng lm kn nh ni lq nj bi translated"><strong class="ak">如何计算注意力？</strong></h2><p id="84b0" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">这两种基于注意力的方法都有以下共同步骤:</p><ol class=""><li id="94c6" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated">这两种方法首先将堆栈 LSTM 顶层的隐藏状态<strong class="jw ir"><em class="ks">【h(t)】</em></strong>作为输入。(棕色单元格/解码器的目标状态)</li><li id="1560" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">派生<strong class="jw ir"><em class="ks">【c(t)</em></strong>来捕获相关的源端信息，以帮助预测<strong class="jw ir"><em class="ks">【y(t)</em></strong>(顶部蓝色单元格)。<strong class="jw ir"><em class="ks">【c(t)</em></strong>基本上是你根据每个单词的对齐权重和编码器的隐藏状态为每个单词建立的上下文。</li><li id="4796" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">从 h(t)和 c(t)的简单串联计算<strong class="jw ir"><em class="ks">【h _ bar(t)</em></strong>(顶部灰色单元格)。</li></ol><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ebff2bdb68f638821add61e0ddbae3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*ddBjiAfpUKXFfqfwxxF3CQ.png"/></div></figure><p id="1d12" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">与仅将编码器的最终输出提供给解码器的基于非注意力的架构相反，<strong class="jw ir"><em class="ks">【h _ bar(t)</em></strong>具有对编码器的隐藏状态的所有状态的<strong class="jw ir"> <em class="ks"> </em> </strong>访问，这提供了源语句的信息视图。</p><p id="8349" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">4.使用 softmax 层转换注意力向量以产生预测分布。我们使用 softmax 层，因为我们必须从我们的词汇表中所有可用的单词中找到最可能的单词。</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a7bd27f7edea385b1a0f53f4a153a5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*JXu-zc9I-ZRMBRtivhpRWA.png"/></div></figure><p id="063f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">以上段落解释了基于注意力的网络的基本架构。在下面的段落中，我们将理解上下文向量<strong class="jw ir"><em class="ks">【c(t)</em></strong>在局部和全局注意力中是如何不同地计算的，以及它的影响是什么。</p><ol class=""><li id="d782" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated"><strong class="jw ir">全球关注</strong></li></ol><p id="5ca2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">全局注意力考虑所有编码器隐藏状态以导出上下文向量(c(t))。为了计算<strong class="jw ir"> <em class="ks"> c(t)，</em> </strong>我们计算<strong class="jw ir"> <em class="ks"> a(t) </em> </strong>这是一个可变长度的对齐向量。通过计算<strong class="jw ir"><em class="ks">【t】</em></strong>和<strong class="jw ir"> <em class="ks"> h_bar(s) </em> </strong>之间的相似性度量来导出对齐向量，其中<strong class="jw ir"> <em class="ks"> h(t) </em> </strong>是源隐藏状态，而<strong class="jw ir"> <em class="ks"> h_bar(s) </em> </strong>是目标隐藏状态。编码器和解码器中类似的状态实际上指的是同一个意思。</p><p id="f70f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">对齐向量(a(t)) </strong></p><p id="2fee" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对齐向量(<strong class="jw ir"><em class="ks">【t，s】</em></strong>)定义为</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/11bebcad8dae22115f5310ca58532f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*BszUN96gX292Ex17UMIgdQ.png"/></div></figure><p id="50da" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">分数是一个基于内容的函数，可以使用以下任何一种替代方法:</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c820da6681baf4b3aeb6fb672dda7ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*cKw6bEHIFyD8TC66jJulSg.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">The score function</figcaption></figure><p id="49b2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过得分函数，我们试图计算目标和源的隐藏状态之间的相似性。直观上，隐藏和源中的相似状态指的是相同的意思，但在不同的语言中。</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ca13d6f3d6f8d99e28024f27bf13c125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*KBKsDHiUBM__dxRV31wZpw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk"><strong class="bd nr">Figure 4: Global attentional model:</strong> At each time step t, the model infers a variable-length alignment weight vector a(t) based on the current target state h(t) and all source states h_bar(s). A global context vector,c(t) is then computed as the weighted average, according to a(t), over all the source states.</figcaption></figure><p id="d2fc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">图 4 中的连接线代表相互依赖的变量。</p><p id="905f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">例如</p><ol class=""><li id="e8c8" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated"><em class="ks"> a(t) </em>依赖于 h(t)和 h_bar(s)</li><li id="6a9c" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated"><em class="ks"> c(t) </em>依赖于 a(t)和 h_bar(s)</li><li id="c98b" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated"><em class="ks"> h_bar(t) </em>依赖于 c(t)和 h(t)</li></ol><h2 id="28ea" class="my kv iq bd kw mz na dn la nb nc dp le kf nd ne li kj nf ng lm kn nh ni lq nj bi translated">2.当地的关注</h2><p id="3be9" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">由于全局注意力集中在所有目标单词的所有源端单词上，这在计算上非常昂贵，并且在翻译长句时不切实际。为了克服这一缺陷，局部注意力选择只关注每个目标单词的编码器隐藏状态的一个小子集。</p><p id="d665" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">局部注意力有以下不同于全局注意力的步骤:</p><ol class=""><li id="90b8" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated">该模型首先在时间 t 为每个目标单词生成对齐位置<em class="ks"> p(t) </em>，与假设单调对齐的全局注意模型相反，我们在局部注意中学习对齐位置。换句话说，除了学习翻译之外，你还可以学习翻译的顺序是否与源句子不同(源的单词 1 可能是翻译句子中的单词 4，因此我们需要计算这个，否则我们的相似性得分将完全错误，因为我们的注意力将集中在源句子中与源句子的单词 1 不相关的单词上)。</li><li id="11a9" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">上下文向量(c(t))被导出为窗口[p(t)-D，p(t)+D]内的源隐藏状态集合的加权平均值；d 是凭经验选择的。与全局对准向量相比，局部对准向量<em class="ks"> a(t)现在是</em>固定维度的。</li></ol><p id="126e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">到目前为止，我们一直假设翻译句子和源句子都是单调对齐的。在此基础上，我们对局部注意力有了进一步的区分，具体如下:</p><ol class=""><li id="6ef7" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated"><strong class="jw ir">单调对齐(<em class="ks"> local-m </em> ) </strong></li></ol><p id="f4db" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Set p(t)=t，这意味着我们假设源序列和目标序列大致单调对齐。对齐向量与全局对齐相同</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/11bebcad8dae22115f5310ca58532f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*BszUN96gX292Ex17UMIgdQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Local alignment is the same as that of global alignment</figcaption></figure><p id="2448" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 2。预测比对</strong> ( <strong class="jw ir"> <em class="ks">局部-p </em> </strong>)</p><p id="5ca4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们的模型不是假设单调排列，而是预测排列位置如下:</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/fb6c2be4d5e660baa08cd44b776b98df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*9uzxKvaasEHazxIObXgzrA.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Alignment Position for <strong class="bd nr"><em class="ns">local-p </em></strong>model</figcaption></figure><p id="5fe1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="ks"> W(p) </em> </strong>和<strong class="jw ir"> v(p) </strong>是模型参数<strong class="jw ir"> </strong>，将学习这些参数来预测位置。<br/> <strong class="jw ir"> <em class="ks"> S </em> </strong>是源句长度<br/><strong class="jw ir"><em class="ks">p(t)</em></strong>:【0，S】</p><p id="a4d1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了有利于对齐位置<strong class="jw ir"> <em class="ks"> p(t)，</em> </strong>我们放置一个以 p(t)为中心的高斯分布。这赋予位置<strong class="jw ir"> <em class="ks"> p(t) </em> </strong>更多的权重。我们将校准权重修改为</p><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/973b5c0c0948ad0231c19dcd13f9e2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*3bwB5SrR8HPLZgnGw1I8yw.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Alignment vector for <strong class="bd nr"><em class="ns">local-p </em></strong><em class="ns">model</em></figcaption></figure><p id="85e5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">去捕捉同样的东西。</p><p id="f24b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">总而言之，全局注意力在计算上更昂贵，并且对长句无用，而局部注意力集中在<strong class="jw ir"><em class="ks"/></strong>D<strong class="jw ir"><em class="ks">【p(t)</em></strong>两侧的隐藏状态来克服这一点。局部注意有 2 种口味<strong class="jw ir"> <em class="ks"> local-m ( </em> </strong>源和目标对齐假设相同<strong class="jw ir"> <em class="ks"> ) </em> </strong>和<strong class="jw ir"> <em class="ks"> local-p </em> </strong>(这里我们计算的是<strong class="jw ir"> <em class="ks"> p(t))。</em> </strong></p><h2 id="fb31" class="my kv iq bd kw mz na dn la nb nc dp le kf nd ne li kj nf ng lm kn nh ni lq nj bi translated">投入-供给方法</h2><p id="b336" class="pw-post-body-paragraph ju jv iq jw b jx ls jz ka kb lt kd ke kf lu kh ki kj lv kl km kn lw kp kq kr ij bi translated">在所提出的注意机制中，独立地做出注意决定(先前预测的对准不影响下一次对准)，这是次优的。为了确保未来的对准决策考虑到过去的对准信息<strong class="jw ir"> <em class="ks">，h_bar(t) </em> </strong>在下一时间步与输入连接，如图所示。</p><p id="0823" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这样做是为了:</p><ol class=""><li id="1baa" class="mg mh iq jw b jx jy kb kc kf mi kj mj kn mk kr ml mm mn mo bi translated">让模型完全了解之前的对齐选择。</li><li id="4a10" class="mg mh iq jw b jx mp kb mq kf mr kj ms kn mt kr ml mm mn mo bi translated">创建一个横向和纵向都很深的网络。</li></ol><figure class="ly lz ma mb gt jq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f4c45f2b61cbd1fea547e27a3f634e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RK8g9D0DWwReMeGu8HyunQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Attentional Vectors h_bar(t) is fed to the next time steps to inform the model about past alignment decisions</figcaption></figure><p id="b6d1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在上面的文章中，我们和 NMT 一起讨论了注意力网络的基础知识。</p><h1 id="6590" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h1><ol class=""><li id="2b3e" class="mg mh iq jw b jx ls kb lt kf nk kj nl kn nm kr ml mm mn mo bi translated"><a class="ae kt" href="https://arxiv.org/pdf/1508.04025.pdf" rel="noopener ugc nofollow" target="_blank">基于注意力的神经机器翻译的有效方法</a></li></ol><p id="2c08" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你对上面的帖子有任何想法或补充，请随时联系我。</p></div></div>    
</body>
</html>