<html>
<head>
<title>The 3 Best Optimization Methods in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的三种最佳优化方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-3-best-optimization-methods-in-neural-networks-40879c887873?source=collection_archive---------3-----------------------#2019-03-14">https://towardsdatascience.com/the-3-best-optimization-methods-in-neural-networks-40879c887873?source=collection_archive---------3-----------------------#2019-03-14</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><div class=""><h2 id="c552" class="pw-subtitle-paragraph js iu iv bd b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj dk translated">了解 Adam 优化器、momentum、小批量梯度下降和随机梯度下降</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/15a39fb1490bc83bb68dc8eecc6ea468.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K0XkHkotuk-oL4Ba"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Photo by <a class="ae la" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae la" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="05bf" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">深度学习是一个迭代的过程。有如此多的参数要调整或方法要尝试，为了快速完成迭代周期，能够快速训练模型是很重要的。这是提高机器学习团队速度和效率的关键。</p><p id="aada" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，优化算法非常重要，例如随机梯度下降、最小批量梯度下降、动量梯度下降和 Adam 优化器。</p><p id="e95b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这些方法让我们的神经网络<em class="lx">学习</em>成为可能。但是，就速度而言，有些方法比其他方法执行得更好。</p><p id="007a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在这里，您将了解随机梯度下降的最佳替代方案，我们将实现每种方法，看看神经网络使用每种方法可以多快地<em class="lx">学习</em>。</p><blockquote class="ly"><p id="cda9" class="lz ma iv bd mb mc md me mf mg mh lw dk translated">对于机器学习、深度学习和人工智能的实践视频教程，请查看我的<a class="ae la" href="https://www.youtube.com/channel/UC-0lpiwlftqwC7znCcF83qg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> YouTube 频道</a>。</p></blockquote><figure class="mi mj mk ml mm kp"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Gradient…Gradient descent… Get it?</figcaption></figure><h1 id="8462" class="mp mq iv bd mr ms mt mu mv mw mx my mz kb na kc nb ke nc kf nd kh ne ki nf ng bi translated">小批量梯度下降</h1><p id="e782" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">传统的梯度下降需要在对参数进行第一次更新之前处理所有的训练样本。从现在起，更新参数将被称为<em class="lx">采取步骤</em>。</p><p id="aee9" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，我们知道深度学习在处理大量数据时效果最好。因此，梯度下降需要在数百万个训练点上进行训练，然后才能迈出一步。这显然是低效的。</p><p id="a02f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">相反，考虑将测试集分解成更小的集合。每一小组被称为<strong class="ld iw">小批量</strong>。假设每个小批量有 64 个训练点。然后，我们可以一次在一个小批量上训练算法，并在每个小批量的训练完成后采取一个步骤！</p><p id="7698" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此得名:<strong class="ld iw">小批量梯度下降</strong>。</p><h2 id="22e4" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">成本图</h2><p id="9d83" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated"><a class="ae la" rel="noopener" target="_blank" href="/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3">在之前的</a>中，我们已经看到了成本图，在每次迭代后成本都平稳下降，如下所示。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fe8b02b719aa08df09e4c4e9db074f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*AdymvNY2rfk0B8R6CYF91Q.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Example of a cost plot using gradient descent</figcaption></figure><p id="3531" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在使用最小批量梯度下降的情况下，曲线将振荡更多，总体呈下降趋势。当我们编写这个方法时，我们将看到一个例子。这种波动是有意义的，因为一组新的数据被用来优化成本函数，这意味着在回落之前，成本函数有时可能会上升。</p><h2 id="0e2b" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">如何选择小批量</h2><p id="ea39" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">让我们考虑两个极端。一方面，你可以将你的小批量设置为所有训练集的大小。这将简单地导致传统的梯度下降方法(也称为批量梯度下降)。</p><p id="844b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">另一方面，您可以将最小批量设置为 1。这意味着每一步仅在 1 个数据点上训练后进行。这叫做<strong class="ld iw">随机梯度下降</strong>。然而，这种方法不是很好，因为它经常在错误的方向上采取步骤，并且它不会收敛到全局最小值；相反，它会在全局最小值附近振荡。</p><p id="420e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，你的小批量应该在这两个极端之间。一般来说，可以遵循以下准则:</p><ul class=""><li id="010f" class="nz oa iv ld b le lf lh li lk ob lo oc ls od lw oe of og oh bi translated">如果数据集很小(少于 2000 个样本),则使用批量梯度下降</li><li id="447c" class="nz oa iv ld b le oi lh oj lk ok lo ol ls om lw oe of og oh bi translated">对于较大的数据集，典型的小批量大小为 64、128、256 或 512。当然，你的小批量必须适合你的 CPU/GPU 内存</li></ul><p id="9934" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">同样，小批量大小可以反复选择。你有时需要测试不同的尺寸，看看哪种训练速度最快。</p><h1 id="4c59" class="mp mq iv bd mr ms mt mu mv mw mx my mz kb na kc nb ke nc kf nd kh ne ki nf ng bi translated">动量梯度下降</h1><p id="acbb" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">具有动量的梯度下降包括对计算的梯度应用指数平滑。这将加速训练，因为算法将更少地向最小值振荡，并且它将向最小值采取更多的步骤。</p><p id="4779" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">如果你不知道指数平滑，你可能想读读<a class="ae la" rel="noopener" target="_blank" href="/almost-everything-you-need-to-know-about-time-series-860241bdc578">这篇</a>文章。</p><p id="60ef" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通常，使用简单的指数平滑，这意味着还有两个超参数需要调整:学习率<em class="lx">α</em>和平滑参数<em class="lx">β</em>。</p><p id="1db8" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通常，这种方法几乎总是比传统的梯度下降效果更好，并且它可以与小批量梯度下降相结合。</p><h1 id="bf0b" class="mp mq iv bd mr ms mt mu mv mw mx my mz kb na kc nb ke nc kf nd kh ne ki nf ng bi translated">亚当优化算法</h1><p id="6dd5" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">Adam 代表:<strong class="ld iw"> ada </strong>感受性<strong class="ld iw">m</strong>moment 估计。简而言之，这种方法结合了动量和 RMSprop(均方根 prop)。</p></div><div class="ab cl on oo hz op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="io ip iq ir is"><p id="a831" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">RMSprop 平滑了梯度，就像动量一样，但它使用了一种不同的方法，这种方法在数学上可以得到最好的解释。</p><p id="807c" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">首先，梯度计算如下:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ou"><img src="../Images/38ae40d594db935187bb1af081d1c8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ve3bTw6JQ9M0vALAe1odWg.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Gradient in RMSprop</figcaption></figure><p id="3149" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">然后，权重和偏差矩阵更新如下:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ov"><img src="../Images/7c937a959ca10d33279a4be0e35d8624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4Z8ZeU7Qng3gfM2MrVLGQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Weight and bias update in RMSprop</figcaption></figure><p id="ad66" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">注意<em class="lx">β2</em>是一个新的超参数(不要与动量的<em class="lx">β</em>相混淆)。另外，<em class="lx">ε</em>是一个非常小的值，以防止被 0 除。</p></div><div class="ab cl on oo hz op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="io ip iq ir is"><p id="db79" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，结合动量和 RMSprop，Adam 引入了四个超参数:</p><ul class=""><li id="9612" class="nz oa iv ld b le lf lh li lk ob lo oc ls od lw oe of og oh bi translated">学习率<em class="lx">α</em></li><li id="0662" class="nz oa iv ld b le oi lh oj lk ok lo ol ls om lw oe of og oh bi translated"><em class="lx">β</em>来自动量(通常为 0.9)</li><li id="2d9f" class="nz oa iv ld b le oi lh oj lk ok lo ol ls om lw oe of og oh bi translated">来自 RMSprop 的<em class="lx"> beta2 </em>(通常为 0.999)</li><li id="5b32" class="nz oa iv ld b le oi lh oj lk ok lo ol ls om lw oe of og oh bi translated"><em class="lx">ε</em>(通常为 1e-8)</li></ul><p id="2ea9" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">如上所述，您通常不需要调整<em class="lx"> beta </em>、<em class="lx"> beta2 </em>和<em class="lx"> epsilon </em>，因为上面列出的值通常会工作得很好。为了加速训练，只需要调整学习速率。</p><p id="8bc6" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在您已经了解了每种优化方法，让我们用 Python 实现它们，并看看它们之间的比较。</p><h1 id="10db" class="mp mq iv bd mr ms mt mu mv mw mx my mz kb na kc nb ke nc kf nd kh ne ki nf ng bi translated">编码优化方法</h1><p id="651b" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">我们将在样本数据集上运行每种方法，看看神经网络将如何执行。为了测试训练速度，我们将保持历元数恒定为 10 000。</p><p id="b6b4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">当然，完整的代码可以在<a class="ae la" href="https://github.com/marcopeix/Deep_Learning_AI/blob/master/2.Improving%20Deep%20Neural%20Networks/2.Algorithm%20Optimization/Optimization%20Methods.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中找到。</p><h2 id="24aa" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">随机梯度下降</h2><p id="1867" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">让我们从编码随机梯度下降法开始:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ow"><img src="../Images/39e7b02c392c9b5dbc96e1f8f3d9374e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W2tq8jy11sk1IkC4_jndbg.png"/></div></div></figure><p id="ba26" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这是相当直接的，因为我们使用单个数据点来进行梯度下降。</p><h2 id="5c49" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">小批量梯度下降</h2><p id="0a0b" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">小批量梯度下降有点难以实现，因为训练规模可能无法被小批量规模整除。因此，我们需要处理最后一批，以适应这种情况:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ox"><img src="../Images/3578a135705d8b9e6bea9d8c62effd85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iGdl3IIIUTt-W3feJdDtOA.png"/></div></div></figure><h2 id="ce5a" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">动力</h2><p id="c034" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">要实现动量，我们首先需要初始化<em class="lx">速度</em>:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oy"><img src="../Images/2f632a24db1f0ca7ab1f15ce3724116a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aa9l52Z5jUpmkfbr7AM3xg.png"/></div></div></figure><p id="c755" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">然后，我们可以更新我们的参数:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ow"><img src="../Images/d000d9d90acd51b48e73cf86b0efdac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1HiNwfvVREK3Xa69kiNIQ.png"/></div></div></figure><h2 id="7518" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">Adam 优化器</h2><p id="ba11" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">现在，由于 Adam 结合了 momentum 和 RMSprop，我们需要初始化两个参数:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oy"><img src="../Images/585f9b09dd9e58a2cfafca71508b8097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qLGIlo0plCFZx_XN8NXkDg.png"/></div></div></figure><p id="f0b9" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">然后，我们可以这样更新参数:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ow"><img src="../Images/207b36779fd1cee4f3523075487d9b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQFthxaeSMhvgEtNn3Qt-w.png"/></div></div></figure><h2 id="c3a8" class="nm mq iv bd mr nn no dn mv np nq dp mz lk nr ns nb lo nt nu nd ls nv nw nf nx bi translated">采用不同优化方法的模型</h2><p id="8644" class="pw-post-body-paragraph lb lc iv ld b le nh jw lg lh ni jz lj lk nj lm ln lo nk lq lr ls nl lu lv lw io bi translated">现在，我们将用不同的优化方法训练一个神经网络，看看它能学习多快。</p><p id="ece0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">让我们将我们的模型定义为:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="oz mo l"/></div></figure><p id="8c6b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">然后，在对 10 000 个时期进行小批量梯度下降训练后，我们得到:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/61a9dbed32db953ff7234f3335e7b03d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*g7D_XpcIk68H23VI9sYMrQ.png"/></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d063918eeff2c997aa6ae63d4a7a0419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*_nO07jY8Wyb0WsQ0d4ba8w.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Results for mini-batch gradient descent</figcaption></figure><p id="f1bc" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">正如你所看到的，我们在 10 000 个时期后只达到了 80%的测试准确度，并且判定边界不是很好。</p><p id="9325" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，经过动力训练后，我们得到:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/139f308fd133dbda2a3125ac0a92cee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*Av-Pkdgt1PpEpPCMsS8U9w.png"/></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/05b33967323e5e96e1481f8d8e6b3a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*MwoXpIYObIC_3l_v2PAVhw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Results for momentum</figcaption></figure><p id="6e1a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">有趣的是，势头并没有真正帮助。</p><p id="ca19" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，我们使用亚当，我们得到:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi pe"><img src="../Images/a1c18f27bd1c73e8cb4a4013cf3ab9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*sYXrsN63F_HKTA9aV6z6oQ.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/fad9a3c40fbad4f657b671ed743c08a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*CpNRtk-a8NcxM--e63oj6w.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Results with Adam optimizer</figcaption></figure><p id="4552" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太神奇了！使用 Adam，我们获得了 10 000 个历元的更高精度。</p><p id="97c2" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">请注意，更好的<em class="lx">方法就是更快的<em class="lx">方法。给定更多的时期，其他方法可以给出更好的准确度分数。不过，这个练习的目的是评估每种方法的速度，亚当显然是赢家。</em></em></p></div><div class="ab cl on oo hz op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="io ip iq ir is"><p id="7098" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">就是这样！您学习了神经网络的不同优化方法，实现了这些方法，并且发现 Adam 的性能非常好。</p><p id="e63f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通常，人工智能从业者使用小批量梯度下降或 Adam，因为它们在大多数时候表现良好。</p><p id="e2a0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">幸运的是，深度学习框架内置了优化方法的功能。在下一篇文章中，我们将介绍 TensorFlow，看看编写更大、更复杂的神经网络有多容易。</p><p id="c9a6" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">下次见！</p></div></div>    
</body>
</html>