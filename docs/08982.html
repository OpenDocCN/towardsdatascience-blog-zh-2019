<html>
<head>
<title>Testing Glue Pyspark jobs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">测试胶水 Pyspark 作业</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/testing-glue-pyspark-jobs-4b544d62106e?source=collection_archive---------10-----------------------#2019-11-30">https://towardsdatascience.com/testing-glue-pyspark-jobs-4b544d62106e?source=collection_archive---------10-----------------------#2019-11-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="97a9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">设置你的环境，这样你的<a class="ae ki" href="https://aws.amazon.com/glue/" rel="noopener ugc nofollow" target="_blank"> Glue </a> PySpark 作业就可以读取和写入一个模仿的 S3 桶，这要感谢<a class="ae ki" href="https://github.com/spulec/moto/blob/master/docs/docs/server_mode.rst" rel="noopener ugc nofollow" target="_blank"> moto 服务器</a>。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/af147c165a711187d658700efcbf9054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ho40cACEDqB9dslCIOckFA.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by <a class="ae ki" href="https://unsplash.com/@scottsanker?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Scott Sanker</a> on <a class="ae ki" href="https://unsplash.com/s/photos/glue?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="7514" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">挑战</h1><p id="74ea" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">胶合作业的典型用例是:</p><ul class=""><li id="662a" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">你阅读来自 S3 的数据；</li><li id="a487" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">你对这些数据进行一些转换；</li><li id="ade2" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">你把转换后的数据传回 S3。</li></ul><p id="5f53" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">当编写 PySpark 作业时，用 Python 编写代码和测试，并使用<a class="ae ki" href="https://pypi.org/project/pyspark/" rel="noopener ugc nofollow" target="_blank"> PySpark 库</a>在 Spark 集群上执行代码。但是我如何让 Python 和 Spark 用同一个被嘲笑的 S3 桶进行通信呢？</p><blockquote class="ng"><p id="fa70" class="nh ni it bd nj nk nl nm nn no np mm dk translated">在本文中，我将向您展示如何设置一个模拟的 S3 桶，您可以从 python 进程以及 Spark 集群访问它。</p></blockquote><h1 id="09bb" class="kz la it bd lb lc ld le lf lg lh li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">测试环境</h1><h2 id="432f" class="nt la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">创建粘合作业的先决条件</h2><p id="474a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们用的是<a class="ae ki" href="https://docs.aws.amazon.com/glue/latest/dg/release-notes.html" rel="noopener ugc nofollow" target="_blank"> Glue 1.0 </a>，也就是 Python 3.6.8，Spark/PySpark 2.4.3，Hadoop 2.8.5。<br/>确定；</p><ul class=""><li id="a391" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">你已经安装了<a class="ae ki" href="https://www.python.org/downloads/release/python-368/" rel="noopener ugc nofollow" target="_blank">python 3 . 6 . 8</a>；</li><li id="7ae8" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">你已经安装了 Java<a class="ae ki" href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" rel="noopener ugc nofollow" target="_blank">JDK 8</a>；</li><li id="794d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">您已经安装了用于 hadoop 2.7 的 spark 2.4.3。</li></ul><blockquote class="of og oh"><p id="b839" class="lr ls oi lt b lu mp ju lw lx mq jx lz oj nd mc md ok ne mg mh ol nf mk ml mm im bi translated"><strong class="lt iu">注意</strong> : Glue 使用 Hadoop 2.8.5，但是为了简单起见，我们使用 Hadoop 2.7，因为它是 Spark 2.4.3 附带的。</p></blockquote><h2 id="338d" class="nt la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">Python 依赖性</h2><pre class="kk kl km kn gt om on oo op aw oq bi"><span id="fbc0" class="nt la it on b gy or os l ot ou">pipenv --python 3.6<br/>pipenv install moto[server]<br/>pipenv install boto3<br/>pipenv install pyspark==2.4.3</span></pre><h1 id="24dc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">PySpark 代码使用了一个模仿的 S3 桶</h1><p id="b2f0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果您遵循了上述步骤，您应该能够成功运行以下脚本:</p><pre class="kk kl km kn gt om on oo op aw oq bi"><span id="1be3" class="nt la it on b gy or os l ot ou">import os<br/>import signal<br/>import subprocess</span><span id="3fba" class="nt la it on b gy ov os l ot ou">import boto3<br/>from pyspark.sql import DataFrame<br/>from pyspark.sql import SparkSession<br/></span><span id="bad4" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># start moto server, by default it runs on localhost on port 5000.<br/></strong>process = subprocess.Popen(<br/>    "moto_server s3", stdout=subprocess.PIPE,<br/>    shell=True, preexec_fn=os.setsid<br/>)</span><span id="9880" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># create an s3 connection that points to the moto server. <br/></strong>s3_conn = boto3.resource(<br/>    "s3", endpoint_url="http://127.0.0.1:5000"<br/>)<br/><strong class="on iu"># create an S3 bucket.<br/></strong>s3_conn.create_bucket(Bucket="bucket")</span><span id="8e7c" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># configure pyspark to use hadoop-aws module.<br/># notice that we reference the hadoop version we installed.</strong><br/>os.environ[<br/>    "PYSPARK_SUBMIT_ARGS"<br/>] = '--packages "org.apache.hadoop:hadoop-aws:2.7.3" pyspark-shell'</span><span id="6026" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># get the spark session object and hadoop configuration.<br/></strong>spark = SparkSession.builder.getOrCreate()<br/>hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()</span><span id="807c" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># mock the aws credentials to access s3.<br/></strong>hadoop_conf.set("fs.s3a.access.key", "dummy-value")<br/>hadoop_conf.set("fs.s3a.secret.key", "dummy-value")<br/><strong class="on iu"># we point s3a to our moto server.<br/></strong>hadoop_conf.set("fs.s3a.endpoint", "http://127.0.0.1:5000")<br/><strong class="on iu"># we need to configure hadoop to use s3a.<br/></strong>hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")</span><span id="b98b" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># create a pyspark dataframe.<br/></strong>values = [("k1", 1), ("k2", 2)]<br/>columns = ["key", "value"]<br/>df = spark.createDataFrame(values, columns)</span><span id="0eac" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># write the dataframe as csv to s3.<br/></strong>df.write.csv("s3://bucket/source.csv")</span><span id="f477" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># read the dataset from s3<br/></strong>df = spark.read.csv("s3://bucket/source.csv")</span><span id="7c66" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># assert df is a DataFrame<br/></strong>assert isinstance(df, DataFrame)</span><span id="cd81" class="nt la it on b gy ov os l ot ou"><strong class="on iu"># shut down the moto server.<br/></strong>os.killpg(os.getpgid(process.pid), signal.SIGTERM)</span><span id="7dbc" class="nt la it on b gy ov os l ot ou">print("yeeey, the test ran without errors.")</span></pre><p id="3882" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">将上述代码复制粘贴到一个名为“py spark-mocked-S3 . py”<em class="oi"/>的文件中，并执行:</p><pre class="kk kl km kn gt om on oo op aw oq bi"><span id="650c" class="nt la it on b gy or os l ot ou">pipenv shell<br/>python pyspark-mocked-s3.py</span></pre><p id="ca50" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">输出将类似于:</p><pre class="kk kl km kn gt om on oo op aw oq bi"><span id="ac69" class="nt la it on b gy or os l ot ou">(glue-test-1) bash-3.2$ python pyspark-mocked-s3.py<br/>* Running on <a class="ae ki" href="http://127.0.0.1:5000/" rel="noopener ugc nofollow" target="_blank">http://127.0.0.1:5000/</a> (Press CTRL+C to quit)</span><span id="5650" class="nt la it on b gy ov os l ot ou">...</span><span id="f0e2" class="nt la it on b gy ov os l ot ou">127.0.0.1 - - [28/Nov/2019 20:54:59] "HEAD /bucket/source.csv/part-00005-0f74bb8c-599f-4511-8bcf-8665c6c77cc3-c000.csv HTTP/1.1" 200 -<br/>127.0.0.1 - - [28/Nov/2019 20:54:59] "GET /bucket/source.csv/part-00005-0f74bb8c-599f-4511-8bcf-8665c6c77cc3-c000.csv HTTP/1.1" 206 -</span><span id="feb3" class="nt la it on b gy ov os l ot ou">yeeey, the test ran without errors.</span></pre><h1 id="1ff1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">编写测试用例</h1><p id="e1c2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">上面脚本中显示的原则在我的 repo<a class="ae ki" href="https://github.com/vincentclaes/testing-glue-pyspark-jobs" rel="noopener ugc nofollow" target="_blank">testing-glue-py spark-jobs</a>中以更结构化的方式应用。</p><p id="778d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在这个 repo 中，你会发现一个 Python 文件，<strong class="lt iu"> test_glue_job.py </strong>。这个文件是 Glue PySpark 作业的一个测试用例的例子。它结合了上述逻辑和我写的一篇关于<a class="ae ki" rel="noopener" target="_blank" href="/testing-serverless-services-59c688812a0d">测试无服务器服务</a>的文章中概述的原则。查看测试用例，并按照自述文件中的步骤运行测试。为了方便起见，我在下面添加了测试用例。</p><p id="11b5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">祝你好运！</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="ow ox l"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk"><a class="ae ki" href="https://gist.github.com/vincentclaes/5d78f3890a117c613f23e3539e5e3e5d" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/vincentclaes/5d78f3890a117c613f23e3539e5e3e5d</a></figcaption></figure><p id="6add" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">【https://stackoverflow.com/a/50242383/1771155<br/><a class="ae ki" href="https://gist.github.com/tobilg/e03dbc474ba976b9f235" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/tobilg/e03dbc474ba976b9f235</a><br/><a class="ae ki" href="https://github.com/spulec/moto/issues/1543#issuecomment-429000739" rel="noopener ugc nofollow" target="_blank">https://github . com/spulec/moto/issues/1543 # issue comment-429000739</a></p></div></div>    
</body>
</html>