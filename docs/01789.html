<html>
<head>
<title>Feature Selection and Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择和降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-and-dimensionality-reduction-f488d1a035de?source=collection_archive---------2-----------------------#2019-03-25">https://towardsdatascience.com/feature-selection-and-dimensionality-reduction-f488d1a035de?source=collection_archive---------2-----------------------#2019-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fb8b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索 Kaggle“不要过度适应 II”竞赛中的特征选择和降维技术</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ce53b14d918cef6f91834ed12ec5ba88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ts7RrSQGUXxymsLwKWDXxg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/photos/CMuFjjDHI70?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae ky" href="https://unsplash.com/search/photos/collection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据<a class="ae ky" href="https://en.wikipedia.org/wiki/Feature_selection" rel="noopener ugc nofollow" target="_blank">维基百科</a>，“特征选择是选择相关特征子集用于模型构建的过程”，或者换句话说，选择最重要的特征。</p><p id="49dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在正常情况下，领域知识起着重要的作用，我们可以选择我们认为最重要的特性。例如，在预测房价时，卧室的数量和面积通常被认为是重要的。</p><p id="f01b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，在“不要过度适应”II 竞赛中，使用领域知识是不可能的，因为我们有一个二元目标和 300 个“神秘起源”的连续变量，迫使我们尝试自动特征选择技术。</p><p id="52dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的笔记本可以在这里找到<a class="ae ky" href="https://www.kaggle.com/tboyle10/feature-selection" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="0335" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">特征选择与降维</h1><p id="0a87" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通常，特征选择和降维被组合在一起(如本文中所示)。虽然这两种方法都用于减少数据集中的要素数量，但有一个重要的区别。</p><p id="255c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征选择是简单地选择和排除给定的特征<strong class="lb iu">而不改变它们</strong>。</p><p id="460d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">维度缩减<strong class="lb iu">将</strong>特征转换到一个更低的维度。</p><p id="1e98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将探讨以下特征选择和维度缩减技术:</p><h2 id="8abf" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">特征选择</h2><ul class=""><li id="9d64" class="ne nf it lb b lc mn lf mo li ng lm nh lq ni lu nj nk nl nm bi translated">移除缺少值的要素</li><li id="6a77" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">移除方差较小的要素</li><li id="4546" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">移除高度相关的要素</li><li id="28bd" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">单变量特征选择</li><li id="518c" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">递归特征消除</li><li id="ca49" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">使用 SelectFromModel 进行特征选择</li></ul><h2 id="0719" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">降维</h2><ul class=""><li id="2e39" class="ne nf it lb b lc mn lf mo li ng lm nh lq ni lu nj nk nl nm bi translated">主成分分析</li></ul><h1 id="0488" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">基线模型</h1><p id="eb69" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将使用逻辑回归作为基线模型。我们首先分成测试集和训练集，并缩放数据:</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="29f7" class="ms lw it nt b gy nx ny l nz oa"># prepare for modeling<br/>X_train_df = train.drop(['id', 'target'], axis=1)<br/>y_train = train['target']</span><span id="1aed" class="ms lw it nt b gy ob ny l nz oa"># scaling data<br/>scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train_df)</span><span id="91e9" class="ms lw it nt b gy ob ny l nz oa">lr = LogisticRegression(solver='liblinear')<br/>lr_scores = cross_val_score(lr,<br/>                            X_train,<br/>                            y_train,<br/>                            cv=5,<br/>                            scoring='roc_auc')</span><span id="8770" class="ms lw it nt b gy ob ny l nz oa">print('LR Scores: ', lr_scores)</span><span id="31d9" class="ms lw it nt b gy ob ny l nz oa">LR Scores:  [0.80729167 0.71875    0.734375   0.80034722 0.66319444]</span></pre><p id="f937" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以从交叉验证分数的变化中看到模型过度拟合。我们可以尝试通过特征选择来提高这些分数。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="4a40" class="lv lw it bd lx ly oj ma mb mc ok me mf jz ol ka mh kc om kd mj kf on kg ml mm bi translated">移除缺少值的要素</h1><p id="a0e0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">检查缺失值是任何机器学习问题的良好开端。然后，我们可以删除超出我们定义的阈值的列。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="cf2a" class="ms lw it nt b gy nx ny l nz oa"># check missing values<br/>train.isnull().any().any()</span><span id="0f56" class="ms lw it nt b gy ob ny l nz oa">False</span></pre><p id="9dc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，对于我们的降维工作来说，这个数据集没有缺失值。</p><h1 id="2fc3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">移除方差较小的要素</h1><p id="f0e8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在 sklearn 的特征选择模块中我们找到了<code class="fe oo op oq nt b">VarianceThreshold</code>。它会移除方差未达到某个阈值的所有要素。默认情况下，它会移除方差为零的要素或所有样本值相同的要素。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="f36a" class="ms lw it nt b gy nx ny l nz oa">from sklearn import feature_selection<br/><br/>sel = feature_selection.VarianceThreshold()<br/>train_variance = sel.fit_transform(train)<br/>train_variance.shape</span><span id="a4d6" class="ms lw it nt b gy ob ny l nz oa">(250, 302)</span></pre><p id="c308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比赛描述说我们的特征都是连续的。从上面我们可以看到，所有列中没有具有相同值的特性，因此我们在这里没有要删除的特性。</p><p id="ad83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以随时重新审视这一技术，并考虑移除方差较低的特征。</p><h1 id="3ae1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">移除高度相关的要素</h1><p id="2cae" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">高度相关或共线的特征会导致过度拟合。</p><p id="d1d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当一对变量高度相关时，我们可以删除其中一个以降低维度，而不会丢失太多信息。我们应该保留哪一个？与目标相关性更高的那个。</p><p id="aac0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们探索一下我们的特征之间的相互关系:</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="8ce9" class="ms lw it nt b gy nx ny l nz oa"># find correlations to target<br/>corr_matrix = train.corr().abs()</span><span id="2631" class="ms lw it nt b gy ob ny l nz oa">print(corr_matrix['target'].sort_values(ascending=False).head(10))</span><span id="bc31" class="ms lw it nt b gy ob ny l nz oa">target    1.000000<br/>33        0.373608<br/>65        0.293846<br/>217       0.207215<br/>117       0.197496<br/>91        0.192536<br/>24        0.173096<br/>295       0.170501<br/>73        0.167557<br/>183       0.164146</span></pre><p id="b19f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们看到了与我们的目标变量高度相关的特征。特征 33 与目标的相关性最高，但是相关性值只有 0.37，所以相关性很弱。</p><p id="4c3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以检查功能与其他功能的相关性。下面我们可以看到一个相关矩阵。看起来我们的特征没有一个高度相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/b35e1f26460ff5f670dd60c0b3277fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBzmHiAstugZ1tNL1QlIdQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Correlation Matrix</figcaption></figure><p id="370f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们尝试删除相关值大于 0.5 的要素:</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="1b1d" class="ms lw it nt b gy nx ny l nz oa"># Find index of feature columns with high correlation<br/>to_drop = [column for column in matrix.columns if any(matrix[column] &gt; 0.50)]<br/>print('Columns to drop: ' , (len(to_drop)))</span><span id="bda3" class="ms lw it nt b gy ob ny l nz oa">Columns to drop: 0</span></pre><p id="4f63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用高度相关的特性，我们没有要删除的列。让我们继续探索其他策略。</p><h1 id="e2fb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">单变量特征选择</h1><p id="fe4e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">单变量特征选择通过基于单变量统计测试选择最佳特征来工作。</p><p id="53fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用 sklearn 的 SelectKBest 来选择一些要保留的特性。该方法使用统计测试来选择与目标具有最高相关性的特征。这里我们将保留前 100 个功能。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="73de" class="ms lw it nt b gy nx ny l nz oa">from sklearn.feature_selection import SelectKBest, f_classif</span><span id="fab5" class="ms lw it nt b gy ob ny l nz oa"><em class="os"># feature extraction</em><br/>k_best = SelectKBest(score_func=f_classif, k=100)</span><span id="4eb8" class="ms lw it nt b gy ob ny l nz oa"><em class="os"># fit on train set</em><br/>fit = k_best.fit(X_train, y_train)</span><span id="7560" class="ms lw it nt b gy ob ny l nz oa"><em class="os"># transform train set</em><br/>univariate_features = fit.transform(X_train)</span></pre><h1 id="a6e4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">递归特征消除</h1><p id="26f3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">递归特征选择通过消除最不重要的特征来工作。它以递归方式继续，直到达到指定的特征数。递归消除可用于任何通过<code class="fe oo op oq nt b">coef_</code>或<code class="fe oo op oq nt b">feature_importances_</code>为特征分配权重的模型</p><p id="dc46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们将使用随机森林来选择 100 个最佳功能:</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="71a7" class="ms lw it nt b gy nx ny l nz oa">from sklearn.feature_selection import RFE</span><span id="45c6" class="ms lw it nt b gy ob ny l nz oa"># feature extraction<br/>rfe = RFE(rfc, n_features_to_select=100)</span><span id="795c" class="ms lw it nt b gy ob ny l nz oa"># fit on train set<br/>fit = rfe.fit(X_train, y_train)</span><span id="8fc4" class="ms lw it nt b gy ob ny l nz oa"># transform train set<br/>recursive_features = fit.transform(X_train)</span></pre><h1 id="ea18" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用<code class="fe oo op oq nt b">SelectFromModel</code>功能选择</h1><p id="15ad" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">像递归特征选择一样，sklearn 的<code class="fe oo op oq nt b">SelectFromModel</code>与任何具有<code class="fe oo op oq nt b">coef_</code>或<code class="fe oo op oq nt b">feature_importances_</code>属性的估计器一起使用。它会移除值低于设定阈值的要素。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="8b7e" class="ms lw it nt b gy nx ny l nz oa">from sklearn.feature_selection import SelectFromModel<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="8594" class="ms lw it nt b gy ob ny l nz oa"># define model<br/>rfc = RandomForestClassifier(n_estimators=100)</span><span id="d28b" class="ms lw it nt b gy ob ny l nz oa"># feature extraction<br/>select_model = feature_selection.SelectFromModel(rfc)</span><span id="8c54" class="ms lw it nt b gy ob ny l nz oa"># fit on train set<br/>fit = select_model.fit(X_train, y_train)</span><span id="d10a" class="ms lw it nt b gy ob ny l nz oa"># transform train set<br/>model_features = fit.transform(X_train)</span></pre><h1 id="9d33" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">主成分分析</h1><p id="ece8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">PCA(主成分分析)是一种降维技术，它将数据投影到一个更低维的空间中。</p><p id="b6a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然有许多有效的降维技术，主成分分析是我们在这里探讨的唯一例子。</p><p id="9913" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA 在许多情况下都很有用，但特别是在多重共线性过大或预测因子的解释不重要的情况下。</p><p id="9cb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们将应用 PCA 并保留 90%的方差:</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="808f" class="ms lw it nt b gy nx ny l nz oa">from sklearn.decomposition import PCA<br/># pca - keep 90% of variance<br/>pca = PCA(0.90)</span><span id="366c" class="ms lw it nt b gy ob ny l nz oa">principal_components = pca.fit_transform(X_train)<br/>principal_df = pd.DataFrame(data = principal_components)</span><span id="b46e" class="ms lw it nt b gy ob ny l nz oa">print(principal_df.shape)</span><span id="fec9" class="ms lw it nt b gy ob ny l nz oa">(250, 139)</span></pre><p id="5358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到剩下的 139 个特征解释了我们数据中 90%的差异。</p><h1 id="71a0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="618d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">特征选择是任何机器学习过程的重要部分。在这里，我们探讨了几种有助于提高模型性能的特征选择和降维方法。</p></div></div>    
</body>
</html>