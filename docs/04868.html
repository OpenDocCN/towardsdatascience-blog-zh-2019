<html>
<head>
<title>Style Transfer with GANs on HD Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高清图像上的 GANs 风格转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716?source=collection_archive---------0-----------------------#2019-07-23">https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716?source=collection_archive---------0-----------------------#2019-07-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="813d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用 GANs 生成高清图像，而无需昂贵的硬件</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0251101806fb5ea838a769a8ca74114e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KI61LPKTfOfVRL3vGDDyxA.jpeg"/></div></div></figure><h1 id="81a9" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">简介</strong></h1><p id="6d48" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">最近的一些研究探索了使用 GANs(生成对抗网络)生成高清晰度图像(1024×1024 像素)的一些方法和技术。看到<strong class="lo iu">超逼真的</strong>，由算法生成的人脸、动物和其他事物的高清图像，尤其是记得几年前的第一张 GAN 图像，令人难以置信地惊讶。我们很快就从低质量的像素化图像发展到了接近现实的<strong class="lo iu">图像</strong>:这是该领域研究进展有多快的一个非常清晰的证明。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/25e21d69b6338348d8ea334984a10742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZ5I8JsZTiL-aTl0r123Bg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">StyleGAN HD face samples</figcaption></figure><p id="f4c3" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">但是阅读这些最近的研究(最相关的是 Nvidia 的 StyleGAN 和 Google 的 BigGAN 论文),我总是发现一个方面能够降低我的惊讶和兴奋感:计算能力。发现<strong class="lo iu">巨大的计算能力</strong>被用来完成那些图像让我意识到在我和那些结果之间有一个不可逾越的障碍。光是这个想法就让我觉得研究中探索的整个新技术对我来说都很遥远，因此不那么令人惊讶。</p><p id="75d5" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">这就是为什么在这篇文章中，我想探讨如何使 GANs 和 HD 图像一起工作<strong class="lo iu">而不需要昂贵的硬件</strong>，为不一定有机会使用高级 GPU 的人打开新的机会。这里解释的一切都可以使用免费的<a class="ae ms" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank">谷歌合作实验室</a>平台来实现，该平台为你所有的机器/深度学习项目提供免费的 GPU。</p><h1 id="c7da" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">我们的目标</strong></h1><p id="24cc" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们将尝试在高清图像的两个域之间执行<strong class="lo iu">风格的传输</strong>，使用特殊但简单的 GAN 架构来执行我们的任务。更具体地说，我们将把梵高的绘画风格应用到风景的高分辨率图像上。公平地说，在过去的几年里，风格转换一直是计算机视觉中的一个热门话题；开创潮流的原始论文是<strong class="lo iu">《一种艺术风格的神经算法》</strong> (Gatys 等人)，在一个预训练的卷积网络上使用了一个内容和风格损失来执行任务。虽然这种方法可以在高清图像上工作，但它只能使用一个<strong class="lo iu">单幅图像</strong>(假设是“星夜”)作为画家风格的表示，这不是我们想要的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/b33d91154f693c99b2a2dc49d8d45f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5iiD4zYn6NyyQVaPO9lnw.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Style Transfer examples from the original paper</figcaption></figure><p id="d604" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">另一方面，GAN 通常需要一个图像域来进行训练，因此在我们的情况下能够完全捕捉画家的风格(CycleGAN 的论文显示了风格转移的有趣结果)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/e5dada49eb36ca046a371088d478be7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TXi0wpBvUmqnuaU9sZ_QHg.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">CycleGAN Style Transfer examples</figcaption></figure><p id="92f7" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">然而，训练 GANs 在计算上极其昂贵<strong class="lo iu"/>:高分辨率图像的生成只有在非常高端的硬件和长训练时间的情况下才有可能。我希望本文中解释的<strong class="lo iu">技巧和技术</strong>能够帮助你进行高清图像生成的冒险。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/1767ea5eb7266eda58cfad49b1be0983.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*VZdR-4TwnLT9W1Rmk4x1YA.jpeg"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Van Gogh paintings</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/831907b222795f7c17f93f56a9128996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHgqChPow9h85Ful5D-kwQ.jpeg"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/99f54e15eb5ef25a6e730c8a23de8296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7Qyasx0Yp10ZjlNT1PHIA.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">HD images translated to Van Gogh Style</figcaption></figure><p id="c54e" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我们开始吧！</p><h1 id="0ae5" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">架构</strong></h1><p id="10bb" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们要努力实现的叫做图像到图像的翻译(从域 A 到域 B)。有不同的方法和网络架构来实现它:最著名的可能是 CycleGAN，但也存在许多关于同一主题的其他论文。</p><p id="39b2" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在我的实验中，我使用了一个定制的架构，该架构包含一个作为鉴别器的暹罗网络和一个特殊的(但超级简单的)损失函数。我选择这种方法是因为它不依赖于任何损失中的每像素差异:这仅仅意味着网络不遵循生成图像的任何几何约束，因此能够创建更令人信服的图像转换(这在我们的情况下是有效的)。</p><p id="af49" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在我写的另一篇文章<a class="ae ms" rel="noopener" target="_blank" href="/a-new-way-to-look-at-gans-7c6b6e6e9737"> <strong class="lo iu">这里</strong> </a>中可以找到对这种架构及其工作原理的深入透彻的解释。</p><p id="dd4d" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">下面简单介绍一下暹罗甘建筑。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/123509fb888de3ce0847d9f582e951bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKpInVmPEvcOzaFvKjsP1g.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Siamese GAN Architecture</figcaption></figure><p id="aebc" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">它由单个生成器(G)和鉴别器(D)组成:G 以一幅图像为输入，输出翻译后的图像；d 以一幅图像作为输入，输出一个潜在向量。</p><p id="569b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">暹罗鉴别器有两个目标:告诉 G 如何生成更真实的图像，并在这些假图像中保持与原始图像的相关性(相同的“内容”)。</p><p id="d511" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">调用<em class="mz"> A1、</em>A2、<em class="mz"> B1、B2 </em>分别来自 A 域和 B 域的随机图像、X 域的随机图像、<em class="mz"> G(X) </em>生成器生成的图像，鉴别器必须将图像编码成<strong class="lo iu">矢量</strong><em class="mz">【D(X)</em>如:</p><p id="3a82" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">1.<em class="mz"> D(B1) </em>一定离一个固定点(例如原点)很近(欧几里德距离)，而<em class="mz"> D(G(A1)) </em>一定离同一点很远。因此，更接近固定点的向量代表更真实的图像。另一方面，生成器试图以经典的对抗方式最小化从<em class="mz"> D(G(A1)) </em>到固定点的距离。</p><p id="0bca" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">2.<em class="mz"> (D(A1)-D(A2)) </em>必须与<em class="mz"> (D(G(A1))-D(G(A2))) </em>相似(余弦相似)，才能保留<em class="mz"> A </em>与<em class="mz"> G(A) </em>之间的‘内容’。发生器和鉴别器都参与这个目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/cb3c690b391cafc42033e809ad076c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p42h4mhKBBxUw5pnO3R_cw.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Siamese Discriminator</figcaption></figure><p id="352b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">有了这两个约束(损失)，第一个依赖于向量的大小，而第二个依赖于向量之间的角度，我们的全部目标得到满足，我们可以实现从域 A 到域 b 的图像到图像转换的最终目标。我真的建议您阅读<a class="ae ms" rel="noopener" target="_blank" href="/a-new-way-to-look-at-gans-7c6b6e6e9737"> <strong class="lo iu">这篇文章</strong> </a>，其中我给出了该架构的更全面和深入的解释，同时展示了插图和示例。</p><p id="88ff" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">现在我们已经锁定了架构，让我们来探索如何以及向网络提供什么，以实现高清图像生成。</p><h1 id="159f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">图像提取</strong></h1><p id="4af6" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们需要 2 个高清图像数据集:在我们的例子中，我们将使用一个风景数据集(域 A)和一个梵高画作数据集(域 B)。请记住，您选择处理的图像越大，对这些图像进行预处理(切割、调整大小)所需的时间就越长(尽管它<strong class="lo iu">不会</strong>增加专门用于训练网络的时间！).</p><p id="54e1" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">现在，我们需要选择将被馈送到生成器的图像的大小:显然，我们不能使用来自数据集的整个 HD 图像的大小，否则训练时间和网络大小将是巨大的，并且不会解决任何问题。因此，我们选择了一个足够小的 SxS(例如 64x64 像素),这样训练时间可以得到控制，一切都保持计算上可行，即使对于中端 GPU 也是如此(就像在<a class="ae ms" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank">谷歌合作实验室</a>上免费提供的那些)。</p><p id="67d7" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">因此，正如您可能已经想到的，图像在被馈送到发生器之前，必须被剪切(或裁剪)成更小的 SxS 图像。因此，在读取图像并将其转换为张量后，我们对图像执行<strong class="lo iu">随机 SxS 裁剪</strong>，将其添加到一个批处理中，并将该批处理馈送到网络。这听起来非常简单，事实也确实如此！</p><p id="3dea" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">现在，假设我们使用这种方法训练一个 GAN，直到每一个小的 SxS 裁剪都被生成器以一种令我们满意的方式转换为梵高风格:我们现在如何将整个高清图像从域 A 转换到域 B？</p><p id="e0ad" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">同样，非常简单:<strong class="lo iu">图像被分成</strong>小的 SxS 块(如果 HD 图像的大小是 BxB，那么我们将有(B//S)x(B//S)个小的 SxS 图像)，每个 SxS 图像被生成器翻译，最后<strong class="lo iu">所有的东西被重新组合在一起</strong>。</p><p id="0025" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">然而，如果我们尝试使用这种从较大图像中提取较小图像的简单思想来训练 GAN，在测试期间，我们很快就会注意到一个相当恼人的问题:由我们想要翻译的大图像提取的小图像，当由生成器转换到域 B 时，<strong class="lo iu">不会</strong> <strong class="lo iu">有机地融合在一起</strong>。每个 SxS 图像的边缘在最终构图中清晰可见，破坏了原本成功的风格转移的“魔力”。这是一个相对较小的问题，但可能相当令人讨厌:即使使用基于像素的方法，如 cycle gan<strong class="lo iu">相同的障碍仍然出现</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/3dfbfac3ed298e59b8ee216ad180e420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ep1BH8fhgLwwYYegXmrTJQ.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">The edges are visible</figcaption></figure><p id="aee6" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">怎么才能解决呢？</p><p id="e001" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我使用的解决方案很容易理解，在我看来也很优雅，它代表了我希望你能从这篇文章中记住(也许会用到)的<strong class="lo iu">核心思想</strong>。</p><p id="566f" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">首先，我们需要重新审视我们的数据管道:在我们直接从 BxB HD 图像中剪切 SxS 裁剪之前，我们现在必须得到<strong class="lo iu"> 2Sx2S 裁剪</strong>(如果 S=64，那么我们需要 128x128 裁剪)。然后，在定义了我们的生成器之后，我们创建了一个名为 Combo 的<strong class="lo iu">新模型</strong>，它执行以下操作:</p><p id="e016" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">1.取一批 2Sx2S 图像(来自 A 域)作为输入(<em class="mz">INP</em>)；</p><p id="e812" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">2.将<em class="mz"> INP </em>中的每幅图像切割成 4 幅 SxS 图像(<em class="mz">INP cut</em>)；</p><p id="e92e" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">3.将<em class="mz">input</em>的 4 幅 SxS 图像中的每一幅输入到发生器，并获得<em class="mz"> OUTCUT </em>(与<em class="mz">input</em>的形状完全相同，但每幅 SxS 图像都有翻译版本)；</p><p id="cc4b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">4.将每组 4 张 SxS 图像加入<em class="mz"> OUTCUT </em>中，取出<em class="mz"> OUT </em>(与<em class="mz"> INP </em>形状完全相同，但每张 2Sx2S 图像有翻译版本)；</p><p id="8646" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">5.输出输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/b96c285e8a4e02eff8c14ffd8d69b820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQz83kgo5m7-QMwyloVYfA.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Combo model: cutting, translating, joining</figcaption></figure><p id="b449" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">Combo 的输出然后作为输入传递给鉴别器，鉴别器现在接受比以前大一倍的输入(2Sx2S)。这个小小的调整不需要太多的计算时间，并且可以有效地<strong class="lo iu">解决我们之前的问题</strong>。怎么会？</p><p id="ae34" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">现在，生成器被迫生成关于<strong class="lo iu">边缘和颜色</strong>一致的图像，因为鉴别器不会将不一致的合并图像归类为真实图像，因此会通知生成器可以改进的地方。深入一点，生成器被迫学习如何在 SxS 图像的 4 个边缘中的每一个上生成逼真的边缘:在最终的 2x2 合并图像中，4 个边缘中的每一个都与另一个接触，甚至一个生成不好的边缘都会破坏 2x2 图像的真实感。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/37028c7654c9955fec772857e75468d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7a9hY99y7FjExIC1FN5ovg.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Samples during training: (from left to right) images from domain A, translated images (AB), images from domain B</figcaption></figure><h1 id="5781" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">所有的事情加在一起</strong></h1><p id="475f" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">为了确保到此为止的一切都清楚明白，让我们总结一下整个网络是如何工作的。</p><p id="3442" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">目标是将 B 的样式应用于 A 中的图像。大小为 2Sx2S 的图像是从域 A 和 B 中的高分辨率图像中剪切的。来自 A 的图像是 Combo 的输入；该模型将图像切割成 4 个较小的图像(SxS)，然后使用生成器 G 转换它们，最后将它们连接在一起。我们称这些假图像为 AB。</p><p id="88a4" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">现在让我们来关注一下<strong class="lo iu">连体鉴别器 D </strong>:其输入的大小是生成器输入(2Sx2S)大小的两倍，而输出是大小为<em class="mz"> LENVEC </em>的向量。</p><p id="8ea8" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">D 将图像编码成向量 D(X ),例如:</p><p id="e882" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">1.D(B)必须靠近原点(大小为<em class="mz"> VECLEN </em>的零点向量):</p><p id="b5c2" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">LossD1 是 D(B)到原点的欧氏距离的平方，所以<strong class="lo iu">Eucl(D(A))</strong>；</p><p id="2253" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">2.D(AB)必须远离原点:</p><p id="5b7c" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">LossD2 是(<strong class="lo iu"> max(0，cost — Eucl(D(AB)))) </strong></p><p id="dd4a" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">3.变换向量(D(A1)-D(A2))和(D(AB1)-D(AB2))必须是相似的向量，以保留图像的“内容”:</p><p id="9d8d" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">LossD3 是<strong class="lo iu">余弦 _ 相似度(D(A1)-D(A2)，D(AB1)-D(AB2)) </strong></p><p id="a6b5" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">另一方面，<strong class="lo iu">生成器</strong>必须生成(结合的)图像 AB，例如:</p><p id="a314" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">1.D(AB)必须靠近原点:</p><p id="131b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">LossG1 是<strong class="lo iu"> Eucl(D(AB)) </strong></p><p id="daea" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">2.变换向量(D(A1)-D(A2))和(D(AB1)-D(AB2))必须是相似的向量(与鉴别器的目的相同):</p><p id="ecac" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">LossG2 是<strong class="lo iu">余弦 _ 相似度(D(A1)-D(A2)，D(AB1)-D(AB2)) </strong></p><p id="2385" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在我的文章<a class="ae ms" rel="noopener" target="_blank" href="/a-new-way-to-look-at-gans-7c6b6e6e9737"> <strong class="lo iu"> here </strong> </a>中可以找到关于这些损失如何工作的更深入的解释，在那里我详细解释了暹罗鉴别器是如何工作的(我认为这值得一读！).</p><p id="1f1a" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">就是这样！</p><p id="0cb4" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">按照这种方法，生成器能够学习如何生成小的风格化图像，这些图像可以被连接在一起<strong class="lo iu">而没有任何边缘差异</strong>。因此，当翻译整个高清图像时，在将它切割成单独的较小 SxS 图像并将它们馈送到生成器之后，我们能够将它们连接在一起成为最终的、视觉上令人愉快的和连贯的高清图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0251101806fb5ea838a769a8ca74114e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KI61LPKTfOfVRL3vGDDyxA.jpeg"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/99f54e15eb5ef25a6e730c8a23de8296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7Qyasx0Yp10ZjlNT1PHIA.jpeg"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/831907b222795f7c17f93f56a9128996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHgqChPow9h85Ful5D-kwQ.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Translation examples on HD images: although not perfect, realistic brush strokes are generated and the images look quite coherent. Solutions might be networks fine tuning and bigger capacity</figcaption></figure><h1 id="5907" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">结论</strong></h1><p id="cb53" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">本文中解释的技术仍然存在一些我们需要解决的问题。</p><p id="eca4" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">如果选择极高清晰度的图像，用于训练网络<strong class="lo iu">的较小作物可能不包含任何相关信息</strong>(它们可能只是纯色，类似于单个像素)，因此训练可能不会成功:生成器和鉴别器都需要某种信息来处理(鉴别器必须根据图像的“内容”对图像进行编码)，如果该信息不可用，可能会面临一些问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/bd132d367772e6eed36787f945cbf6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6QhmU7L1A7cDLjm7N_xVg.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Failure case: the generator “hallucinates” incoherent colors and shapes in some areas</figcaption></figure><p id="fbbe" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">即使训练成功结束，当连接具有非常高分辨率的图像的所有不同裁剪时，每个小的翻译图像的<strong class="lo iu">风格贡献</strong>对于整个高清图像来说是不够的，整个高清图像通常看起来与原始图像相似，只是颜色发生了变化。</p><p id="71fd" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在我的实验中，我发现，对于训练阶段，使用调整大小(较低分辨率)的 HD 数据集版本，同时在转换时切换到整个 HD 图像，对于第一个问题肯定有帮助。</p><p id="7b30" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">这项技术给<strong class="lo iu">留下了很多有待探索的空间</strong>:不同于传统风格转换的其他类型的图像翻译也是可能的。重要的是要记住，本例中的生成器不知道整个高清图像的上下文，只“看到”较低分辨率的裁剪。因此，给生成器一些<strong class="lo iu">上下文</strong>(可能以编码的‘上下文向量’的形式)？)可以扩展该技术的应用范围，<strong class="lo iu">为更复杂的“上下文感知”类型的高清图像翻译(对象到其他对象、人脸、动物)提供了可能性</strong>。</p><p id="da3b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">因此，正如你可能已经理解的，可能性是无穷无尽的，尚未被发现！</p><p id="ad74" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我很乐意真诚地<strong class="lo iu">感谢</strong>您对本文的关注，非常感谢，我希望您带着新的东西离开。</p><p id="3d78" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">玩得开心！</p></div></div>    
</body>
</html>