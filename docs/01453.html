<html>
<head>
<title>Implementing deep learning network from scratch. Scala example.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始实现深度学习网络。Scala 例子。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-deep-learning-from-scratch-scala-example-340817ce7760?source=collection_archive---------14-----------------------#2019-03-07">https://towardsdatascience.com/implementing-deep-learning-from-scratch-scala-example-340817ce7760?source=collection_archive---------14-----------------------#2019-03-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="f3d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在过去的几年里，深度学习受到了很多关注。它最初是一种巫术崇拜，现在正在成为一项非常标准的工程任务。它变得不那么神奇，但更像是一个成熟的工具集，可以解决各种各样与数据相关的问题。</p><p id="9e41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管如此，仍然有一个神秘的地方，因为它并不清楚这个东西实际上如何能够自己学习，甚至在没有程序员直接干预的情况下“深入”学习。让我们试着理解这一点。</p><h1 id="2404" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">简而言之就是神经网络。</h1><p id="5eac" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">“深度学习”的概念指的是一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>，它在某种程度上模仿了我们大脑的工作模式。基本上，它是关于通过连接层的链发送输入，其中每一层对最终结果产生自己的影响。</p><p id="a681" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实际的学习是通过迭代搜索每层必须提供的最佳可能影响/权重来实现的，以便获得我们需要的输出。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/2f8627d10670ff49b6ff182e2a09e2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*42mUgWLwyC0mfiGeDV1vlA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 1. Neural Network with 2 hidden layers</figcaption></figure><p id="81f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是在我们看一看实际的实现之前，理解所有这些层(也称为隐藏层)的用途是很重要的。<a class="ae lr" href="https://en.wikipedia.org/wiki/XOR_gate" rel="noopener ugc nofollow" target="_blank"> XOR </a>的问题说明了一切。正如你在图 2 的<em class="me">中所看到的，</em>你找不到任何一个线性函数能把 A 的面积和 B 的面积分开，因为你可以用<a class="ae lr" href="https://en.wikipedia.org/wiki/AND_gate" rel="noopener ugc nofollow" target="_blank">和</a>和<a class="ae lr" href="https://en.wikipedia.org/wiki/OR_gate" rel="noopener ugc nofollow" target="_blank">或</a>来做。中间有一个交叉点，不允许我们决定我们是在 A 段还是在 B 段。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mf"><img src="../Images/c2a4620c1ccdd0843ec31e4a769af22d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*CkyR_iOr3DPpaZ7Yz6LwHA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 2. AND, OR, XOR gates</figcaption></figure><p id="1b6c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了找到答案，我们用额外的维度(或者更多的维度)来扩展我们的 2D 空间，所以你最终可以把一个特征从另一个特征中分离出来。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/b276a0b5bf79c4c39bf02f19f77c2991.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*lzYNhxaWYWm5255W_OGBDQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Figure 3. 3D space of a XOR gate</figcaption></figure><p id="097c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就神经网络而言，额外维度只是另一个隐藏层。所以我们所需要的——是弄清楚这另一个维度是否能让我们解决 XOR 问题。为此，我们将应用一种<a class="ae lr" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>算法——1975 年发表的关键概念，使互连层能够学习它们自己的权重，或者换句话说，学习它们对帮助我们在 XOR 中分离 A 和 B 有多大意义。</p><p id="fa6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有了反向传播算法，我们将基本上应用 3 个步骤来实现我们的“深度”学习:</p><ol class=""><li id="2e91" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated">前进传球</li><li id="36a5" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn mq mr ms mt bi translated">偶数道次</li><li id="9fea" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn mq mr ms mt bi translated">更新权重</li></ol><p id="a021" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="me">前向传递</em>是利用当前可用的权重进行预测。<em class="me">倒向</em> <em class="me">传递</em>给我们关于贡献的信息，每一次预测都做不到目标。有了这些信息，我们将<em class="me">修正</em>我们的维度权重，并希望在下一次迭代中，我们的预测将使我们更接近目标。</p><p id="1fa7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有不同的方法来确定重量。其中之一是<a class="ae lr" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>，我们也将在这里使用。</p><h1 id="ede9" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">Scala 中的实现。</h1><p id="99bc" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">在 Scala 代码中，这 3 个步骤的过程可能是这样的:</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Listing 1. Basic Neural Network Pattern</figcaption></figure><p id="a64e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们仔细看看我们的第一步— <em class="me"> forward </em>函数。我们将把它实现为递归的:</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Listing 2. Making prediction with forward pass</figcaption></figure><p id="d484" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="me">向前传球</em>负责:</p><p id="419d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">1.将权重应用于网络层</p><p id="c93f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.将该加权层通过<a class="ae lr" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">s 形激活</a>功能</p><p id="9da2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一个结果，我们在一个网络中得到一个新的分层预测列表。有了这个<em class="me">列表</em>之后，我们可以去寻找每个层的预测导致错过目标的错误(规则输出)。</p><p id="ba96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将从目标值之间的差异开始，我们希望达到我们在最后一步中所做的预测。由于我们预测的<em class="me">列表</em>是以后进先出的方式生成的，所以<em class="me">列表</em>中的第一个元素也是我们做出的最后一个预测，所以我们可以取一个给定的目标，看看我们离目标有多远。有了第一个误差值，我们就可以使用<em class="me">反向传播</em>模式<em class="me">来寻找其余的误差值。</em></p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Listing 3. Finding the first predictions error and passing it to backpropagation</figcaption></figure><p id="fd44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们在训练网络的最终端发现了一个误差幅度(或增量),我们可以用它来发现前一层的误差增量，因为我们知道这一层得到的预测。在我们进入另一个递归函数之前，每隔一个隐藏层都是如此。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Listing 4. Backpropagation pattern</figcaption></figure><p id="5076" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能会注意到，我们提供的权重是倒序的，这是因为计算反向路径的唯一方法是从网络的末端开始计算。</p><p id="af66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总的来说，我们到此为止。我们知道如何计算预测及其误差增量，以及如何使用它来更新权重。我们唯一需要做的是开始迭代我们的数据集，并将更新的权重应用于我们试图满足的预测。这样做很长时间，直到我们得到一个尽可能接近目标的权重。</p><p id="5893" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了验证你在每一次迭代中有多接近，你需要确定一个网络损耗。随着我们的网络学习，损失必须减少。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Listing 5. Calculation of a prediction’s loss</figcaption></figure><p id="68c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您运行为本博客提供的<a class="ae lr" href="https://github.com/zavalit/neural-network-example" rel="noopener ugc nofollow" target="_blank">示例实现</a>，您将会看到:</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Neural Network learning log</figcaption></figure><p id="8f01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，减少损失意味着我们的预测更接近他们应该有的目标值。如果我们看一看在训练过程结束时所做的预测，它们与预期值非常接近。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Training results. XOR Gate is fulfilled</figcaption></figure><h1 id="691b" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">有点小技巧。</h1><p id="654a" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">可能是最后一件还没有涉及到的事情，就是网络的初始权重。我们非常了解如何更新它们，但是我们首先从哪里得到它们呢？为了澄清这一点，我们需要后退一步，回顾一下 layer 预测的定义。我们已经看到，要制作一个，我们需要两个步骤:</p><ol class=""><li id="d918" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated"><em class="me">输入</em>和<em class="me">权重</em>的标量积:<strong class="js iu"> <em class="me"> net = np.dot(输入，权重)</em> </strong></li><li id="eb9c" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn mq mr ms mt bi translated">激活具有<em class="me"> sigmoid </em>功能的产品:<strong class="js iu"><em class="me">1/(1+NP . exp(-net))</em></strong></li></ol><p id="5891" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是理论上第一步实际上看起来是这样的:</p><blockquote class="nb"><p id="573f" class="nc nd it bd ne nf ng nh ni nj nk kn dk translated"><strong class="ak"> <em class="nl"> net = np.dot(输入，权重)+ b </em> </strong></p></blockquote><p id="7d70" class="pw-post-body-paragraph jq jr it js b jt nm jv jw jx nn jz ka kb no kd ke kf np kh ki kj nq kl km kn im bi translated">其中<em class="me"> b </em>代表<em class="me">偏差</em>或阈值，必须是另一个张量，负责在得到的<em class="me">网</em>被 sigmoid 激活之前对其进行调节。我们实际上还需要有一个<em class="me">偏差</em>，而不仅仅是<em class="me">权重</em>就像我们之前做的那样，听起来我们需要实现更多的东西。但是这里有一个技巧。</p><p id="8de6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了避免额外的复杂性，我们只需执行以下操作:</p><ol class=""><li id="7ff9" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated">向我们的训练集张量添加额外的一列(清单 6。第 3 行)</li><li id="8f98" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn mq mr ms mt bi translated">用相同的一列扩展层权重(清单 6。第 11 行)</li></ol><p id="a6ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此在我们的优化问题中加入了一个<em class="me">偏差</em>。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Listing 6. Prepare network weights and bias</figcaption></figure><p id="9838" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回到我们从哪里得到<em class="me">权重</em>的问题。看看第 1 行上的<em class="me"> generateRandomWeight </em>函数。这就是我们的权重最初的来源，它们或多或少是随机的。第一次意识到这一点是很奇怪的，预测的主干— <em class="me">权重</em>，可能只是随机生成的，并且在我们更新它们几次后仍然可以进行正确的预测。</p><h1 id="372e" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">结论。</strong></h1><p id="3dfb" class="pw-post-body-paragraph jq jr it js b jt lm jv jw jx ln jz ka kb lo kd ke kf lp kh ki kj lq kl km kn im bi translated">所以希望你能够看到“深度学习”非常接近于常规的编程任务。围绕这种软件和平的秘密，基本上基于两种主要模式:</p><ol class=""><li id="8845" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn mq mr ms mt bi translated">通过应用反向传播模式，确定我们的神经网络预测离实际目标有多远。</li><li id="4ce5" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn mq mr ms mt bi translated">通过借助于随机梯度下降模式更新层权重来逐渐减小该误差空间。</li></ol><p id="13bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可能是一些有用的链接:</p><ul class=""><li id="4fa6" class="ml mm it js b jt ju jx jy kb mn kf mo kj mp kn nr mr ms mt bi translated">这篇博文中使用的代码库:<a class="ae lr" href="https://github.com/zavalit/neural-network-example" rel="noopener ugc nofollow" target="_blank">https://github.com/zavalit/neural-network-example</a></li><li id="d0d5" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn nr mr ms mt bi translated">神经网络游乐场:<a class="ae lr" href="http://playground.tensorflow.org" rel="noopener ugc nofollow" target="_blank">http://playground.tensorflow.org</a></li><li id="2ae2" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn nr mr ms mt bi translated">深入学习(基于 MXNet):<a class="ae lr" href="http://d2l.ai" rel="noopener ugc nofollow" target="_blank">http://d2l . ai</a></li><li id="60ce" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn nr mr ms mt bi translated">从数学角度看密集连通层的工作:【https://www.youtube.com/watch?v=fXOsFF95ifk T4】</li><li id="6bcb" class="ml mm it js b jt mu jx mv kb mw kf mx kj my kn nr mr ms mt bi translated">Scala 中的 Numpy 实现<a class="ae lr" href="https://medium.com/@koen_95886/nice-article-b83b6b7afbda?source=post_info_responses---------0---------------------" rel="noopener">https://github.com/botkop/numsca</a></li></ul><p id="309a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="me"> PS。我想值得一提的是，这不是一篇关于用 Scala 编写的神经网络的生产就绪实现的博文。也许下次吧；)这里的主要焦点是尽可能透明和明显地展示基本模式。我希望你喜欢它。</em></p></div></div>    
</body>
</html>