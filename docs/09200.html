<html>
<head>
<title>What Is The Best Starter Model In Table Data ML?— Lessons from A High-rank Kagglers’ New Book</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">表数据 ML 中最好的入门模型是什么？——从一位高层卡格勒的新书中得到的教训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-the-best-starter-model-in-table-data-ml-lessons-from-a-high-rank-kagglers-new-book-f08b821db797?source=collection_archive---------30-----------------------#2019-12-05">https://towardsdatascience.com/what-is-the-best-starter-model-in-table-data-ml-lessons-from-a-high-rank-kagglers-new-book-f08b821db797?source=collection_archive---------30-----------------------#2019-12-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="903a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">《赢得 KAGGLE 的数据分析技术》一书</h2><div class=""/><div class=""><h2 id="e02f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在 Kaggle 或业务建模中开始建模的提示</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/425a71849036d1b74c8fd6788bcda984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VPhumpTEiYvulLzN"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@jontyson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jon Tyson</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="972d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是另一篇文章，介绍了新书<em class="mb">“赢得 Kaggle 的数据分析技术”，</em>中介绍的技巧，作者是三位高级 kaggler(不包括我自己，因此这不是个人提升！:) )</p><p id="3e24" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这本书本身的完整目录，见我的<a class="ae le" href="https://medium.com/@daydreamersjp/a-new-book-data-analysis-techniques-to-win-kaggle-is-a-current-best-and-complete-for-table-data-4af66a88388" rel="noopener">其他帖子</a>。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="acbc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我将谈论哪个模型是围绕特性和参数的首选，以及模型集成的其他选项。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="874d" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">拿到数据了😚！我要去做模特😄！但是等等，我能从哪里开始🙄？？</h1><p id="e79c" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated"><strong class="lh ja">在你把你的最终 ML 结果</strong>发给你的老板或 Kaggle 的排行榜或其他地方之前，你应该把这些模型和最终分数并排比较，用:</p><ul class=""><li id="3e06" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">他们的超参数完全调整好了，</li><li id="37d1" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">它们的功能被完全设计和利用，和/或</li><li id="3019" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">这些模型是成套的。</li></ul><p id="1899" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，<strong class="lh ja">在第一步，</strong> <strong class="lh ja">你的问题可能是“好吧，我知道要去哪里，但是我能从哪里开始呢？</strong>🤔<strong class="lh ja">“玩意儿。</strong></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="47b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过建模，普遍的看法是"<strong class="lh ja">特征工程最重要。</strong>“模型的改变可能也很重要，但不像特征工程那样重要；同样，超参数调整可能会影响模型性能，但对分数影响不大。</p><p id="7c0a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以，我们想决定什么样的特征工程最适合我们的建模，但我们至少需要一些模型来尝试，看看我们制作的特征是否适合进一步发展。那么，这里我们说“<strong class="lh ja">从哪里开始呢？？</strong>😟"</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/4cc70e9540aae79fb91649b677702888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2qN7ZJYTnKtmU5TKv6H5w.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Illustration of modeling cycle</figcaption></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="6b79" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">顶级卡格勒人的选择👈🏻是…</h1><p id="9fb6" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">首先讲结论，在表格数据分析(记住本书的主要主题是关于表格数据 ML)中，本书建议:</p><ul class=""><li id="2d08" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated"><strong class="lh ja">从 GBDT 开始</strong> <em class="mb">(本文稍后讨论)</em> <strong class="lh ja">、</strong></li><li id="0629" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">花费最多的时间进行特征工程(主要作者称占总时间的 80%)来查看对 GBDT 预测的影响，</strong></li><li id="f58c" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">不时检查超参数变化的影响，但还没有剧烈调整，</strong></li><li id="d357" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">根据任务和模型的多样性，考虑神经网络和/或线性模型</strong> <em class="mb">(本文稍后讨论)</em><strong class="lh ja"/></li><li id="5927" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">做 ensemble 时，考虑 Kaggle 常用的模型多样性的其他类型模型；kNN，随机森林，极度随机化树(ERT)，正则化贪婪森林(RGF)，场感知因式分解机(FFM) </strong> <em class="mb">(本文稍后讨论)</em> <strong class="lh ja">，</strong></li><li id="9c35" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">如果新方法更适合数据和/或任务，可能会改变验证策略。</strong></li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="8335" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">GBDT 是许多卡格勒人的首选。</p><blockquote class="nv"><p id="0996" class="nw nx iq bd ny nz oa ob oc od oe ma dk translated">对于第一个模型，我们希望它能够快速正确地运行，并成为一个基线模型。</p><p id="7ce6" class="nw nx iq bd ny nz oa ob oc od oe ma dk translated">我们不喜欢它需要太多的数据转换、功能工程或超参数调整。</p><p id="ad04" class="nw nx iq bd ny nz oa ob oc od oe ma dk translated">我们也希望它不要花太多时间运行，因为我们将尝试许多不同的输入选项一个接一个。</p><p id="733d" class="nw nx iq bd ny nz oa ob oc od oe ma dk translated">GBDT 可以跳过其中的任何一个。</p></blockquote><p id="c1c7" class="pw-post-body-paragraph lf lg iq lh b li of ka lk ll og kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我们将在下一节看到更多的细节。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="cff7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">GBDT 是受爱戴的💖这么多，但是为什么呢？</h1><p id="860a" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">GBDT 是基于决策树的模型。因此，模型训练中的核心行为是将节点分成两个分支。这使得 GBDT:</p><ul class=""><li id="e901" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated"><strong class="lh ja">不需要可变缩放</strong>，</li><li id="5ff2" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">不需要缺失值插补</strong>(如果变量缺失，分割规则也决定了记录转到哪个节点)。</li><li id="77c3" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">能够通过</strong> <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">标签编码</strong> </a>来处理分类变量(尽管因为 sklearn lightgbm 和 catboost APIs 及其原始包会在发现分类变量是分类变量时自动对其进行编码，但这种优势在实践中可能并不明显)</li><li id="f4c3" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja">能够在没有明确交互变量的情况下反映变量交互</strong>(这是通过多个不同的具有交互的变量的重复分割来实现的)。</li></ul><p id="5fdd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，这本书还提到了 GBDT 的好处:</p><ul class=""><li id="187c" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">良好的预测性能，</li><li id="6cf6" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">倾向于提供足够高的性能，而无需强烈的超参数调整，</li><li id="cc67" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">拥有不必要的变量不会对模型性能造成太大的损害，并且</li><li id="4d1a" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">能够默认处理稀疏矩阵对象，例如<code class="fe ok ol om on b">scipy.sparse</code>中的<code class="fe ok ol om on b">csr_matrix</code>或<code class="fe ok ol om on b">csc_matrix</code>。</li></ul><p id="6a45" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">使用 GBDT，我们可以设置一些默认参数，并可以围绕特征工程进行操作，直到找出下降变量，以及一些并行的超参数调整。</strong></p><p id="8ada" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下表显示了书中介绍的 xgboost 的默认超级参数集。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/09a970fa8a8b693a332b7884d3452976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fka7bIKxkACdAI88zq2vcw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">xgboost hyper-parameter defaults introduced in the book.</figcaption></figure><p id="030c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">调优时，一般<code class="fe ok ol om on b">max_depth</code> <em class="mb"> </em>和<code class="fe ok ol om on b">min_child_weight</code>是最先照顾的，比如<code class="fe ok ol om on b">max_depth</code> = 3~9，<code class="fe ok ol om on b">min_child_weigh</code> = 1~5。</p><p id="c943" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，<code class="fe ok ol om on b">subsamle</code>，<code class="fe ok ol om on b">colsample_bytree(_bylevel)</code>，<code class="fe ok ol om on b">alpha</code>，<code class="fe ok ol om on b">lambda</code>，<code class="fe ok ol om on b">gamma</code>，像<code class="fe ok ol om on b">subsample</code> = 0.6~1.0，<code class="fe ok ol om on b">colsample_bytree</code> = 0.6~1.0，<code class="fe ok ol om on b">alpha</code> = 1e-5，1e-2，0.1，1，100，<code class="fe ok ol om on b">gamma</code> = 0.0~0.4。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="89b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有三个常用的 GBDT 图书馆:</p><ul class=""><li id="2731" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated"><strong class="lh ja"> xgboost </strong></li><li id="17db" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja"> lightgbm </strong></li><li id="f9c9" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated"><strong class="lh ja"> catboost </strong></li></ul><p id="4707" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我不会深究每个模型本身。我建议查看一下 t <a class="ae le" rel="noopener" target="_blank" href="/catboost-vs-light-gbm-vs-xgboost-5f93620723db">的牛逼对比帖</a>。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="e5a3" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">神经网络是🖖的第二选择🏻</h1><p id="0030" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">在表数据中，广泛使用的神经网络是相当浅的，如 2 至 4 层，也是完全连接的；又名<strong class="lh ja">多层感知器(MLP) </strong>。</p><p id="1924" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">NN 的属性是:</p><ul class=""><li id="9495" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">需要估算缺失值，</li><li id="8f76" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">更好的模型训练需要缩放，</li><li id="1674" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">对超参数选择敏感，</li><li id="ee24" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">非线性和可变的相互作用可以在一定程度上得到反映，</li><li id="53d7" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">擅长多类分类，并且</li><li id="1010" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">使用 GPU 加速。</li></ul><p id="3383" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">神经网络不像 GBDT 那样方便，它没有激烈的调整和变量转换，但仍然取决于任务，它比 GBDT 工作得更好。</p><p id="5f62" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有针对 MLP 的 sklearn 包装器，但是使用特定的框架在神经网络建模中也很常见。</p><ul class=""><li id="b1d8" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">克拉斯</li><li id="aae6" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">pytorch</li><li id="1885" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">张量流</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="01bb" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">其他型号💮</h1><p id="3b24" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">其他模型可能无法单独击败 GBDT 和 NN 的性能，但<strong class="lh ja">可以用于在集合中获得多样性</strong>。</p><h2 id="eb30" class="op mk iq bd ml oq or dn mp os ot dp mt lo ou ov mv ls ow ox mx lw oy oz mz iw bi translated">线性模型</h2><ul class=""><li id="ef5b" class="ng nh iq lh b li nb ll nc lo pa ls pb lw pc ma nl nm nn no bi translated">性能较低，但在数据量不足或数据有噪声时可能比其他产品更好，</li><li id="6e0e" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">缺少的值需要被估算，</li><li id="215f" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">需要仔细的变量变换(最小值、最大值、宁滨)和缩放，</li><li id="ab90" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">需要明确地嵌入术语来表示非线性和相互作用，</li></ul><p id="4443" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> kNN </strong></p><p id="fdcb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">随机森林</strong></p><p id="d323" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">极度随机化的树</strong></a><strong class="lh ja">【ERT】</strong></p><ul class=""><li id="43e6" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">类似于随机森林，除了分裂规则的改变。</li><li id="a3ec" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">比随机森林更不容易过度适应。</li></ul><p id="ea08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://github.com/RGF-team/rgf/tree/master/python-package" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">【正规化的贪婪森林】</strong></a><strong class="lh ja">【RGF】</strong></p><ul class=""><li id="3de4" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">接近 GBDT，但使用不同的方法来制作新树和调整现有的树。</li></ul><p id="94e7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://www.csie.ntu.edu.tw/~r01922136/libffm/" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja"/></a><strong class="lh ja">(FFM)</strong></p><ul class=""><li id="f23a" class="ng nh iq lh b li lj ll lm lo ni ls nj lw nk ma nl nm nn no bi translated">与推荐任务非常匹配。</li><li id="6de3" class="ng nh iq lh b li np ll nq lo nr ls ns lw nt ma nl nm nn no bi translated">用于“<a class="ae le" href="https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/10555" rel="noopener ugc nofollow" target="_blank">展示广告挑战赛</a>等 Kaggle 比赛</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="ee55" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">结论👏</h1><p id="d9d9" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">由于其易于处理模型设置和下降精度，GBDT 是在表数据 ML 中开始建模的第一选择。</p><p id="7d6e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用 GBDT，我们可以在特征工程周围玩耍，直到找到运行良好的变量。</p><p id="b410" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">神经网络或线性模型可能是一种选择，取决于任务的类型，也将为集合中的模型多样性工作。</p><p id="4464" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">kNN，随机森林，ETR，RGF，FFM 也可以考虑模型多样性来集成。</p></div></div>    
</body>
</html>