<html>
<head>
<title>Solving the AI Accountability Gap</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解决人工智能问责制差距</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/solving-the-ai-accountability-gap-dd35698249fe?source=collection_archive---------13-----------------------#2019-04-05">https://towardsdatascience.com/solving-the-ai-accountability-gap-dd35698249fe?source=collection_archive---------13-----------------------#2019-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="863d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让开发者对他们的作品负责</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ec7ae5bf524767932234c3f1fe483f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWmVo5PD97pIp2VcNJBWLA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">A self-driving car — steering wheel not included</figcaption></figure><p id="cfca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">昨天，一份泄露的英国政府白皮书<a class="ae lu" href="https://www.theguardian.com/technology/2019/apr/04/social-media-bosses-could-be-liable-for-harmful-content-leaked-uk-plan-reveals" rel="noopener ugc nofollow" target="_blank">指出</a>社交媒体高管可能要对其平台算法上的有害内容扩散承担法律责任。这项提议旨在解决自主决策带来的一个最大问题:当人工智能造成伤害时，谁应该受到指责？</p><h2 id="6332" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated"><strong class="ak">心灵的差距</strong></h2><p id="a997" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这种“责任差距”是一个比看起来更糟糕的问题。我们的法律体系是建立在一个基本假设之上的<em class="mt">人类</em>代理人。用自主代理(自动驾驶汽车、社交媒体算法和其他类型的人工智能)取代人类行为者，会让这个系统陷入混乱。这一责任缺口导致了三个方面的问题:因果关系、正义和补偿。</p><p id="52ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mt">因果关系</em></p><p id="c371" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当有人开枪打死另一个人时，从法律角度来看，因果关系通常很简单:决定扣动扳机的人是另一个人死亡的最大“原因”，并由法律系统相应处理。但是，当一个人工智能自动做出一个对人造成伤害的决定时，法律责任的过程就会陷入混乱。</p><p id="1cb9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拿自动驾驶汽车来说。即使车辆完全自主地决定高速离开高速公路并发生碰撞(也许预见到迎面而来的碰撞，并倾向于采取规避行动)，你显然也不能将那辆自动驾驶汽车拖到法庭面前，迫使它面对正义。即使你可以(<a class="ae lu" href="https://www.academia.edu/37924523/Do_We_Need_New_Legal_Personhood_in_the_Age_of_Robots_and_AI" rel="noopener ugc nofollow" target="_blank">一些专家建议</a>人工智能程序被赋予法律人格，这意味着理论上它们<em class="mt">将</em>能够被拖上法庭)，也没有建立因果关系的先例或过程。</p><p id="f70e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与开发商、制造商或首席执行官相比，是否真的可以说自主系统的决策是导致乘客死亡的“最大原因”？我不太确定。</p><p id="0c6c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mt">正义</em></p><p id="4631" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，法律体系的惩罚和补救措施在很大程度上是针对人类的，而不是自主的计算机程序。对违法者实施有效惩罚的能力远非法律程序的附属品，而是对司法体系的基本公正至关重要。为了可信，法律系统能够适当有效地惩罚不法行为是至关重要的。</p><p id="6b83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，如果杀人犯被判处荒谬的“惩罚”,而这些惩罚并没有以任何有意义的方式影响他们的生活，司法系统将失去其作为一种威慑的价值，以及建立在正义得到伸张这一原则上的道德信誉。这抓住了人工智能带来的问题:你不能把人工智能扔进监狱，或对它处以罚款，或让它支付赔偿——不管它是否具有法律人格。法律系统的惩罚或补救措施没有一个对自主计算机程序有效。如果没有有效的惩罚或补救措施，我们的法律体系就失去了正义的基础。</p><p id="b05c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mt">补偿</em></p><p id="bf40" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为受害者伸张正义不仅仅包括对造成伤害的人进行有效的惩罚，在许多情况下，受害者寻求某种形式的金钱赔偿，即使只是收回法律费用。一个普遍的法律原则是，遭受伤害的人应该被放在他们<em class="mt">如果没有犯罪的话应该处于的位置。因此，如果像机器人手术工具这样的自主医疗设备出现故障并伤害了患者，患者应该能够在法律体系中寻求赔偿，以支付医疗费用。</em></p><p id="94b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，当伤害是由自主代理人造成时，我们的法律体系很难适用一般原则(在这种情况下，受害者赔偿)。法院显然不可能强迫人工智能——一个计算机程序——向受害者支付数千美元来支付他们的医疗费用。因此，为了让受害者在法律体系中有任何追索权，他们必须能够寻求有能力支付赔偿的人(或者至少是企业)。因此,“责任漏洞”给寻求通过法律系统获得赔偿的受害者增加了严重的困难。</p><p id="1621" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi">…</p><p id="a752" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因果关系、正义和赔偿这些问题是相互关联的，共同对我们的法律制度提出了巨大挑战。为了让系统保持可信和公正，从根本上需要以某种方式填补人工智能问责制的空白:首先将人工智能相关的伤害归咎于一个人或一群人。</p><h2 id="8718" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated"><strong class="ak">威力巨大……</strong></h2><p id="eb45" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这种将人工智能的行为(以及任何间接伤害)与一个人或一群人联系起来的必要性，向我们提出了公平的问题。当代人工智能系统的决策极其难以理解或解释——即使对负责编程的开发人员和编码人员来说也是如此。人工智能算法和过程非常复杂，以至于自主决策有时被比作黑匣子。在这种复杂的情况下，让一个人承担法律责任真的公平吗？</p><p id="3473" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简而言之:是的。我的观点是，人工智能开发者——定义为直接塑造人工智能编程的个人或群体——应该首先对人工智能决策所导致的行为(和任何伤害)承担法律责任。</p><p id="2efd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从道德上讲，人工智能开发人员是对人工智能所做的决定负最大责任的群体。尽管上面捕捉到了黑盒动态，但如果任何一方应该能够预见人工智能造成的未来伤害，那就是从无到有创造人工智能决策能力的团队。</p><p id="4234" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个(并非不合理的)类比是，一个行为不端的孩子在商店里发脾气并毁坏了一些产品——我们有理由期待孩子的父母为被毁坏的产品买单，即使他们自己没有造成损害，也不能说完全理解孩子为什么会造成损害。像那些父母一样，开发者处于<em class="mt">防止伤害的最佳位置</em>，即使他们对他们的创作缺乏完全的控制，一种道德责任从这种动态中产生。</p><p id="4e3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，同样的开发人员也是到目前为止在创建人工智能的复杂过程中积极采取风险管理和安全措施的最强有力的角色。像这篇文章提出的那样，对人工智能开发人员施加法律责任，会对该群体产生强烈的激励，促使他们加强安全措施，并遵守更严格的风险缓解框架。</p><p id="8956" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果像监管机构或制造商这样的团体在第一时间对人工智能负责，这种健康的激励就不会出现在同样的程度上，因为他们对人工智能的发展方式没有那么直接的控制。人工智能开发者有能力让人工智能变得更安全，我的提议将他们的个人激励与这一责任结合起来。</p><p id="8c95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在实践中，我想象这个提议看起来像一个“可反驳的假设”,即人工智能开发者应该面对特定人工智能的任何受害者的诉讼。一个可反驳的假设与承认有罪不是一回事:它只是意味着，如果一个人工智能开发者认为另一方(比如他们组织的首席执行官)对人工智能造成的伤害负有更大的<em class="mt">责任，他们必须向法庭证明这种信念，以避免责任。</em></p><p id="cb7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">举一个假设的例子(<a class="ae lu" href="https://www.bbc.com/news/technology-44640959" rel="noopener ugc nofollow" target="_blank">咳</a>)某社交媒体巨头的人工智能开发人员，他们创造了一个像老虎机一样令人上瘾的算法。我的可反驳的假设允许那些认为自己受到算法伤害的人起诉那些开发者。但是，如果这些开发人员能够指出强有力的证据，证明他们直接受到管理层的指示，让人工智能尽可能令人上瘾，那么诉讼就会落在那些沉迷于增长的经理们的脚下。</p><p id="5257" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">无论人工智能伤害案件的最终被告是开发人员还是开发人员可以指向的另一方，与人工智能责任差距相关的问题都得到了解决。确定因果关系的责任在于开发商(而不是受害者)，正义可以通过有效的惩罚和补救措施来实现，受害者可以向人类一方寻求赔偿。</p><h2 id="3e6d" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated"><em class="mu">发展网飞，冷却</em></h2><p id="dece" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">AI 问责没有完美的解决方案。让开发者承担责任的提议的最大风险之一是对人工智能开发的寒蝉效应。毕竟 AI 开发者往往是小演员——个人或者小公司。当他们的创造造成伤害时，不管他们是否是最应该受到谴责的，每次他们的人工智能造成伤害时面临诉讼的现实噩梦<em class="mt">可能会合理地使人工智能开发者非常谨慎地将他们的创造发布到世界上(他们的对冲基金投资者可能会在伸手拿支票簿之前停下来)。</em></p><p id="1ddc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，寒蝉效应的威胁不足以压倒上述伦理和实践方面的考虑。许多为大型科技巨头工作的开发人员将受益于替代责任，这意味着科技巨头将被迫将他们的法律资源用于保护他们的开发人员。</p><p id="da4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可能最终会看到政府采用“无过失”保险政策(类似于<a class="ae lu" href="https://www.acc.co.nz/" rel="noopener ugc nofollow" target="_blank">新西兰的 ACC 系统</a>用于支付与事故相关的医疗费用)来支付与人工智能相关的诉讼。例如，考虑到自动驾驶汽车的部署可能会拯救许多生命，国家肯定有强烈的动机以这种方式支持人工智能的部署。受害者将失去起诉开发商个人的能力，但会得到伤害承认，其医疗费用由政府承担。</p><p id="c42e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者，政府可以为自己保留起诉其发明造成伤害的人工智能开发者的权力。这可能是一个将最恶劣的罪犯送上法庭的更有效的系统。自主决策的受害者可以通过一个专门的机构让开发商承担责任，类似于警方在刑事案件中与受害者合作。这有助于防止闸门被打开，并有助于防止基于车库的人工智能编码员被淹没在民事诉讼中。</p><p id="af2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些思想实验捕捉到的是，人工智能问责制的差距可以得到解决——有一个新的假设，即人工智能的开发者首先要负责——<em class="mt">而不需要</em>通过让程序员破产来拖延行业。</p><p id="1645" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi">…</p><p id="80f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从本月泄露的文件来看，英国将提出一种解决人工智能问责差距的方法:针对科技巨头的首席执行官。这篇文章规划了一条不同的道路，一开始就让开发者负责。无论如何，越来越有必要调整我们的法律体系，使其能够以道德和法律公平的方式处理自主代理人的案件。</p><p id="0300" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种结构性的航向修正容易吗？当然不是。但它用公共政策辩论取代了令人担忧的问责差距，这无疑是朝着正确方向迈出的一步。</p></div></div>    
</body>
</html>