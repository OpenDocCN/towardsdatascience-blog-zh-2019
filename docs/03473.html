<html>
<head>
<title>SVM: Feature Selection and Kernels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SVM:特征选择和内核</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c?source=collection_archive---------5-----------------------#2019-06-03">https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c?source=collection_archive---------5-----------------------#2019-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1b0293cac5669e14b3b3aa73e689f2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06GSco3ItM3gwW2scY6Tmg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">(Source: <a class="ae jg" rel="noopener" target="_blank" href="/support-vector-machine-vs-logistic-regression-94cc2975433f">https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f</a>)</figcaption></figure><div class=""/><blockquote class="kg kh ki"><p id="ae40" class="kj kk kl km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">支持向量机(SVM)是一种受监督的机器学习算法，可用于分类和回归目的。</p><p id="83ec" class="kj kk kl km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">诺埃尔·班布里克。</p></blockquote><figure class="li lj lk ll gt iv"><div class="bz fp l di"><div class="lm ln l"/></div></figure><h1 id="460a" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">介绍</h1><p id="43fa" class="pw-post-body-paragraph kj kk jj km b kn mm kp kq kr mn kt ku mo mp kx ky mq mr lb lc ms mt lf lg lh im bi translated">支持向量机(SVM)是一种机器学习算法，可用于许多不同的任务(图 1)。在本文中，我将解释数学基础，以演示该算法如何用于二进制分类目的。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/227e4dc6f1152b0c3a9ca7b8445121ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*C1MId293xqoVio8av9Rc2g.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 1: SVM Applications [1]</figcaption></figure><p id="a726" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">SVM 的主要目标是找到最佳超平面，以便在不同类别的数据点之间进行正确分类(图 2)。超平面维度等于输入特征的数量减一(例如，当处理三个特征时，超平面将是一个二维平面)。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/25bd1007fcca6092a411ff7dcaeb6637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwrX8viaCLljRAAxiSAp8Q.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 2: SVM Hyperplane [2]</figcaption></figure><p id="30b5" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">超平面一侧的数据点将被分类到某个类别，而超平面另一侧的数据点将被分类到不同的类别(例如，如图 2 中的绿色和红色)。超平面和该超平面任一侧上的第一点(对于所有不同的类)之间的距离是该算法关于其分类决策的确信度的度量。距离越远，我们就越有信心 SVM 做出的决定是正确的。</p><p id="4302" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">最接近超平面的数据点称为支持向量。支持向量确定超平面的方向和位置，以便最大化分类器余量(并因此最大化分类分数)。SVM 算法应该使用的支持向量的数量可以根据应用任意选择。</p><p id="5689" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">使用 Scikit-Learn Python 库，只需几行代码就可以轻松实现基本的 SVM 分类。</p><pre class="li lj lk ll gt mw mx my mz aw na bi"><span id="95a1" class="nb lp jj mx b gy nc nd l ne nf">from sklearn import svm<br/>trainedsvm = svm.SVC().fit(X_Train, Y_Train)<br/>predictionsvm = trainedsvm.predict(X_Test)<br/>print(confusion_matrix(Y_Test,predictionsvm))<br/>print(classification_report(Y_Test,predictionsvm))</span></pre><p id="76d5" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">有两种主要的分类 SVM 算法硬边界和软边界:</p><ul class=""><li id="d5cb" class="ng nh jj km b kn ko kr ks mo ni mq nj ms nk lh nl nm nn no bi translated"><strong class="km jk">硬边界:</strong>旨在寻找最佳超平面，而不容忍任何形式的错误分类。</li><li id="e4ed" class="ng nh jj km b kn np kr nq mo nr mq ns ms nt lh nl nm nn no bi translated"><strong class="km jk">软利润:</strong>我们在 SVM 增加了一定程度的宽容。以这种方式，我们允许模型自愿错误分类一些数据点，如果这可以导致识别能够更好地概括看不见的数据的超平面。</li></ul><p id="8cac" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">通过在<code class="fe nu nv nw mx b">svm.SVC</code>中添加一个 C 惩罚项，可以在 Scikit-Learn 中实现软余量 SVM。C 越大，算法在进行错误分类时得到的惩罚越多。</p><h1 id="0706" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">内核技巧</h1><p id="9dae" class="pw-post-body-paragraph kj kk jj km b kn mm kp kq kr mn kt ku mo mp kx ky mq mr lb lc ms mt lf lg lh im bi translated">如果我们正在处理的数据不是线性可分的(因此导致较差的线性 SVM 分类结果)，可以应用一种称为核技巧的技术。这种方法能够将我们的非线性可分数据映射到一个更高维的空间，使我们的数据线性可分。使用这个新的维度空间，SVM 可以很容易地实现(图 3)。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/145be716e902d1c2030a00a72422b938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zWzeMGyCc7KvGD9X8lwlnQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 3: Kernel Trick [3]</figcaption></figure><p id="c3fa" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">有许多不同类型的核可用于创建这种高维空间，一些例子是线性、多项式、Sigmoid 和径向基函数(RBF)。在 Scikit-Learn 中，可以通过在<code class="fe nu nv nw mx b">svm.SVC</code>中添加一个内核参数来指定一个内核函数。可以包含一个称为 gamma 的附加参数来指定内核对模型的影响。</p><p id="6985" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">如果数据集中的要素数量大于观测值数量，通常建议使用线性核(否则 RBF 可能是更好的选择)。</p><p id="cb7e" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">当使用 RBF 处理大量数据时，速度可能会成为一个需要考虑的约束。</p><h1 id="ec8d" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">特征选择</h1><p id="1723" class="pw-post-body-paragraph kj kk jj km b kn mm kp kq kr mn kt ku mo mp kx ky mq mr lb lc ms mt lf lg lh im bi translated">一旦拟合了我们的线性 SVM，就可以使用训练模型上的<code class="fe nu nv nw mx b">.coef_</code>来访问分类器系数。这些权重表示与超平面正交的正交向量坐标。相反，它们的方向代表预测的类。</p><p id="859f" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">因此，可以通过相互比较这些系数的大小来确定特征重要性。因此，通过查看 SVM 系数，可以识别分类中使用的主要特征，并去除不重要的特征(方差较小)。</p><p id="b898" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">减少机器学习中的特征数量非常重要，尤其是在处理大型数据集时。这实际上可以:加速训练，避免过度拟合，并且由于数据中噪声的减少，最终导致更好的分类结果。</p><p id="5e18" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">图 4 显示了我在皮马印第安人糖尿病数据库上使用 SVM 识别的主要特征。绿色表示对应于负系数的所有特征，蓝色表示正系数。如果你想了解更多，我所有的代码都可以在我的<a class="ae jg" href="https://www.kaggle.com/pierpaolo28/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>和<a class="ae jg" href="https://github.com/pierpaolo28/Companies-Data-set-Challenges/blob/master/Microsoft%20Workshop%20-%20Deep%20Learning%20Data%20Analysis%20in%20Azure.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>个人资料中免费获得。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/573a679d932fc110f8dbb6174ea2cc2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*da24RNaZz56cZVcgI2SC5w.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 4: Feature Importance using SVM</figcaption></figure><h1 id="81c6" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">数学</h1><p id="a065" class="pw-post-body-paragraph kj kk jj km b kn mm kp kq kr mn kt ku mo mp kx ky mq mr lb lc ms mt lf lg lh im bi translated">如果你想深入研究 SVM 背后的数学，我在这里留下了 Patrick Winston 的演讲，可以在麻省理工学院开放课程 YouTube 频道上找到。这个讲座说明了如何推导 SVM 决策规则，以及哪些数学约束是适用于使用拉格朗日乘数。</p><figure class="li lj lk ll gt iv"><div class="bz fp l di"><div class="nz ln l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Video 1: <a class="ae jg" href="https://www.youtube.com/channel/UCEBb1b_L6zDS3xTUrIALZOw" rel="noopener ugc nofollow" target="_blank">MIT OpenCourseWare</a> [4]</figcaption></figure><h1 id="c263" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">联系人</h1><p id="8ff9" class="pw-post-body-paragraph kj kk jj km b kn mm kp kq kr mn kt ku mo mp kx ky mq mr lb lc ms mt lf lg lh im bi translated">如果你想了解我最新的文章和项目<a class="ae jg" href="https://medium.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener">，请通过媒体</a>关注我，并订阅我的<a class="ae jg" href="http://eepurl.com/gwO-Dr?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">邮件列表</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="8caa" class="ng nh jj km b kn ko kr ks mo ni mq nj ms nk lh nl nm nn no bi translated"><a class="ae jg" href="https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></li><li id="81ae" class="ng nh jj km b kn np kr nq mo nr mq ns ms nt lh nl nm nn no bi translated"><a class="ae jg" href="https://pierpaolo28.github.io/blog/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人博客</a></li><li id="41a5" class="ng nh jj km b kn np kr nq mo nr mq ns ms nt lh nl nm nn no bi translated"><a class="ae jg" href="https://pierpaolo28.github.io/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人网站</a></li><li id="a856" class="ng nh jj km b kn np kr nq mo nr mq ns ms nt lh nl nm nn no bi translated"><a class="ae jg" href="https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener" target="_blank">中等轮廓</a></li><li id="0ac6" class="ng nh jj km b kn np kr nq mo nr mq ns ms nt lh nl nm nn no bi translated"><a class="ae jg" href="https://github.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="55ef" class="ng nh jj km b kn np kr nq mo nr mq ns ms nt lh nl nm nn no bi translated"><a class="ae jg" href="https://www.kaggle.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><h1 id="904d" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">文献学</h1><p id="19df" class="pw-post-body-paragraph kj kk jj km b kn mm kp kq kr mn kt ku mo mp kx ky mq mr lb lc ms mt lf lg lh im bi translated">[1]无泪支持向量机，Ankit Sharma。访问地点:<a class="ae jg" href="https://www.slideshare.net/ankitksharma/svm-37753690" rel="noopener ugc nofollow" target="_blank">https://www.slideshare.net/ankitksharma/svm-37753690</a></p><p id="585d" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">[2]支持向量机—机器学习算法介绍，<a class="ae jg" href="https://towardsdatascience.com/@grohith327" rel="noopener" target="_blank">罗希斯·甘地</a>。访问:<a class="ae jg" rel="noopener" target="_blank" href="/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">https://towardsdatascience . com/support-vector-machine-introduction-to-machine-learning-algorithms-934 a 444 FCA 47</a></p><p id="8517" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">[3]支持向量机，杰瑞米·乔登。访问地点:<a class="ae jg" href="https://www.jeremyjordan.me/support-vector-machines/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/support-vector-machines/</a></p><p id="547a" class="pw-post-body-paragraph kj kk jj km b kn ko kp kq kr ks kt ku mo kw kx ky mq la lb lc ms le lf lg lh im bi translated">[4]麻省理工学院开放式课程，16。学习:支持向量机。访问地点:<a class="ae jg" href="https://www.youtube.com/watch?v=_PwhiWxHK8o" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=_PwhiWxHK8o</a></p></div></div>    
</body>
</html>